{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample annotations pickle\n",
    "pickle_file = '/home/sxchao/MSI_prediction/tcga_project/tcga_wgd_sa_all.pkl'\n",
    "batch_all, _, _, sa_trains, sa_vals = data_utils.load_COAD_train_val_sa_pickle(pickle_file=pickle_file, \n",
    "                                                                               return_all_cancers=True, \n",
    "                                                                               split_in_two=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Datasets\n",
    "train_sets = []\n",
    "val_sets = []\n",
    "\n",
    "magnification = '10.0'\n",
    "root_dir = '/n/mounted-data-drive/'\n",
    "train_transform = train_utils.transform_train\n",
    "val_transform = train_utils.transform_validation\n",
    "\n",
    "train_cancers = ['COAD', 'BRCA', 'READ_10x', 'LUSC_10x', 'BLCA_10x', 'LUAD_10x', 'STAD_10x', 'HNSC_10x']\n",
    "val_cancers = ['UCEC', 'LIHC_10x', 'KIRC_10x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COAD BRCA READ_10x LUSC_10x BLCA_10x LUAD_10x STAD_10x HNSC_10x "
     ]
    }
   ],
   "source": [
    "for i in range(len(train_cancers)):\n",
    "    print(train_cancers[i], end=' ')\n",
    "    train_set = data_utils.TCGADataset_tiles(sa_trains[batch_all.index(train_cancers[i])], \n",
    "                                             root_dir + train_cancers[i] + '/', \n",
    "                                             transform=train_transform, \n",
    "                                             magnification=magnification, \n",
    "                                             batch_type='tile')\n",
    "    train_sets.append(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCEC LIHC_10x KIRC_10x "
     ]
    }
   ],
   "source": [
    "for j in range(len(val_cancers)):\n",
    "    print(val_cancers[j], end=' ')\n",
    "    val_set = data_utils.TCGADataset_tiles(sa_vals[batch_all.index(val_cancers[j])], \n",
    "                                           root_dir + val_cancers[j] + '/', \n",
    "                                           transform=val_transform, \n",
    "                                           magnification=magnification, \n",
    "                                           batch_type='tile')\n",
    "    val_sets.append(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return torch.stack([d[i][0] for d in self.datasets]), torch.cat([torch.tensor(d[i][1]).view(-1) for d in self.datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(ConcatDataset(*train_sets), \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=16, \n",
    "                                           pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(ConcatDataset(*val_sets), \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=16, \n",
    "                                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, initial_vals=None, dropout=0.0):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d = nn.Dropout(dropout)\n",
    "        self.m = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        if initial_vals != None:\n",
    "            self.linear1.weight = torch.nn.Parameter(initial_vals[0])\n",
    "            self.linear1.bias = torch.nn.Parameter(initial_vals[1])\n",
    "            self.linear2.weight = torch.nn.Parameter(initial_vals[2])\n",
    "            self.linear2.bias = torch.nn.Parameter(initial_vals[3])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        hidden = self.m(self.linear1(self.d(inputs)))\n",
    "        output = self.linear2(self.d(hidden))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model args\n",
    "state_dict_file = '/n/tcga_models/resnet18_WGD_all_10x.pt'\n",
    "device = torch.device('cuda', 0)\n",
    "input_size = 2048\n",
    "hidden_size = 512\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local(tiles, labels, resnet, theta_global, alpha = 0.01, criterion = nn.BCEWithLogitsLoss(),\n",
    "                input_size = input_size, hidden_size = hidden_size, output_size = output_size):\n",
    "    idx = int(tiles.shape[0] / 2)\n",
    "    num_tasks = int(batch.shape[1])\n",
    "    \n",
    "    # initialize models, set theta_local = theta_global    \n",
    "    models = []\n",
    "    for i in range(num_tasks):\n",
    "        models.append(FeedForward(input_size, hidden_size, output_size, theta_global).cuda())  \n",
    "\n",
    "    # grads storage    \n",
    "    grads = [torch.zeros(theta_global[i].shape).cuda() for i in range(len(theta_global))]\n",
    "\n",
    "    for t in range(num_tasks):\n",
    "        # first forward pass, step\n",
    "        net = models[t]\n",
    "        net.train()\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr = alpha)\n",
    "        \n",
    "        inputs = tiles[:idx,t,:,:,:]\n",
    "        embed = resnet(inputs)\n",
    "        output = net(embed)\n",
    "        loss = criterion(output, labels[:idx,t])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # second forward pass, store grads\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch[idx:,t,:,:,:]\n",
    "        embed = resnet(inputs)        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        grads[0] = grads[0] + net.linear1.weight.grad.data\n",
    "        grads[1] = grads[1] + net.linear1.bias.grad.data\n",
    "        grads[2] = grads[2] + net.linear2.weight.grad.data\n",
    "        grads[3] = grads[3] + net.linear2.bias.grad.data\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global(theta_global, model_global, grads, eta = 0.01):\n",
    "    theta_global = [theta_global[i] - (eta * grads[i]) for i in range(len(theta_global))]\n",
    "    \n",
    "    model_global.linear1.weight = torch.nn.Parameter(theta_global[0])\n",
    "    model_global.linear1.bias = torch.nn.Parameter(theta_global[1])\n",
    "    model_global.linear2.weight = torch.nn.Parameter(theta_global[2])\n",
    "    model_global.linear2.bias = torch.nn.Parameter(theta_global[3])\n",
    "\n",
    "    return theta_global, model_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(e, resnet, model_global, val_loader, criterion = nn.BCEWithLogitsLoss()):\n",
    "    model_global.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_output = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch,labels in val_loader:\n",
    "        batch_size = batch.shape[0]\n",
    "        num_tasks = batch.shape[1]\n",
    "        labels = labels.cuda().transpose(0,1)\n",
    "        inputs = batch.cuda().transpose(0,1).reshape(batch_size * len(num_tasks), 3, 256, 256)\n",
    "        \n",
    "        embed = resnet(inputs)\n",
    "        output = model_global(embed)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        all_output.extend((output.contiguous().view(-1) > 0.5).float().detach().cpu().numpy())\n",
    "        all_labels.extend(labels.contiguous().view(-1).float().detach().cpu().numpy())\n",
    "    \n",
    "    acc = np.mean(np.array(all_output) == np.array(all_labels))\n",
    "    if e % 1 == 0:\n",
    "        print('Epoch: {0}, Val NLL: {1:0.4f}, Val Acc: {2:0.4f}'.format(e, loss, acc))\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize trained resnet\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(2048, output_size, bias=True)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc: storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "\n",
    "# freeze layers\n",
    "resnet.fc = Identity()\n",
    "resnet.cuda(device=device)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize theta_global\n",
    "model_global = FeedForward(input_size, hidden_size, output_size).cuda()\n",
    "theta_global = []\n",
    "for p in model_global.parameters():\n",
    "    theta_global.append(torch.randn(list(p.shape)).cuda())\n",
    "    \n",
    "model_global.linear1.weight = torch.nn.Parameter(theta_global[0])\n",
    "model_global.linear1.bias = torch.nn.Parameter(theta_global[1])\n",
    "model_global.linear2.weight = torch.nn.Parameter(theta_global[2])\n",
    "model_global.linear2.bias = torch.nn.Parameter(theta_global[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train params\n",
    "num_epochs = 1000\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "patience = 3\n",
    "factor = 0.1\n",
    "patience_count = 0\n",
    "previous_loss = 1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train meta-learner\n",
    "for e in range(num_epochs):\n",
    "    # reduce LR on plateau\n",
    "    if patience_count > patience:\n",
    "        alpha = factor * alpha\n",
    "        eta = factor * eta\n",
    "        patience_count = 0\n",
    "        print('--- LR DECAY --- Alpha: {0:0.8f}, Eta: {1:0.8f}'.format(alpha, eta))\n",
    "    \n",
    "    for step, (tiles, labels) in enumerate(train_loader):  \n",
    "        if step % 1 == 0:\n",
    "            print(step, end=' ')\n",
    "        tiles, labels = tiles.cuda(), labels.cuda()           \n",
    "        grads = train_local(tiles, labels, resnet, theta_global, alpha = alpha)\n",
    "        theta_global, model_global = train_global(theta_global, model_global, grads, eta = eta)\n",
    "    \n",
    "    loss, acc = run_validation(e, resnet, model_global, val_loader)\n",
    "    \n",
    "    if loss > previous_loss:\n",
    "        patience_count += 1\n",
    "    else:\n",
    "        patience_count = 0\n",
    "        \n",
    "    previous_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
