{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import model_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample annotations pickle\n",
    "pickle_file = '/home/sxchao/MSI_prediction/tcga_project/tcga_wgd_sa_all.pkl'\n",
    "batch_all, _, _, sa_trains, sa_vals = data_utils.load_COAD_train_val_sa_pickle(pickle_file=pickle_file, \n",
    "                                                                               return_all_cancers=True, \n",
    "                                                                               split_in_two=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Datasets\n",
    "train_sets = []\n",
    "val_sets = []\n",
    "\n",
    "magnification = '10.0'\n",
    "root_dir = '/n/mounted-data-drive/'\n",
    "train_transform = train_utils.transform_train\n",
    "val_transform = train_utils.transform_validation\n",
    "\n",
    "train_cancers = ['COAD', 'BRCA', 'READ_10x', 'LUSC_10x', 'BLCA_10x', 'LUAD_10x', 'STAD_10x', 'HNSC_10x']\n",
    "val_cancers = ['UCEC', 'LIHC_10x', 'KIRC_10x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COAD BRCA READ_10x LUSC_10x BLCA_10x LUAD_10x STAD_10x HNSC_10x "
     ]
    }
   ],
   "source": [
    "for i in range(len(train_cancers)):\n",
    "    print(train_cancers[i], end=' ')\n",
    "    train_set = data_utils.TCGADataset_tiles(sa_trains[batch_all.index(train_cancers[i])], \n",
    "                                             root_dir + train_cancers[i] + '/', \n",
    "                                             transform=train_transform, \n",
    "                                             magnification=magnification, \n",
    "                                             batch_type='tile')\n",
    "    train_sets.append(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCEC LIHC_10x KIRC_10x "
     ]
    }
   ],
   "source": [
    "for j in range(len(val_cancers)):\n",
    "    print(val_cancers[j], end=' ')\n",
    "    val_set = data_utils.TCGADataset_tiles(sa_vals[batch_all.index(val_cancers[j])], \n",
    "                                           root_dir + val_cancers[j] + '/', \n",
    "                                           transform=val_transform, \n",
    "                                           magnification=magnification, \n",
    "                                           batch_type='tile')\n",
    "    val_sets.append(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 100\n",
    "train_loader = torch.utils.data.DataLoader(data_utils.ConcatDataset(*train_sets), \n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=20, \n",
    "                                           pin_memory=True)\n",
    "batch_size_val = 100\n",
    "val_loader = torch.utils.data.DataLoader(data_utils.ConcatDataset(*val_sets), \n",
    "                                        batch_size=batch_size_val, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=20, \n",
    "                                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model args\n",
    "state_dict_file = '/n/tcga_models/resnet18_WGD_all_10x.pt'\n",
    "device = torch.device('cuda', 0)\n",
    "input_size = 2048\n",
    "hidden_size = 512\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local(step, tiles, labels, resnet, local_models, alpha=0.01, criterion=nn.BCEWithLogitsLoss()):\n",
    "    resnet.eval()\n",
    "    idx = int(tiles.shape[0] / 2)\n",
    "    num_tasks = int(tiles.shape[1])\n",
    "    \n",
    "    # grads storage    \n",
    "    grads = [torch.zeros(p.shape).cuda() for p in local_models[0].parameters()]\n",
    "\n",
    "    #t = torch.randint(num_tasks, (1,)).item()\n",
    "    for t in range(num_tasks):\n",
    "        # first forward pass, step\n",
    "        net = local_models[t]\n",
    "        net.train()\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr = alpha)\n",
    "\n",
    "        inputs = tiles[:idx,t,:,:,:]\n",
    "        embed = resnet(inputs)\n",
    "        output = net(embed)\n",
    "        loss = criterion(output, labels[:idx,t].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # second forward pass, store grads\n",
    "        inputs = tiles[idx:,t,:,:,:]\n",
    "        embed = resnet(inputs)\n",
    "        output = net(embed)\n",
    "        loss = criterion(output, labels[idx:,t].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        grads[0] = grads[0] + net.linear1.weight.grad.data\n",
    "        grads[1] = grads[1] + net.linear1.bias.grad.data\n",
    "        grads[2] = grads[2] + net.linear2.weight.grad.data\n",
    "        grads[3] = grads[3] + net.linear2.bias.grad.data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    if step % 50 == 0:\n",
    "        output = (output.contiguous().view(-1) > 0.5).float().detach().cpu().numpy()\n",
    "        labels = labels[idx:,t].contiguous().view(-1).float().detach().cpu().numpy()\n",
    "        acc, tile_acc_by_label = train_utils.calc_tile_acc_stats(labels, output)\n",
    "        print('Step: {0}, Train NLL: {1:0.4f}, Acc: {2:0.4f}, By Label: {3}'.format(step, loss, acc, tile_acc_by_label))\n",
    "\n",
    "    return grads, local_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global(theta_global, model_global, grads, eta=0.01):\n",
    "    theta_global = [theta_global[i] - (eta * grads[i]) for i in range(len(theta_global))]\n",
    "    \n",
    "    model_global.linear1.weight = torch.nn.Parameter(theta_global[0])\n",
    "    model_global.linear1.bias = torch.nn.Parameter(theta_global[1])\n",
    "    model_global.linear2.weight = torch.nn.Parameter(theta_global[2])\n",
    "    model_global.linear2.bias = torch.nn.Parameter(theta_global[3])\n",
    "\n",
    "    return theta_global, model_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(e, resnet, model_global, val_loader, criterion=nn.BCEWithLogitsLoss()):\n",
    "    resnet.eval()\n",
    "    model_global.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_output = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for step, (batch,labels) in enumerate(val_loader):\n",
    "        batch_size = batch.shape[0]\n",
    "        num_tasks = batch.shape[1]\n",
    "        labels = labels.cuda().transpose(0,1).reshape(batch_size * num_tasks, 1).float()\n",
    "        inputs = batch.cuda().transpose(0,1).reshape(batch_size * num_tasks, 3, 256, 256)\n",
    "        \n",
    "        embed = resnet(inputs)\n",
    "        output = model_global(embed)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        output = (output.contiguous().view(-1) > 0.5).float().detach().cpu().numpy()\n",
    "        labels = labels.contiguous().view(-1).float().detach().cpu().numpy()\n",
    "        \n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        all_output.extend(output)\n",
    "        all_labels.extend(labels)\n",
    "    \n",
    "        if step % 50 == 0:\n",
    "            acc, tile_acc_by_label = train_utils.calc_tile_acc_stats(labels, output)\n",
    "            print('Step: {0}, Val NLL: {1:0.4f}, Acc: {2:0.4f}, By Label: {3}'.format(step, loss, acc, tile_acc_by_label))\n",
    "                \n",
    "    acc, tile_acc_by_label = train_utils.calc_tile_acc_stats(all_labels, all_output)\n",
    "    print('Epoch: {0}, Val NLL: {1:0.4f}, Acc: {2:0.4f}, By Label: {3}'.format(e, loss, acc, tile_acc_by_label))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize trained resnet\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(2048, output_size, bias=True)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc: storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "\n",
    "# freeze layers\n",
    "resnet.fc = model_utils.Identity()\n",
    "resnet.cuda(device=device)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize theta_global\n",
    "model_global = model_utils.FeedForward(input_size, hidden_size, output_size).cuda()\n",
    "theta_global = []\n",
    "for p in model_global.parameters():\n",
    "    theta_global.append(torch.randn(list(p.shape)).cuda())\n",
    "    \n",
    "model_global.linear1.weight = torch.nn.Parameter(theta_global[0])\n",
    "model_global.linear1.bias = torch.nn.Parameter(theta_global[1])\n",
    "model_global.linear2.weight = torch.nn.Parameter(theta_global[2])\n",
    "model_global.linear2.bias = torch.nn.Parameter(theta_global[3])\n",
    "\n",
    "# initialize local models, set theta_local = theta_global    \n",
    "local_models = []\n",
    "for i in range(len(train_cancers)):\n",
    "    local_models.append(model_utils.FeedForward(input_size, hidden_size, output_size, theta_global).cuda()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train params\n",
    "num_epochs = 1000\n",
    "alpha = 1e-4\n",
    "eta = 1e-4\n",
    "patience = 1\n",
    "factor = 0.1\n",
    "patience_count = 0\n",
    "previous_loss = 1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train NLL: 492.5553, Acc: 0.3200, By Label: 0: 0.0571, 1: 0.9333\n",
      "Step: 50, Train NLL: 163.4610, Acc: 0.5400, By Label: 0: 0.5757, 1: 0.4705\n",
      "Step: 100, Train NLL: 100.3759, Acc: 0.6600, By Label: 0: 0.8, 1: 0.45\n",
      "Step: 150, Train NLL: 72.8824, Acc: 0.7200, By Label: 0: 0.8275, 1: 0.5714\n"
     ]
    }
   ],
   "source": [
    "# train meta-learner\n",
    "for e in range(num_epochs):\n",
    "    # reduce LR on plateau\n",
    "    if patience_count > patience:\n",
    "        alpha = factor * alpha\n",
    "        eta = factor * eta\n",
    "        patience_count = 0\n",
    "        print('--- LR DECAY --- Alpha: {0:0.8f}, Eta: {1:0.8f}'.format(alpha, eta))\n",
    "    \n",
    "    for step, (tiles, labels) in enumerate(train_loader):  \n",
    "        tiles, labels = tiles.cuda(), labels.cuda().float()           \n",
    "        grads, local_models = train_local(step, tiles, labels, resnet, local_models, alpha = alpha)\n",
    "        theta_global, model_global = train_global(theta_global, model_global, grads, eta = eta)\n",
    "        for i in range(len(local_models)):\n",
    "            local_models[i].update_params(theta_global)\n",
    "            \n",
    "    loss, acc = run_validation(e, resnet, model_global, val_loader)\n",
    "    \n",
    "    if loss > previous_loss:\n",
    "        patience_count += 1\n",
    "    else:\n",
    "        patience_count = 0\n",
    "        \n",
    "    previous_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Val NLL: 0.7332, Acc: 0.7100, By Label: 0: 0.9330, 1: 0.0526\n",
      "Step: 50, Val NLL: 0.7045, Acc: 0.7067, By Label: 0: 0.9398, 1: 0.1071\n",
      "Step: 100, Val NLL: 0.6924, Acc: 0.7100, By Label: 0: 0.9571, 1: 0.1333\n",
      "Step: 150, Val NLL: 0.7229, Acc: 0.7167, By Label: 0: 0.9452, 1: 0.0987\n",
      "Step: 200, Val NLL: 0.7237, Acc: 0.6800, By Label: 0: 0.9377, 1: 0.0879\n",
      "Step: 250, Val NLL: 0.7034, Acc: 0.7033, By Label: 0: 0.9369, 1: 0.0384\n",
      "Step: 300, Val NLL: 0.6934, Acc: 0.6867, By Label: 0: 0.9336, 1: 0.1011\n",
      "Step: 350, Val NLL: 0.7325, Acc: 0.6833, By Label: 0: 0.9436, 1: 0.0459\n",
      "Step: 400, Val NLL: 0.7169, Acc: 0.6967, By Label: 0: 0.9248, 1: 0.1379\n",
      "Step: 450, Val NLL: 0.7155, Acc: 0.7533, By Label: 0: 0.9511, 1: 0.16\n",
      "Step: 500, Val NLL: 0.7533, Acc: 0.7233, By Label: 0: 0.9372, 1: 0.1038\n",
      "Step: 550, Val NLL: 0.7105, Acc: 0.7033, By Label: 0: 0.9209, 1: 0.1529\n",
      "Step: 600, Val NLL: 0.7235, Acc: 0.6867, By Label: 0: 0.9086, 1: 0.0864\n",
      "Step: 650, Val NLL: 0.6936, Acc: 0.7033, By Label: 0: 0.9311, 1: 0.0975\n",
      "Step: 700, Val NLL: 0.7152, Acc: 0.6933, By Label: 0: 0.9386, 1: 0.1022\n",
      "Step: 750, Val NLL: 0.7106, Acc: 0.7300, By Label: 0: 0.9638, 1: 0.0759\n",
      "Step: 800, Val NLL: 0.7160, Acc: 0.7633, By Label: 0: 0.9778, 1: 0.1081\n",
      "Step: 850, Val NLL: 0.6996, Acc: 0.7300, By Label: 0: 0.9549, 1: 0.0897\n",
      "Step: 900, Val NLL: 0.7380, Acc: 0.7200, By Label: 0: 0.9330, 1: 0.0921\n",
      "Step: 950, Val NLL: 0.6976, Acc: 0.7233, By Label: 0: 0.9459, 1: 0.0897\n",
      "Step: 1000, Val NLL: 0.6922, Acc: 0.7133, By Label: 0: 0.9541, 1: 0.0731\n",
      "Step: 1050, Val NLL: 0.7157, Acc: 0.7267, By Label: 0: 0.9633, 1: 0.0975\n",
      "Step: 1100, Val NLL: 0.7010, Acc: 0.7233, By Label: 0: 0.9414, 1: 0.1025\n",
      "Step: 1150, Val NLL: 0.7446, Acc: 0.7200, By Label: 0: 0.9333, 1: 0.08\n",
      "Epoch: 0, Val NLL: 0.7478, Acc: 0.7135, By Label: 0: 0.9420, 1: 0.0894\n"
     ]
    }
   ],
   "source": [
    "loss, acc = run_validation(e, resnet, model_global, val_loader) # batch size = 200, lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Val NLL: 0.9906, Acc: 0.6233, By Label: 0: 0.7336, 1: 0.2676\n",
      "Step: 50, Val NLL: 1.1043, Acc: 0.6100, By Label: 0: 0.7370, 1: 0.2988\n",
      "Step: 100, Val NLL: 0.9727, Acc: 0.6633, By Label: 0: 0.7782, 1: 0.3417\n",
      "Step: 150, Val NLL: 1.1521, Acc: 0.6033, By Label: 0: 0.7162, 1: 0.3176\n",
      "Step: 200, Val NLL: 1.2545, Acc: 0.5800, By Label: 0: 0.7017, 1: 0.1944\n",
      "Step: 250, Val NLL: 1.2396, Acc: 0.6133, By Label: 0: 0.7534, 1: 0.2345\n",
      "Step: 300, Val NLL: 1.3489, Acc: 0.5767, By Label: 0: 0.7242, 1: 0.2093\n",
      "Step: 350, Val NLL: 0.8989, Acc: 0.6733, By Label: 0: 0.7973, 1: 0.2876\n",
      "Step: 400, Val NLL: 1.1073, Acc: 0.5800, By Label: 0: 0.6950, 1: 0.2467\n",
      "Step: 450, Val NLL: 1.1416, Acc: 0.5933, By Label: 0: 0.7104, 1: 0.2658\n",
      "Step: 500, Val NLL: 1.1372, Acc: 0.6200, By Label: 0: 0.7268, 1: 0.3452\n",
      "Step: 550, Val NLL: 1.0899, Acc: 0.6467, By Label: 0: 0.7899, 1: 0.2592\n",
      "Step: 600, Val NLL: 1.0686, Acc: 0.6200, By Label: 0: 0.7314, 1: 0.3333\n",
      "Step: 650, Val NLL: 1.2468, Acc: 0.6033, By Label: 0: 0.7053, 1: 0.3026\n",
      "Step: 700, Val NLL: 1.2153, Acc: 0.5933, By Label: 0: 0.7351, 1: 0.2098\n",
      "Step: 750, Val NLL: 1.2087, Acc: 0.5700, By Label: 0: 0.6985, 1: 0.2747\n",
      "Step: 800, Val NLL: 1.0931, Acc: 0.5933, By Label: 0: 0.7320, 1: 0.2747\n",
      "Step: 850, Val NLL: 1.0909, Acc: 0.6133, By Label: 0: 0.7731, 1: 0.2023\n",
      "Step: 900, Val NLL: 0.9876, Acc: 0.6767, By Label: 0: 0.8380, 1: 0.3\n",
      "Step: 950, Val NLL: 1.0360, Acc: 0.6333, By Label: 0: 0.7467, 1: 0.2388\n",
      "Step: 1000, Val NLL: 1.2623, Acc: 0.6033, By Label: 0: 0.7649, 1: 0.1807\n",
      "Step: 1050, Val NLL: 0.9999, Acc: 0.6400, By Label: 0: 0.7692, 1: 0.2784\n",
      "Step: 1100, Val NLL: 1.0177, Acc: 0.5900, By Label: 0: 0.7318, 1: 0.2\n",
      "Step: 1150, Val NLL: 1.2845, Acc: 0.6233, By Label: 0: 0.7942, 1: 0.2307\n",
      "Epoch: 0, Val NLL: 0.9291, Acc: 0.6147, By Label: 0: 0.7436, 1: 0.2626\n"
     ]
    }
   ],
   "source": [
    "loss, acc = run_validation(e, resnet, model_global, val_loader) # batch_size = 100, lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
