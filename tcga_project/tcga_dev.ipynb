{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import accimage\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms, set_image_backend, get_image_backend\n",
    "import data_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accimage'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/pytorch/accimage\n",
    "set_image_backend('accimage')\n",
    "get_image_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "TCGA_COAD_IMG_DIR = '/n/mounted-data-drive/COAD/'\n",
    "\n",
    "dirs = os.listdir(TCGA_COAD_IMG_DIR)\n",
    "imgs = [d[:-4] for d in dirs]\n",
    "current_img = TCGA_COAD_IMG_DIR + dirs[i] + '/' + imgs[i] + '_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/mounted-data-drive/COAD/TCGA-DM-A0XF-01Z-00-DX1.6FD3D3CF-A1E2-4F4E-BF02-F81B1A1061CC.svs/TCGA-DM-A0XF-01Z-00-DX1.6FD3D3CF-A1E2-4F4E-BF02-F81B1A1061CC_files'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/pytorch/examples/issues/236\n",
    "current_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101\n",
    "train_dir = current_img\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.class_to_idx['20.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,img in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py\n",
    "# https://pillow.readthedocs.io/en/5.1.x/handbook/concepts.html#concept-modes\n",
    "sample_annotations = {'TCGA-T9-A92H-01Z-00-DX3.1DE7D5ED-60F7-4645-8243-AB0C027B3ED7': 0, \n",
    "                      'TCGA-WS-AB45-01Z-00-DX1.1FD99E7A-830F-40DC-98CD-53C62C678AC6': 1,\n",
    "                      'TCGA-NH-A8F8-01Z-00-DX1.0C13D583-0BCE-44F7-A4E6-5994FE97B99C': 0,\n",
    "                      'TCGA-QG-A5YV-01Z-00-DX1.9B7FD3EA-D1AB-44B3-B728-820939EF56EA': 1,\n",
    "                      'TCGA-QG-A5YW-01Z-00-DX1.3242285F-FA82-4A92-9D0E-951013A3C91A': 0,\n",
    "                      'TCGA-QG-A5YX-01Z-00-DX1.28125B5A-B696-44AE-8A86-72E2CF7B9A6A': 1,\n",
    "                      'TCGA-QG-A5Z1-01Z-00-DX2.2CE72B6A-557F-43BD-BA4C-B252E14E46EF': 0,\n",
    "                      'TCGA-QG-A5Z2-01Z-00-DX2.F2352352-8F00-4BB3-8A62-8D1C1E374F95': 1,\n",
    "                      'TCGA-QL-A97D-01Z-00-DX1.6B48E95D-BE3C-4448-A1AF-6988C00B7AF1': 0,\n",
    "                      'TCGA-SS-A7HO-01Z-00-DX1.D20B9109-F984-40DE-A4F1-2DFC61002862': 1}\n",
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_utils.TCGADataset(sample_annotations, root_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_set.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['slide'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in train_loader:\n",
    "    print(s['slide'].shape, s['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#normalize = transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_path = '/home/sxchao/MSI_prediction/tcga_project/msi_raw_data.xlsx'\n",
    "msi_raw = pd.read_excel(msi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msi_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_raw.rename(columns={'Tumor type':'tumor_type', \n",
    "                        'Donor id':\"donor_id\", \n",
    "                        'Tumor sample id':'tumor_id',\n",
    "                        'Normal sample id':'normal_id',\n",
    "                        'Number of mutations A motif':'muts_A', \n",
    "                        'Number of covered A loci':'covg_A',\n",
    "                        'Number of mutations C motif':'muts_C', \n",
    "                        'Number of covered C loci':'covg_C',\n",
    "                        'Number of mutations AC motif':'muts_AC', \n",
    "                        'Number of covered AC loci':'covg_AC',\n",
    "                        'Number of mutations AG motif':'muts_AG', \n",
    "                        'Number of covered AG loci':'covg_AG'}, \n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = 50\n",
    "\n",
    "msi_raw['muts_tot'] = msi_raw['muts_A'] + msi_raw['muts_C'] + msi_raw['muts_AC'] + msi_raw['muts_AG']\n",
    "msi_raw['msi'] = msi_raw['muts_tot'] > cut_off\n",
    "msi_raw.msi = msi_raw.msi.astype(int)\n",
    "#msi_raw.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msi_raw.groupby('tumor_type')['msi'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_msi = msi_raw.loc[msi_raw['tumor_type']=='COAD','donor_id'].values\n",
    "#coad_msi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = coad_msi[-1]\n",
    "name_len = len(sample_name)\n",
    "#sample_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_full_name = os.listdir(root_dir)\n",
    "coad_img = np.array([v[0:name_len] for v in coad_full_name])\n",
    "#len(coad_img), coad_img[5], coad_full_name[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_both = np.intersect1d(coad_img, coad_msi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = []\n",
    "for sample in coad_both:\n",
    "    if sample != 'TCGA-A6-2675': # 5.0 empty for 'TCGA-A6-2675-01Z-00-DX1.d37847d6-c17f-44b9-b90a-84cd1946c8ab'\n",
    "        key = np.argwhere(coad_img == sample).squeeze()\n",
    "        if key.size != 0:\n",
    "            sample_names.append(coad_full_name[key][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_raw.set_index('donor_id', inplace=True)\n",
    "#msi_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed=29)\n",
    "reorder = np.random.permutation(len(sample_names))\n",
    "train = reorder[:int(np.floor(len(sample_names)*0.8))]\n",
    "val = reorder[int(np.floor(len(sample_names)*0.8)):]\n",
    "len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = np.array(sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annotations = {}\n",
    "for sample_name in sample_names[train]:\n",
    "    sample_annotations[sample_name] = msi_raw.loc[sample_name[0:name_len], 'msi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19387755102040816"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_coad = list(sample_annotations.values())\n",
    "sum(all_coad) / len(all_coad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_utils.TCGADataset(sample_annotations, root_dir, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annotations_val = {}\n",
    "for sample_name in sample_names[val]:\n",
    "    sample_annotations_val[sample_name] = msi_raw.loc[sample_name[0:name_len], 'msi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_coad_val = list(sample_annotations_val.values())\n",
    "sum(all_coad_val) / len(all_coad_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = data_utils.TCGADataset(sample_annotations_val, root_dir, transform=transform)\n",
    "valid_loader = DataLoader(valid_set, batch_size=1, shuffle=False, pin_memory=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annotations_dev = {}\n",
    "for sample_name in sample_names[9:14]:\n",
    "    sample_annotations_dev[sample_name] = msi_raw.loc[sample_name[0:name_len], 'msi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_coad_dev = list(sample_annotations_dev.values())\n",
    "sum(all_coad_dev) / len(all_coad_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = data_utils.TCGADataset(sample_annotations_dev, root_dir, transform=transform)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=True, pin_memory=True, num_workers=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inception_v3 expects tensors with a size of N x 3 x 299 x 299\n",
    "#net = models.inception_v3(pretrained=True)\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Linear(2048,2,bias=True)\n",
    "#for param in resnet.parameters():\n",
    "#    param.requires_grad = False\n",
    "resnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tile_shape(H_in, W_in, kernel_size, dilation=1., padding=0., stride=1.):\n",
    "    H_out = (H_in + 2. * padding - dilation * (kernel_size - 1) - 1)/stride + 1\n",
    "    W_out = (W_in + 2. * padding - dilation * (kernel_size - 1) - 1)/stride + 1\n",
    "    return int(np.floor(H_out)), int(np.floor(W_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=0.5,\n",
    "                dilation = 1., padding = 0, H_in = 256, W_in = 256):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.n_conv_layers = n_conv_layers\n",
    "        self.n_fc_layers = n_fc_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_conv_filters = n_conv_filters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.mp_ker = 2 # max pool kernel size\n",
    "        self.mp_str = 2 # max pool stride\n",
    "        self.m = nn.MaxPool2d(self.mp_ker, stride=self.mp_str)\n",
    "        self.n = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.H_in, self.W_in = H_in, W_in\n",
    "        \n",
    "        in_channels = 3        \n",
    "        for layer in range(self.n_conv_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, self.n_conv_filters[layer], self.kernel_size[layer]))\n",
    "            self.conv_layers.append(self.relu)\n",
    "            self.conv_layers.append(self.m)\n",
    "            # convolution\n",
    "            self.H_in, self.W_in = update_tile_shape(self.H_in, self.W_in, kernel_size[layer])\n",
    "            # max pooling\n",
    "            self.H_in, self.W_in = update_tile_shape(self.H_in, self.W_in, self.mp_ker, stride=self.mp_str)\n",
    "            in_channels = self.n_conv_filters[layer]\n",
    "        in_channels = in_channels * self.H_in * self.W_in\n",
    "        for layer in range(self.n_fc_layers):\n",
    "            self.fc_layers.append(nn.Linear(in_channels, self.hidden_size[layer]))\n",
    "            self.fc_layers.append(self.relu)\n",
    "            self.fc_layers.append(self.n)\n",
    "            in_channels = self.hidden_size[layer]\n",
    "        self.conv = nn.Sequential(*self.conv_layers)\n",
    "        self.fc = nn.Sequential(*self.fc_layers)\n",
    "        self.classification_layer = nn.Linear(in_channels, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.conv(x)\n",
    "        embed = embed.view(x.shape[0],-1)\n",
    "        y = self.fc(embed)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conv_layers = 2\n",
    "n_fc_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = [512,512]\n",
    "dropout = 0.5\n",
    "net = ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slide,label in train_loader:\n",
    "    slide = slide.squeeze(0).cuda()\n",
    "    embed = resnet(slide)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 3, 256, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slide = slide[:,np.random.permutation(slide.shape[1])[:200],:,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([165, 1000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = net(slide)\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = pool_fn(embed).unsqueeze(0)\n",
    "pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net.classification_layer(pool)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_training_loop(e, train_loader, net, criterion, optimizer, pool_fn):\n",
    "    net.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx,(slide,label) in enumerate(train_loader):\n",
    "        #slide = slide[:,np.random.permutation(slide.shape[1])[:100],:,:,:].squeeze(0)\n",
    "        slide, label = slide.squeeze(0).cuda(), label.cuda()\n",
    "        output = net(slide)\n",
    "        output = pool_fn(output).unsqueeze(0)\n",
    "        #output = net.classification_layer(pool)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        #if idx % 10 == 0:\n",
    "        #    print('Epoch: {0}, Slide: {1}, Train NLL: {2:0.4f}'.format(e, idx, loss))\n",
    "            \n",
    "    print('Epoch: {0}, Avg Train NLL: {1:0.4f}'.format(e, total_loss/float(idx+1)))\n",
    "    \n",
    "\n",
    "def embedding_validation_loop(e, valid_loader, net, criterion, pool_fn, dataset='Val', scheduler=None):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    \n",
    "    for idx,(slide,label) in enumerate(valid_loader):\n",
    "        #slide = slide[:,np.random.permutation(slide.shape[1])[:100],:,:,:].squeeze(0)\n",
    "        slide, label = slide.squeeze(0).cuda(), label.cuda()\n",
    "        output = net(slide)\n",
    "        output = pool_fn(output).unsqueeze(0)\n",
    "        #output = net.classification_layer(pool)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        labels.extend(label.float().cpu().numpy())\n",
    "        preds.append(torch.argmax(output).float().detach().cpu().numpy())\n",
    "    \n",
    "        #if idx % 10 == 0:\n",
    "        #    print('Epoch: {0}, Slide: {1}, {3} NLL: {2:0.4f}'.format(e, idx, loss, dataset))\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        scheduler.step(total_loss)\n",
    "            \n",
    "    acc = np.mean(np.array(labels) == np.array(preds))\n",
    "    print('Epoch: {0}, Avg {3} NLL: {1:0.4f}, {3} Acc: {2:0.4f}'.format(e, total_loss/float(idx+1), acc, dataset))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.tensor([1.0,6.0],device='cuda'))\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- LR: 0.01000 ----------\n",
      "Epoch: 0, Avg Train NLL: 0.5352\n",
      "Epoch: 0, Avg Val NLL: 0.6510, Val Acc: 0.6800\n",
      "Epoch: 1, Avg Train NLL: 0.5352\n",
      "Epoch: 1, Avg Val NLL: 0.6332, Val Acc: 0.7400\n",
      "Epoch: 2, Avg Train NLL: 0.5358\n",
      "Epoch: 2, Avg Val NLL: 0.6344, Val Acc: 0.7400\n",
      "Epoch: 3, Avg Train NLL: 0.5351\n",
      "Epoch: 3, Avg Val NLL: 0.6285, Val Acc: 0.7400\n",
      "Epoch: 4, Avg Train NLL: 0.5352\n",
      "Epoch: 4, Avg Val NLL: 0.6412, Val Acc: 0.7400\n",
      "Epoch: 5, Avg Train NLL: 0.5359\n",
      "Epoch: 5, Avg Val NLL: 0.6168, Val Acc: 0.7400\n",
      "Epoch: 6, Avg Train NLL: 0.5360\n",
      "Epoch: 6, Avg Val NLL: 0.6253, Val Acc: 0.7400\n",
      "Epoch: 7, Avg Train NLL: 0.5352\n",
      "Epoch: 7, Avg Val NLL: 0.6316, Val Acc: 0.7400\n",
      "Epoch: 8, Avg Train NLL: 0.5356\n",
      "Epoch: 8, Avg Val NLL: 0.6239, Val Acc: 0.7400\n",
      "Epoch: 9, Avg Train NLL: 0.5353\n",
      "Epoch: 9, Avg Val NLL: 0.6349, Val Acc: 0.7200\n",
      "---------- LR: 0.01000 ----------\n",
      "Epoch: 10, Avg Train NLL: 0.5349\n",
      "Epoch: 10, Avg Val NLL: 0.6384, Val Acc: 0.7800\n",
      "Epoch: 11, Avg Train NLL: 0.5356\n",
      "Epoch: 11, Avg Val NLL: 0.6263, Val Acc: 0.7400\n",
      "Epoch: 12, Avg Train NLL: 0.5350\n",
      "Epoch: 12, Avg Val NLL: 0.6216, Val Acc: 0.7400\n",
      "Epoch: 13, Avg Train NLL: 0.5360\n",
      "Epoch: 13, Avg Val NLL: 0.6050, Val Acc: 0.7600\n",
      "Epoch: 14, Avg Train NLL: 0.5346\n",
      "Epoch: 14, Avg Val NLL: 0.6307, Val Acc: 0.7400\n",
      "Epoch: 15, Avg Train NLL: 0.5354\n",
      "Epoch: 15, Avg Val NLL: 0.6358, Val Acc: 0.7400\n",
      "Epoch: 16, Avg Train NLL: 0.5350\n",
      "Epoch: 16, Avg Val NLL: 0.6553, Val Acc: 0.6600\n",
      "Epoch: 17, Avg Train NLL: 0.5357\n",
      "Epoch: 17, Avg Val NLL: 0.6503, Val Acc: 0.7400\n",
      "Epoch: 18, Avg Train NLL: 0.5358\n",
      "Epoch: 18, Avg Val NLL: 0.6476, Val Acc: 0.7400\n",
      "Epoch: 19, Avg Train NLL: 0.5357\n",
      "Epoch: 19, Avg Val NLL: 0.6439, Val Acc: 0.7200\n",
      "---------- LR: 0.01000 ----------\n",
      "Epoch: 20, Avg Train NLL: 0.5352\n",
      "Epoch: 20, Avg Val NLL: 0.6346, Val Acc: 0.7400\n",
      "Epoch: 21, Avg Train NLL: 0.5353\n",
      "Epoch: 21, Avg Val NLL: 0.6508, Val Acc: 0.7200\n",
      "Epoch: 22, Avg Train NLL: 0.5349\n",
      "Epoch: 22, Avg Val NLL: 0.6083, Val Acc: 0.7400\n",
      "Epoch: 23, Avg Train NLL: 0.5354\n",
      "Epoch: 23, Avg Val NLL: 0.6143, Val Acc: 0.7400\n",
      "Epoch: 24, Avg Train NLL: 0.5353\n",
      "Epoch: 24, Avg Val NLL: 0.6156, Val Acc: 0.7400\n",
      "Epoch: 25, Avg Train NLL: 0.5354\n",
      "Epoch: 25, Avg Val NLL: 0.6340, Val Acc: 0.7600\n",
      "Epoch: 26, Avg Train NLL: 0.5356\n",
      "Epoch: 26, Avg Val NLL: 0.6221, Val Acc: 0.7400\n",
      "Epoch: 27, Avg Train NLL: 0.5353\n",
      "Epoch: 27, Avg Val NLL: 0.6226, Val Acc: 0.7200\n",
      "Epoch: 28, Avg Train NLL: 0.5354\n",
      "Epoch: 28, Avg Val NLL: 0.6311, Val Acc: 0.7600\n",
      "Epoch: 29, Avg Train NLL: 0.5357\n",
      "Epoch: 29, Avg Val NLL: 0.6219, Val Acc: 0.7400\n",
      "---------- LR: 0.01000 ----------\n",
      "Epoch: 30, Avg Train NLL: 0.5354\n",
      "Epoch: 30, Avg Val NLL: 0.6508, Val Acc: 0.7400\n",
      "Epoch: 31, Avg Train NLL: 0.5355\n",
      "Epoch: 31, Avg Val NLL: 0.6136, Val Acc: 0.7400\n",
      "Epoch: 32, Avg Train NLL: 0.5356\n",
      "Epoch: 32, Avg Val NLL: 0.6369, Val Acc: 0.7600\n",
      "Epoch: 33, Avg Train NLL: 0.5355\n",
      "Epoch: 33, Avg Val NLL: 0.6417, Val Acc: 0.7600\n",
      "Epoch: 34, Avg Train NLL: 0.5360\n",
      "Epoch: 34, Avg Val NLL: 0.6142, Val Acc: 0.7400\n",
      "Epoch: 35, Avg Train NLL: 0.5351\n",
      "Epoch: 35, Avg Val NLL: 0.6639, Val Acc: 0.6800\n",
      "Epoch: 36, Avg Train NLL: 0.5348\n",
      "Epoch: 36, Avg Val NLL: 0.6269, Val Acc: 0.7400\n",
      "Epoch: 37, Avg Train NLL: 0.5347\n",
      "Epoch: 37, Avg Val NLL: 0.6093, Val Acc: 0.7600\n",
      "Epoch: 38, Avg Train NLL: 0.5349\n",
      "Epoch: 38, Avg Val NLL: 0.6398, Val Acc: 0.7400\n",
      "Epoch: 39, Avg Train NLL: 0.5355\n",
      "Epoch: 39, Avg Val NLL: 0.6270, Val Acc: 0.7400\n",
      "---------- LR: 0.01000 ----------\n",
      "Epoch: 40, Avg Train NLL: 0.5353\n",
      "Epoch: 40, Avg Val NLL: 0.6443, Val Acc: 0.7200\n",
      "Epoch: 41, Avg Train NLL: 0.5356\n",
      "Epoch: 41, Avg Val NLL: 0.6017, Val Acc: 0.7600\n",
      "Epoch: 42, Avg Train NLL: 0.5357\n",
      "Epoch: 42, Avg Val NLL: 0.6252, Val Acc: 0.7400\n",
      "Epoch: 43, Avg Train NLL: 0.5349\n",
      "Epoch: 43, Avg Val NLL: 0.6333, Val Acc: 0.7400\n",
      "Epoch: 44, Avg Train NLL: 0.5353\n",
      "Epoch: 44, Avg Val NLL: 0.6362, Val Acc: 0.7200\n",
      "Epoch: 45, Avg Train NLL: 0.5358\n",
      "Epoch: 45, Avg Val NLL: 0.6137, Val Acc: 0.7600\n",
      "Epoch: 46, Avg Train NLL: 0.5347\n",
      "Epoch: 46, Avg Val NLL: 0.6189, Val Acc: 0.7400\n",
      "Epoch: 47, Avg Train NLL: 0.5355\n",
      "Epoch: 47, Avg Val NLL: 0.6605, Val Acc: 0.6600\n",
      "Epoch: 48, Avg Train NLL: 0.5349\n",
      "Epoch: 48, Avg Val NLL: 0.6603, Val Acc: 0.7200\n",
      "Epoch: 49, Avg Train NLL: 0.5353\n",
      "Epoch: 49, Avg Val NLL: 0.6201, Val Acc: 0.7400\n",
      "---------- LR: 0.01000 ----------\n",
      "Epoch: 50, Avg Train NLL: 0.5356\n",
      "Epoch: 50, Avg Val NLL: 0.6644, Val Acc: 0.6800\n",
      "Epoch: 51, Avg Train NLL: 0.5358\n",
      "Epoch: 51, Avg Val NLL: 0.6567, Val Acc: 0.7200\n",
      "Epoch: 52, Avg Train NLL: 0.5357\n",
      "Epoch: 52, Avg Val NLL: 0.6548, Val Acc: 0.7400\n",
      "Epoch: 53, Avg Train NLL: 0.5350\n",
      "Epoch: 53, Avg Val NLL: 0.6164, Val Acc: 0.7400\n",
      "Epoch: 54, Avg Train NLL: 0.5357\n",
      "Epoch: 54, Avg Val NLL: 0.6122, Val Acc: 0.7400\n",
      "Epoch: 55, Avg Train NLL: 0.5356\n",
      "Epoch: 55, Avg Val NLL: 0.6263, Val Acc: 0.7400\n",
      "Epoch: 56, Avg Train NLL: 0.5351\n",
      "Epoch: 56, Avg Val NLL: 0.6109, Val Acc: 0.7600\n",
      "Epoch: 57, Avg Train NLL: 0.5347\n",
      "Epoch: 57, Avg Val NLL: 0.6279, Val Acc: 0.7400\n",
      "Epoch: 58, Avg Train NLL: 0.5359\n",
      "Epoch: 58, Avg Val NLL: 0.6267, Val Acc: 0.7200\n",
      "Epoch: 59, Avg Train NLL: 0.5355\n",
      "Epoch: 59, Avg Val NLL: 0.6406, Val Acc: 0.7400\n",
      "---------- LR: 0.01000 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e94d684936ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#embedding_training_loop(e, dev_loader, net, criterion, optimizer, pool_fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#dev_loss = embedding_validation_loop(e, dev_loader, net, criterion, pool_fn, dataset='Dev', scheduler=scheduler)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0membedding_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#train_loss = embedding_validation_loop(e, train_loader, net, criterion, pool_fn, dataset='Train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_validation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-fa818ca8cc62>\u001b[0m in \u001b[0;36membedding_training_loop\u001b[0;34m(e, train_loader, net, criterion, optimizer, pool_fn)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "for e in range(0,200):\n",
    "    if e % 10 == 0:\n",
    "        print('---------- LR: {0:0.5f} ----------'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    #embedding_training_loop(e, dev_loader, net, criterion, optimizer, pool_fn)\n",
    "    #dev_loss = embedding_validation_loop(e, dev_loader, net, criterion, pool_fn, dataset='Dev', scheduler=scheduler)\n",
    "    embedding_training_loop(e, train_loader, resnet, criterion, optimizer, pool_fn)\n",
    "    #train_loss = embedding_validation_loop(e, train_loader, net, criterion, pool_fn, dataset='Train')\n",
    "    val_loss = embedding_validation_loop(e, valid_loader, resnet, criterion, pool_fn, dataset='Val', scheduler=scheduler)\n",
    "    #loss_history.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(slide,label) in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, slide.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11689512"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx,(slide,label) in enumerate(valid_loader):\n",
    "    print(idx, slide.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tiles = 100\n",
    "\n",
    "all_slides = []\n",
    "all_slide_labels = []\n",
    "\n",
    "for idx,sample_name in enumerate(sample_names):\n",
    "    sample_name = sample_names[-1]\n",
    "    slide_tiles = []\n",
    "    img_dir = root_dir + sample_name + '.svs/' + sample_name + '_files/5.0'\n",
    "    imgs = os.listdir(img_dir)\n",
    "\n",
    "    for im in imgs:\n",
    "        path = img_dir + '/' + im\n",
    "        image = data_utils.accimage_loader(path)\n",
    "\n",
    "        if transform is not None:\n",
    "            image = transform(image)\n",
    "\n",
    "        if image.shape[1] == 256 and image.shape[2] == 256:\n",
    "            slide_tiles.append(image)\n",
    "\n",
    "    slide = torch.stack(slide_tiles)\n",
    "    label = msi_raw.loc[sample_name[0:name_len], 'msi']\n",
    "    slide = slide[np.random.permutation(slide.shape[0])[:max_tiles],:,:,:]\n",
    "\n",
    "    all_slides.append(slide)\n",
    "    all_slide_labels.append(label)\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        print(idx, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_COAD_TRAIN = '/n/tcga_coad_train.pkl'\n",
    "TCGA_COAD_VALID = '/n/tcga_coad_valid.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_slides[0].shape, all_slide_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TCGA_COAD_TRAIN, 'wb') as f: \n",
    "    pickle.dump([all_slides[:196], all_slide_labels[:196]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TCGA_COAD_VALID, 'wb') as f: \n",
    "    pickle.dump([all_slides[196:], all_slide_labels[196:]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCGADataset_from_pkl(Dataset):\n",
    "    def __init__(self, pickle_file):\n",
    "        with open(pickle_file, 'rb') as f: \n",
    "            all_slides, all_slide_labels = pickle.load(f)\n",
    "        self.data = all_slides\n",
    "        self.labels = all_slide_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TCGADataset_from_pkl(TCGA_COAD_TRAIN)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True, pin_memory=True)\n",
    "valid = TCGADataset_from_pkl(TCGA_COAD_VALID)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = os.listdir(img_dir)\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
