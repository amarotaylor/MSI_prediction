{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import accimage\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms, set_image_backend, get_image_backend\n",
    "import data_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accimage'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/pytorch/accimage\n",
    "set_image_backend('accimage')\n",
    "get_image_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "TCGA_COAD_IMG_DIR = '/n/mounted-data-drive/COAD/'\n",
    "\n",
    "dirs = os.listdir(TCGA_COAD_IMG_DIR)\n",
    "imgs = [d[:-4] for d in dirs]\n",
    "current_img = TCGA_COAD_IMG_DIR + dirs[i] + '/' + imgs[i] + '_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/issues/236\n",
    "current_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101\n",
    "train_dir = current_img\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.class_to_idx['20.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,img in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py\n",
    "# https://pillow.readthedocs.io/en/5.1.x/handbook/concepts.html#concept-modes\n",
    "sample_annotations = {'TCGA-T9-A92H-01Z-00-DX3.1DE7D5ED-60F7-4645-8243-AB0C027B3ED7': 0, \n",
    "                      'TCGA-WS-AB45-01Z-00-DX1.1FD99E7A-830F-40DC-98CD-53C62C678AC6': 1,\n",
    "                      'TCGA-NH-A8F8-01Z-00-DX1.0C13D583-0BCE-44F7-A4E6-5994FE97B99C': 0,\n",
    "                      'TCGA-QG-A5YV-01Z-00-DX1.9B7FD3EA-D1AB-44B3-B728-820939EF56EA': 1,\n",
    "                      'TCGA-QG-A5YW-01Z-00-DX1.3242285F-FA82-4A92-9D0E-951013A3C91A': 0,\n",
    "                      'TCGA-QG-A5YX-01Z-00-DX1.28125B5A-B696-44AE-8A86-72E2CF7B9A6A': 1,\n",
    "                      'TCGA-QG-A5Z1-01Z-00-DX2.2CE72B6A-557F-43BD-BA4C-B252E14E46EF': 0,\n",
    "                      'TCGA-QG-A5Z2-01Z-00-DX2.F2352352-8F00-4BB3-8A62-8D1C1E374F95': 1,\n",
    "                      'TCGA-QL-A97D-01Z-00-DX1.6B48E95D-BE3C-4448-A1AF-6988C00B7AF1': 0,\n",
    "                      'TCGA-SS-A7HO-01Z-00-DX1.D20B9109-F984-40DE-A4F1-2DFC61002862': 1}\n",
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_utils.TCGADataset(sample_annotations, root_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_set.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['slide'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in train_loader:\n",
    "    print(s['slide'].shape, s['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_path = '/home/sxchao/MSI_prediction/tcga_project/msi_raw_data.xlsx'\n",
    "msi_raw = pd.read_excel(msi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msi_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_raw.rename(columns={'Tumor type':'tumor_type', \n",
    "                        'Donor id':\"donor_id\", \n",
    "                        'Tumor sample id':'tumor_id',\n",
    "                        'Normal sample id':'normal_id',\n",
    "                        'Number of mutations A motif':'muts_A', \n",
    "                        'Number of covered A loci':'covg_A',\n",
    "                        'Number of mutations C motif':'muts_C', \n",
    "                        'Number of covered C loci':'covg_C',\n",
    "                        'Number of mutations AC motif':'muts_AC', \n",
    "                        'Number of covered AC loci':'covg_AC',\n",
    "                        'Number of mutations AG motif':'muts_AG', \n",
    "                        'Number of covered AG loci':'covg_AG'}, \n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_raw['muts_tot'] = msi_raw['muts_A'] + msi_raw['muts_C'] + msi_raw['muts_AC'] + msi_raw['muts_AG']\n",
    "msi_raw['msi'] = msi_raw['muts_tot'] >= 20\n",
    "msi_raw.msi = msi_raw.msi.astype(int)\n",
    "#msi_raw.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msi_raw.groupby('tumor_type')['msi'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_msi = msi_raw.loc[msi_raw['tumor_type']=='COAD','donor_id'].values\n",
    "#coad_msi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = coad_msi[-1]\n",
    "name_len = len(sample_name)\n",
    "#sample_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_full_name = os.listdir(root_dir)\n",
    "coad_img = np.array([v[0:name_len] for v in coad_full_name])\n",
    "#len(coad_img), coad_img[5], coad_full_name[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_both = np.intersect1d(coad_img, coad_msi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = []\n",
    "for sample in coad_both:\n",
    "    key = np.argwhere(coad_img == sample).squeeze()\n",
    "    if key.size != 0:\n",
    "        sample_names.append(coad_full_name[key][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_raw.set_index('donor_id', inplace=True)\n",
    "#msi_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=54321)\n",
    "reorder = np.random.permutation(len(sample_names))\n",
    "train = reorder[:int(np.floor(len(sample_names)*0.7))]\n",
    "val = reorder[int(np.floor(len(sample_names)*0.7)):]\n",
    "#len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = np.array(sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annotations = {}\n",
    "for sample_name in sample_names[train]:\n",
    "    sample_annotations[sample_name] = msi_raw.loc[sample_name[0:name_len], 'msi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4127906976744186"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_coad = list(sample_annotations.values())\n",
    "sum(all_coad) / len(all_coad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "train_set = data_utils.TCGADataset(sample_annotations, root_dir, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annotations_val = {}\n",
    "for sample_name in sample_names[val]:\n",
    "    sample_annotations_val[sample_name] = msi_raw.loc[sample_name[0:name_len], 'msi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_coad_val = list(sample_annotations_val.values())\n",
    "sum(all_coad_val) / len(all_coad_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = data_utils.TCGADataset(sample_annotations_val, root_dir, transform=transform)\n",
    "valid_loader = DataLoader(valid_set, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception_v3 expects tensors with a size of N x 3 x 299 x 299\n",
    "#net = models.inception_v3(pretrained=True)\n",
    "net = models.resnet152(pretrained=True)\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tile_shape(H_in, W_in, kernel_size, dilation=1., padding=0., stride=1.):\n",
    "    H_out = (H_in + 2. * padding - dilation * (kernel_size-1) -1)/stride + 1\n",
    "    W_out = (W_in + 2. * padding - dilation * (kernel_size-1) -1)/stride + 1\n",
    "    return int(np.floor(H_out)), int(np.floor(W_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=0.5,\n",
    "                dilation = 1., padding = 0, H_in = 256, W_in = 256):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.n_conv_layers = n_conv_layers\n",
    "        self.n_fc_layers = n_fc_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_conv_filters = n_conv_filters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.mp_ker = 16 # max pool kernel size\n",
    "        self.mp_str = 16 # max pool stride\n",
    "        self.m = nn.MaxPool2d(self.mp_ker, stride=self.mp_str)\n",
    "        self.n = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.H_in, self.W_in = H_in, W_in\n",
    "        \n",
    "        in_channels = 3        \n",
    "        for layer in range(self.n_conv_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, self.n_conv_filters[layer], self.kernel_size[layer]))\n",
    "            self.conv_layers.append(self.relu)\n",
    "            self.conv_layers.append(self.m)\n",
    "            # convolution\n",
    "            self.H_in, self.W_in = update_tile_shape(self.H_in, self.W_in, kernel_size[layer])\n",
    "            # max pooling\n",
    "            self.H_in, self.W_in = update_tile_shape(self.H_in, self.W_in, self.mp_ker, stride=self.mp_str)\n",
    "            in_channels = self.n_conv_filters[layer]\n",
    "        in_channels = in_channels * self.H_in * self.W_in\n",
    "        for layer in range(self.n_fc_layers):\n",
    "            self.fc_layers.append(nn.Linear(in_channels, self.hidden_size[layer]))\n",
    "            self.fc_layers.append(self.relu)\n",
    "            self.fc_layers.append(self.n)\n",
    "            in_channels = self.hidden_size[layer]\n",
    "        self.conv = nn.Sequential(*self.conv_layers)\n",
    "        self.fc = nn.Sequential(*self.fc_layers)\n",
    "        self.classification_layer = nn.Linear(in_channels, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.conv(x)\n",
    "        embed = embed.view(x.shape[0],-1)\n",
    "        y = self.fc(embed)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (m): MaxPool2d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n",
       "  (n): Dropout(p=0.5)\n",
       "  (relu): ReLU()\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 5, kernel_size=(64, 64), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=720, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "  )\n",
       "  (classification_layer): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_conv_layers = 1\n",
    "n_fc_layers = 1\n",
    "kernel_size = [64]\n",
    "n_conv_filters = [5]\n",
    "hidden_size = [256]\n",
    "dropout = 0.5\n",
    "net = ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slide,label in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide = slide[:,np.random.permutation(slide.shape[1])[:200],:,:,:]\n",
    "slide = slide.squeeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = net(slide)\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = pool_fn(embed).unsqueeze(0)\n",
    "pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net.classification_layer(pool)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_training_loop(e, train_loader, net, criterion, optimizer, pool_fn):\n",
    "    net.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx,(slide,label) in enumerate(train_loader):\n",
    "        slide = slide[:,np.random.permutation(slide.shape[1])[:100],:,:,:].squeeze(0)\n",
    "        slide, label = slide.cuda(), label.cuda()\n",
    "        output = net(slide)\n",
    "        pool = pool_fn(output).unsqueeze(0)\n",
    "        output = net.classification_layer(pool)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if idx % 10 == 0:\n",
    "            print('Epoch: {0}, Slide: {1}, Train NLL: {2:0.4f}'.format(e, idx, loss))\n",
    "            \n",
    "    print('Epoch: {0}, Train NLL: {1:0.4f}'.format(e, total_loss))\n",
    "    \n",
    "\n",
    "def embedding_validation_loop(e, valid_loader, net, criterion, pool_fn, dataset='Val'):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    \n",
    "    for idx,(slide,label) in enumerate(valid_loader):\n",
    "        slide = slide[:,np.random.permutation(slide.shape[1])[:100],:,:,:].squeeze(0)\n",
    "        slide, label = slide.cuda(), label.cuda()\n",
    "        output = net(slide)\n",
    "        pool = pool_fn(output).unsqueeze(0)\n",
    "        output = net.classification_layer(pool)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        labels.extend(label.float().cpu().numpy())\n",
    "        preds.append(torch.argmax(output).float().detach().cpu().numpy())\n",
    "    \n",
    "        if idx % 10 == 0:\n",
    "            print('Epoch: {0}, Slide: {1}, {3} NLL: {2:0.4f}'.format(e, idx, loss, dataset))\n",
    "            \n",
    "    acc = np.mean(np.array(labels) == np.array(preds))\n",
    "    print('Epoch: {0}, {3} NLL: {1:0.4f}, {3} Acc: {2:0.4f}'.format(e, total_loss, acc, dataset))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Slide: 0, Train NLL: 0.8545\n",
      "Epoch: 0, Slide: 10, Train NLL: 0.7570\n",
      "Epoch: 0, Slide: 20, Train NLL: 0.6780\n",
      "Epoch: 0, Slide: 30, Train NLL: 0.6625\n",
      "Epoch: 0, Slide: 40, Train NLL: 0.7246\n",
      "Epoch: 0, Slide: 50, Train NLL: 0.6867\n",
      "Epoch: 0, Slide: 60, Train NLL: 0.6680\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    embedding_training_loop(e, train_loader, net, criterion, optimizer, pool_fn)\n",
    "    train_loss = embedding_validation_loop(e, train_loader, net, criterion, pool_fn, dataset='Train')\n",
    "    val_loss = embedding_validation_loop(e, valid_loader, net, criterion, pool_fn, dataset='Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(slide,label) in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, slide.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246535"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
