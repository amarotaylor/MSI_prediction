{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import accimage\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms, set_image_backend, get_image_backend\n",
    "import data_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accimage'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/pytorch/accimage\n",
    "set_image_backend('accimage')\n",
    "get_image_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "TCGA_COAD_IMG_DIR = '/n/mounted-data-drive/COAD/'\n",
    "\n",
    "dirs = os.listdir(TCGA_COAD_IMG_DIR)\n",
    "imgs = [d[:-4] for d in dirs]\n",
    "current_img = TCGA_COAD_IMG_DIR + dirs[i] + '/' + imgs[i] + '_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/issues/236\n",
    "current_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101\n",
    "train_dir = current_img\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.class_to_idx['20.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,img in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py\n",
    "# https://pillow.readthedocs.io/en/5.1.x/handbook/concepts.html#concept-modes\n",
    "sample_annotations = {'TCGA-T9-A92H-01Z-00-DX3.1DE7D5ED-60F7-4645-8243-AB0C027B3ED7': 0, \n",
    "                      'TCGA-WS-AB45-01Z-00-DX1.1FD99E7A-830F-40DC-98CD-53C62C678AC6': 1,\n",
    "                      'TCGA-NH-A8F8-01Z-00-DX1.0C13D583-0BCE-44F7-A4E6-5994FE97B99C': 0,\n",
    "                      'TCGA-QG-A5YV-01Z-00-DX1.9B7FD3EA-D1AB-44B3-B728-820939EF56EA': 1,\n",
    "                      'TCGA-QG-A5YW-01Z-00-DX1.3242285F-FA82-4A92-9D0E-951013A3C91A': 0,\n",
    "                      'TCGA-QG-A5YX-01Z-00-DX1.28125B5A-B696-44AE-8A86-72E2CF7B9A6A': 1,\n",
    "                      'TCGA-QG-A5Z1-01Z-00-DX2.2CE72B6A-557F-43BD-BA4C-B252E14E46EF': 0,\n",
    "                      'TCGA-QG-A5Z2-01Z-00-DX2.F2352352-8F00-4BB3-8A62-8D1C1E374F95': 1,\n",
    "                      'TCGA-QL-A97D-01Z-00-DX1.6B48E95D-BE3C-4448-A1AF-6988C00B7AF1': 0,\n",
    "                      'TCGA-SS-A7HO-01Z-00-DX1.D20B9109-F984-40DE-A4F1-2DFC61002862': 1}\n",
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_utils.TCGADataset(sample_annotations, root_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_set.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['slide'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in train_loader:\n",
    "    print(s['slide'].shape, s['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "normalize = transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_path = '/home/sxchao/MSI_prediction/tcga_project/msi_raw_data.xlsx'\n",
    "msi_raw = pd.read_excel(msi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msi_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_raw.rename(columns={'Tumor type':'tumor_type', \n",
    "                        'Donor id':\"donor_id\", \n",
    "                        'Tumor sample id':'tumor_id',\n",
    "                        'Normal sample id':'normal_id',\n",
    "                        'Number of mutations A motif':'muts_A', \n",
    "                        'Number of covered A loci':'covg_A',\n",
    "                        'Number of mutations C motif':'muts_C', \n",
    "                        'Number of covered C loci':'covg_C',\n",
    "                        'Number of mutations AC motif':'muts_AC', \n",
    "                        'Number of covered AC loci':'covg_AC',\n",
    "                        'Number of mutations AG motif':'muts_AG', \n",
    "                        'Number of covered AG loci':'covg_AG'}, \n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_raw['muts_tot'] = msi_raw['muts_A'] + msi_raw['muts_C'] + msi_raw['muts_AC'] + msi_raw['muts_AG']\n",
    "msi_raw['msi'] = msi_raw['muts_tot'] >= 20\n",
    "msi_raw.msi = msi_raw.msi.astype(int)\n",
    "#msi_raw.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msi_raw.groupby('tumor_type')['msi'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_msi = msi_raw.loc[msi_raw['tumor_type']=='COAD','donor_id'].values\n",
    "#coad_msi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = coad_msi[-1]\n",
    "name_len = len(sample_name)\n",
    "#sample_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_full_name = os.listdir(root_dir)\n",
    "coad_img = np.array([v[0:name_len] for v in coad_full_name])\n",
    "#len(coad_img), coad_img[5], coad_full_name[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "coad_both = np.intersect1d(coad_img, coad_msi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = []\n",
    "for sample in coad_both:\n",
    "    if sample != 'TCGA-A6-2675': # 5.0 empty for 'TCGA-A6-2675-01Z-00-DX1.d37847d6-c17f-44b9-b90a-84cd1946c8ab'\n",
    "        key = np.argwhere(coad_img == sample).squeeze()\n",
    "        if key.size != 0:\n",
    "            sample_names.append(coad_full_name[key][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_raw.set_index('donor_id', inplace=True)\n",
    "#msi_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 50)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.seed(seed=54321)\n",
    "reorder = np.random.permutation(len(sample_names))\n",
    "train = reorder[:int(np.floor(len(sample_names)*0.8))]\n",
    "val = reorder[int(np.floor(len(sample_names)*0.8)):]\n",
    "len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = np.array(sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annotations = {}\n",
    "for sample_name in sample_names[train]:\n",
    "    sample_annotations[sample_name] = msi_raw.loc[sample_name[0:name_len], 'msi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39285714285714285"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_coad = list(sample_annotations.values())\n",
    "sum(all_coad) / len(all_coad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_utils.TCGADataset(sample_annotations, root_dir, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annotations_val = {}\n",
    "for sample_name in sample_names[val]:\n",
    "    sample_annotations_val[sample_name] = msi_raw.loc[sample_name[0:name_len], 'msi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_coad_val = list(sample_annotations_val.values())\n",
    "sum(all_coad_val) / len(all_coad_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = data_utils.TCGADataset(sample_annotations_val, root_dir, transform=transform)\n",
    "valid_loader = DataLoader(valid_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception_v3 expects tensors with a size of N x 3 x 299 x 299\n",
    "#net = models.inception_v3(pretrained=True)\n",
    "net = models.resnet152(pretrained=True)\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tile_shape(H_in, W_in, kernel_size, dilation=1., padding=0., stride=1.):\n",
    "    H_out = (H_in + 2. * padding - dilation * (kernel_size-1) -1)/stride + 1\n",
    "    W_out = (W_in + 2. * padding - dilation * (kernel_size-1) -1)/stride + 1\n",
    "    return int(np.floor(H_out)), int(np.floor(W_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=0.5,\n",
    "                dilation = 1., padding = 0, H_in = 256, W_in = 256):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.n_conv_layers = n_conv_layers\n",
    "        self.n_fc_layers = n_fc_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_conv_filters = n_conv_filters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.mp_ker = 5 # max pool kernel size\n",
    "        self.mp_str = 5 # max pool stride\n",
    "        self.m = nn.MaxPool2d(self.mp_ker, stride=self.mp_str)\n",
    "        self.n = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.H_in, self.W_in = H_in, W_in\n",
    "        \n",
    "        in_channels = 3        \n",
    "        for layer in range(self.n_conv_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, self.n_conv_filters[layer], self.kernel_size[layer]))\n",
    "            self.conv_layers.append(self.relu)\n",
    "            self.conv_layers.append(self.m)\n",
    "            # convolution\n",
    "            self.H_in, self.W_in = update_tile_shape(self.H_in, self.W_in, kernel_size[layer])\n",
    "            # max pooling\n",
    "            self.H_in, self.W_in = update_tile_shape(self.H_in, self.W_in, self.mp_ker, stride=self.mp_str)\n",
    "            in_channels = self.n_conv_filters[layer]\n",
    "        in_channels = in_channels * self.H_in * self.W_in\n",
    "        for layer in range(self.n_fc_layers):\n",
    "            self.fc_layers.append(nn.Linear(in_channels, self.hidden_size[layer]))\n",
    "            self.fc_layers.append(self.relu)\n",
    "            self.fc_layers.append(self.n)\n",
    "            in_channels = self.hidden_size[layer]\n",
    "        self.conv = nn.Sequential(*self.conv_layers)\n",
    "        self.fc = nn.Sequential(*self.fc_layers)\n",
    "        self.classification_layer = nn.Linear(in_channels, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.conv(x)\n",
    "        embed = embed.view(x.shape[0],-1)\n",
    "        y = self.fc(embed)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (m): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  (n): Dropout(p=0.5)\n",
       "  (relu): ReLU()\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 36, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(36, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=3888, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5)\n",
       "  )\n",
       "  (classification_layer): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_conv_layers = 2\n",
    "n_fc_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = [512,512]\n",
    "dropout = 0.5\n",
    "net = ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slide,label in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide = slide[:,np.random.permutation(slide.shape[1])[:200],:,:,:]\n",
    "slide = slide.squeeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = net(slide)\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = pool_fn(embed).unsqueeze(0)\n",
    "pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net.classification_layer(pool)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_training_loop(e, train_loader, net, criterion, optimizer, pool_fn):\n",
    "    net.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx,(slide,label) in enumerate(train_loader):\n",
    "        #slide = slide[:,np.random.permutation(slide.shape[1])[:100],:,:,:].squeeze(0)\n",
    "        slide, label = slide.squeeze(0).cuda(), label.cuda()\n",
    "        output = net(slide)\n",
    "        pool = pool_fn(output).unsqueeze(0)\n",
    "        output = net.classification_layer(pool)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        #if idx % 10 == 0:\n",
    "        #    print('Epoch: {0}, Slide: {1}, Train NLL: {2:0.4f}'.format(e, idx, loss))\n",
    "            \n",
    "    print('Epoch: {0}, Avg Train NLL: {1:0.4f}'.format(e, total_loss/float(idx+1)))\n",
    "    \n",
    "\n",
    "def embedding_validation_loop(e, valid_loader, net, criterion, pool_fn, dataset='Val'):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    \n",
    "    for idx,(slide,label) in enumerate(valid_loader):\n",
    "        #slide = slide[:,np.random.permutation(slide.shape[1])[:100],:,:,:].squeeze(0)\n",
    "        slide, label = slide.squeeze(0).cuda(), label.cuda()\n",
    "        output = net(slide)\n",
    "        pool = pool_fn(output).unsqueeze(0)\n",
    "        output = net.classification_layer(pool)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        labels.extend(label.float().cpu().numpy())\n",
    "        preds.append(torch.argmax(output).float().detach().cpu().numpy())\n",
    "    \n",
    "        #if idx % 10 == 0:\n",
    "        #    print('Epoch: {0}, Slide: {1}, {3} NLL: {2:0.4f}'.format(e, idx, loss, dataset))\n",
    "            \n",
    "    acc = np.mean(np.array(labels) == np.array(preds))\n",
    "    print('Epoch: {0}, Avg {3} NLL: {1:0.4f}, {3} Acc: {2:0.4f}'.format(e, total_loss/float(idx+1), acc, dataset))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(50):\n",
    "    embedding_training_loop(e, train_loader, net, criterion, optimizer, pool_fn)\n",
    "    #train_loss = embedding_validation_loop(e, train_loader, net, criterion, pool_fn, dataset='Train')\n",
    "    val_loss = embedding_validation_loop(e, valid_loader, net, criterion, pool_fn, dataset='Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(slide,label) in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, slide.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2272214"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx,(slide,label) in enumerate(valid_loader):\n",
    "    print(idx, slide.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 "
     ]
    }
   ],
   "source": [
    "max_tiles = 100\n",
    "\n",
    "all_slides = []\n",
    "all_slide_labels = []\n",
    "\n",
    "for idx,sample_name in enumerate(sample_names):\n",
    "    sample_name = sample_names[-1]\n",
    "    slide_tiles = []\n",
    "    img_dir = root_dir + sample_name + '.svs/' + sample_name + '_files/5.0'\n",
    "    imgs = os.listdir(img_dir)\n",
    "\n",
    "    for im in imgs:\n",
    "        path = img_dir + '/' + im\n",
    "        image = data_utils.accimage_loader(path)\n",
    "\n",
    "        if transform is not None:\n",
    "            image = transform(image)\n",
    "\n",
    "        if image.shape[1] == 256 and image.shape[2] == 256:\n",
    "            slide_tiles.append(image)\n",
    "\n",
    "    slide = torch.stack(slide_tiles)\n",
    "    label = msi_raw.loc[sample_name[0:name_len], 'msi']\n",
    "    slide = slide[np.random.permutation(slide.shape[0])[:max_tiles],:,:,:]\n",
    "\n",
    "    all_slides.append(slide)\n",
    "    all_slide_labels.append(label)\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        print(idx, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_COAD_TRAIN = '/n/tcga_coad_train.pkl'\n",
    "TCGA_COAD_VALID = '/n/tcga_coad_valid.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 3, 256, 256]), 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_slides[0].shape, all_slide_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c702c7e70b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTCGA_COAD_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_slides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_slide_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(TCGA_COAD_TRAIN, 'wb') as f: \n",
    "    pickle.dump([all_slides[:196], all_slide_labels[:196]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6d03af2a8339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTCGA_COAD_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_slides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_slide_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTCGA_COAD_VALID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_slides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_slide_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(TCGA_COAD_VALID, 'wb') as f: \n",
    "    pickle.dump([all_slides[196:], all_slide_labels[196:]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCGADataset_from_pkl(Dataset):\n",
    "    def __init__(self, pickle_file):\n",
    "        with open(pickle_file, 'rb') as f: \n",
    "            all_slides, all_slide_labels = pickle.load(f)\n",
    "        self.data = all_slides\n",
    "        self.labels = all_slide_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TCGADataset_from_pkl(TCGA_COAD_TRAIN)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True, pin_memory=True)\n",
    "valid = TCGADataset_from_pkl(TCGA_COAD_VALID)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = os.listdir(img_dir)\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
