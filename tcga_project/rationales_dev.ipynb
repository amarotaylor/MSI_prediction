{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import model_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = '/n/data_labeled_histopathology_images/COAD/train.pkl'\n",
    "with open(pickle_file, 'rb') as f: \n",
    "    train_embeddings, train_labels, train_jpgs_to_slide = pickle.load(f)\n",
    "    \n",
    "pickle_file = '/n/data_labeled_histopathology_images/COAD/val.pkl'\n",
    "with open(pickle_file, 'rb') as f: \n",
    "    val_embeddings, val_labels, val_jpgs_to_slide = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20): \n",
    "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "    U = torch.rand(shape,dtype=torch.float32,device='cuda')\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature): \n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape)\n",
    "    return F.softmax( y / temperature,dim=1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"\n",
    "    Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        temperature: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if hard:\n",
    "        y = torch.argmax(logits,dim=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2048\n",
    "hidden_size = 2048\n",
    "output_size_gen = 2\n",
    "output_size_enc = 1\n",
    "\n",
    "gen = model_utils.Generator(input_size, hidden_size, output_size_gen, dropout=0.5)\n",
    "enc = model_utils.Encoder(input_size, hidden_size, output_size_enc, pool_fn, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 10\n",
    "lamb1 = 0\n",
    "lamb2 = 0\n",
    "temp = 10\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "params = list(enc.parameters()) + list(gen.parameters())\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(params, lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_val = val_jpgs_to_slide.max()\n",
    "n_samples_train = train_jpgs_to_slide.max()\n",
    "idxs_train = np.linspace(0,n_samples_train,n_samples_train+1,dtype=int)\n",
    "labels_to_idxs_train = np.concatenate([(train_labels[train_jpgs_to_slide==i]).unique().numpy() for i in idxs_train])\n",
    "weights = 1/np.sum(labels_to_idxs_train==0),1/np.sum(labels_to_idxs_train==1)\n",
    "sample_weight = [weights[l] for l in labels_to_idxs_train]\n",
    "sample_weight = sample_weight/np.sum(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = train_embeddings.cuda()\n",
    "val_embeddings = val_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (d): Dropout(p=0.5)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.cuda()\n",
    "enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsm = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_loop_vars(step_size):\n",
    "    logits_vec = torch.zeros((step_size+1,1)).cuda()\n",
    "    labels_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    znorm_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    zdist_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    batch_idx = 0\n",
    "    return logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_rationales_gs(e, step_size, optimizer, gen, enc, pool_fn, train_embeddings, train_jpgs_to_slide, \n",
    "                                train_labels, criterion, n_samples, sample_weight, lamb1, lamb2, temp):\n",
    "    gen.train()\n",
    "    enc.train()\n",
    "    \n",
    "    logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(step_size)\n",
    "    track_loss = 0   \n",
    "    track_omega = 0\n",
    "    rat_tiles = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    idxs_train = np.linspace(0,n_samples,n_samples+1,dtype=int)\n",
    "    idexs = np.random.choice(idxs_train,size=n_samples.numpy(),p=sample_weight)\n",
    "    \n",
    "    for idx in idexs:\n",
    "        slide = train_embeddings[train_jpgs_to_slide==idx] \n",
    "        labels_vec[batch_idx] = train_labels[train_jpgs_to_slide==idx].unique().float().cuda()\n",
    "        \n",
    "        preds = gen(slide) \n",
    "        logits = lsm(preds)\n",
    "        sample = gumbel_softmax(logits, temperature=temp)\n",
    "        rationale = slide * sample[:,1].unsqueeze(1)\n",
    "        \n",
    "        # predict class based on rationales\n",
    "        logits = enc(rationale)\n",
    "        logits_vec[batch_idx] = logits\n",
    "        \n",
    "        znorm = torch.sum(sample[:,1])\n",
    "        znorm_vec[batch_idx] = znorm / sample.shape[0]\n",
    "        \n",
    "        rat_tiles += znorm.detach().cpu().numpy()\n",
    "        total_tiles += sample.shape[0]\n",
    "        \n",
    "        zdist = torch.sum(torch.abs(sample[:-1,1] - sample[1:,1]))\n",
    "        zdist_vec[batch_idx] = zdist / sample.shape[0]\n",
    "        \n",
    "        if batch_idx == step_size:\n",
    "            # compute loss and regularization term\n",
    "            omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / step_size\n",
    "            bceloss = criterion(logits_vec, labels_vec)\n",
    "            loss = bceloss + omega\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            optimizer.zero_grad()\n",
    "            track_loss += bceloss.detach().cpu().numpy()\n",
    "            track_omega += omega.detach().cpu().numpy()\n",
    "            logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(step_size)\n",
    "        else:\n",
    "            batch_idx += 1\n",
    "            \n",
    "    omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / batch_idx\n",
    "    bceloss = criterion(logits_vec, labels_vec)\n",
    "    loss = bceloss + omega\n",
    "    loss.backward()\n",
    "    track_loss += bceloss.detach().cpu().numpy()\n",
    "    track_loss = track_loss * (float(step_size) / float(n_samples))\n",
    "    track_omega += omega.detach().cpu().numpy()\n",
    "    track_omega = track_omega * (float(step_size) / float(n_samples))\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "       \n",
    "    frac_tiles = rat_tiles / total_tiles\n",
    "    print('Epoch: {0}, Train Loss: {1:0.4f}, Train Omega: {2:0.4f}, Frac of Tiles: {3:0.4f}'.format(e, track_loss, \n",
    "                                                                                                   track_omega, \n",
    "                                                                                                   frac_tiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop_rationales_gs(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, val_jpgs_to_slide, \n",
    "                                  val_labels, criterion, n_samples, lamb1, lamb2, temp):\n",
    "    gen.eval()\n",
    "    enc.eval()\n",
    "    \n",
    "    logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(n_samples)\n",
    "    rat_tiles = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    idexs = np.linspace(0,n_samples,n_samples+1,dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for idx in idexs:\n",
    "            slide = val_embeddings[val_jpgs_to_slide==idx] # num_tiles x 2048\n",
    "            labels_vec[idx] = val_labels[val_jpgs_to_slide==idx].unique().float().cuda()\n",
    "\n",
    "            preds = gen(slide) # num_tiles x 1\n",
    "            logits = lsm(preds)\n",
    "            sample = gumbel_softmax(logits, temperature=temp)\n",
    "            rationale = slide * sample[:,1].unsqueeze(1)\n",
    "\n",
    "            # predict class based on rationales\n",
    "            logits = enc(rationale)\n",
    "            logits_vec[idx] = logits\n",
    "\n",
    "            znorm = torch.sum(sample[:,1])\n",
    "            znorm_vec[idx] = znorm / sample.shape[0]\n",
    "\n",
    "            rat_tiles += znorm.detach().cpu().numpy()\n",
    "            total_tiles += sample.shape[0]\n",
    "\n",
    "            zdist = torch.sum(torch.abs(sample[:-1,1] - sample[1:,1]))\n",
    "            zdist_vec[idx] = zdist / sample.shape[0]\n",
    "\n",
    "    # compute loss and regularization term\n",
    "    omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / n_samples\n",
    "    bceloss = criterion(logits_vec, labels_vec)\n",
    "    loss = bceloss + omega\n",
    "    frac_tiles = rat_tiles / total_tiles\n",
    "    \n",
    "    mask = labels_vec.cpu().numpy() == (logits_vec>0.5).float().cpu().numpy()\n",
    "    acc = np.mean(mask)\n",
    "    acc_1 = np.mean(mask[labels_vec.cpu().numpy()==1.])\n",
    "    acc_0 = np.mean(mask[labels_vec.cpu().numpy()==0.])\n",
    "    print('Epoch: {0}, Val Loss: {1:0.4f}, Val Omega: {2:0.4f}, Frac of Tiles: {3:0.4f}, Acc: {4:0.4f}, By Label: 0: {5:0.4f}, 1: {6:0.4f}'.format(e, bceloss, omega, frac_tiles, acc, acc_0, acc_1))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(100):\n",
    "    training_loop_rationales_gs(e, step_size, optimizer, gen, enc, pool_fn, train_embeddings, train_jpgs_to_slide, \n",
    "                                train_labels, criterion, n_samples_train, sample_weight, lamb1, lamb2, temp)\n",
    "    loss, acc = validation_loop_rationales_gs(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, \n",
    "                                              val_jpgs_to_slide, val_labels, criterion, n_samples_val, lamb1, lamb2, temp)\n",
    "    if e > 30:\n",
    "        lamb1 += 0.01\n",
    "        temp -= 0.1\n",
    "    temp = np.max([temp, 1.0])\n",
    "    lamb1 = np.min([lamb1, 1.0])\n",
    "    if e % 5 == 0:\n",
    "        print('Lambda: {0:0.4f}, Temperature: {1:0.4f}, LR: {2:0.4f}'.format(lamb1, temp, optimizer.state_dict()['param_groups'][0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
