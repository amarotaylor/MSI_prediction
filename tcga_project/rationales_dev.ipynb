{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import model_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = '/n/data_labeled_histopathology_images/COAD/train.pkl'\n",
    "with open(pickle_file, 'rb') as f: \n",
    "    train_embeddings, train_labels, train_jpgs_to_slide = pickle.load(f)\n",
    "    \n",
    "pickle_file = '/n/data_labeled_histopathology_images/COAD/val.pkl'\n",
    "with open(pickle_file, 'rb') as f: \n",
    "    val_embeddings, val_labels, val_jpgs_to_slide = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20): \n",
    "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "    U = torch.rand(shape,dtype=torch.float32,device='cuda')\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature): \n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape)\n",
    "    return F.softmax( y / temperature,dim=1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"\n",
    "    Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        temperature: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if hard:\n",
    "        y = torch.argmax(logits,dim=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2048\n",
    "hidden_size = 2048\n",
    "output_size_gen = 2\n",
    "output_size_enc = 1\n",
    "\n",
    "gen = model_utils.Generator(input_size, hidden_size, output_size_gen, dropout=0.5)\n",
    "enc = model_utils.Encoder(input_size, hidden_size, output_size_enc, pool_fn, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 10\n",
    "lamb1 = 0\n",
    "lamb2 = 0\n",
    "temp = 10\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "params = list(enc.parameters()) + list(gen.parameters())\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(params, lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_val = val_jpgs_to_slide.max()\n",
    "n_samples_train = train_jpgs_to_slide.max()\n",
    "idxs_train = np.linspace(0,n_samples_train,n_samples_train+1,dtype=int)\n",
    "labels_to_idxs_train = np.concatenate([(train_labels[train_jpgs_to_slide==i]).unique().numpy() for i in idxs_train])\n",
    "weights = 1/np.sum(labels_to_idxs_train==0),1/np.sum(labels_to_idxs_train==1)\n",
    "sample_weight = [weights[l] for l in labels_to_idxs_train]\n",
    "sample_weight = sample_weight/np.sum(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = train_embeddings.cuda()\n",
    "val_embeddings = val_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (d): Dropout(p=0.5)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.cuda()\n",
    "enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsm = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_loop_vars(step_size):\n",
    "    logits_vec = torch.zeros((step_size+1,1)).cuda()\n",
    "    labels_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    znorm_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    zdist_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    batch_idx = 0\n",
    "    return logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_rationales_gs(e, step_size, optimizer, gen, enc, pool_fn, train_embeddings, train_jpgs_to_slide, \n",
    "                                train_labels, criterion, n_samples, sample_weight, lamb1, lamb2, temp):\n",
    "    gen.train()\n",
    "    enc.train()\n",
    "    \n",
    "    logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(step_size)\n",
    "    track_loss = 0   \n",
    "    track_omega = 0\n",
    "    rat_tiles = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    idxs_train = np.linspace(0,n_samples,n_samples+1,dtype=int)\n",
    "    idexs = np.random.choice(idxs_train,size=n_samples.numpy(),p=sample_weight)\n",
    "    \n",
    "    for idx in idexs:\n",
    "        slide = train_embeddings[train_jpgs_to_slide==idx] \n",
    "        labels_vec[batch_idx] = train_labels[train_jpgs_to_slide==idx].unique().float().cuda()\n",
    "        \n",
    "        preds = gen(slide) \n",
    "        logits = lsm(preds)\n",
    "        sample = gumbel_softmax(logits, temperature=temp)\n",
    "        rationale = slide * sample[:,1].unsqueeze(1)\n",
    "        \n",
    "        # predict class based on rationales\n",
    "        logits = enc(rationale)\n",
    "        logits_vec[batch_idx] = logits\n",
    "        \n",
    "        znorm = torch.sum(sample[:,1])\n",
    "        znorm_vec[batch_idx] = znorm / sample.shape[0]\n",
    "        \n",
    "        rat_tiles += znorm.detach().cpu().numpy()\n",
    "        total_tiles += sample.shape[0]\n",
    "        \n",
    "        zdist = torch.sum(torch.abs(sample[:-1,1] - sample[1:,1]))\n",
    "        zdist_vec[batch_idx] = zdist / sample.shape[0]\n",
    "        \n",
    "        if batch_idx == step_size:\n",
    "            # compute loss and regularization term\n",
    "            omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / step_size\n",
    "            bceloss = criterion(logits_vec, labels_vec)\n",
    "            loss = bceloss + omega\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            optimizer.zero_grad()\n",
    "            track_loss += bceloss.detach().cpu().numpy()\n",
    "            track_omega += omega.detach().cpu().numpy()\n",
    "            logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(step_size)\n",
    "        else:\n",
    "            batch_idx += 1\n",
    "            \n",
    "    omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / batch_idx\n",
    "    bceloss = criterion(logits_vec, labels_vec)\n",
    "    loss = bceloss + omega\n",
    "    loss.backward()\n",
    "    track_loss += bceloss.detach().cpu().numpy()\n",
    "    track_loss = track_loss * (float(step_size) / float(n_samples))\n",
    "    track_omega += omega.detach().cpu().numpy()\n",
    "    track_omega = track_omega * (float(step_size) / float(n_samples))\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "       \n",
    "    frac_tiles = rat_tiles / total_tiles\n",
    "    print('Epoch: {0}, Train Loss: {1:0.4f}, Train Omega: {2:0.4f}, Frac of Tiles: {3:0.4f}'.format(e, track_loss, \n",
    "                                                                                                   track_omega, \n",
    "                                                                                                   frac_tiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop_rationales_gs(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, val_jpgs_to_slide, \n",
    "                                  val_labels, criterion, n_samples, lamb1, lamb2, temp):\n",
    "    gen.eval()\n",
    "    enc.eval()\n",
    "    \n",
    "    logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(n_samples)\n",
    "    rat_tiles = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    idexs = np.linspace(0,n_samples,n_samples+1,dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for idx in idexs:\n",
    "            slide = val_embeddings[val_jpgs_to_slide==idx] # num_tiles x 2048\n",
    "            labels_vec[idx] = val_labels[val_jpgs_to_slide==idx].unique().float().cuda()\n",
    "\n",
    "            preds = gen(slide)\n",
    "            sample = torch.argmax(preds, dim=1).float()\n",
    "            rationale = slide * sample.unsqueeze(1)\n",
    "\n",
    "            # predict class based on rationales\n",
    "            logits = enc(rationale)\n",
    "            logits_vec[idx] = logits\n",
    "\n",
    "            znorm = torch.sum(sample)\n",
    "            znorm_vec[idx] = znorm / sample.shape[0]\n",
    "\n",
    "            rat_tiles += znorm.detach().cpu().numpy()\n",
    "            total_tiles += sample.shape[0]\n",
    "\n",
    "            zdist = torch.sum(torch.abs(sample[:-1] - sample[1:]))\n",
    "            zdist_vec[idx] = zdist / sample.shape[0]\n",
    "\n",
    "    # compute loss and regularization term\n",
    "    omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / n_samples\n",
    "    bceloss = criterion(logits_vec, labels_vec)\n",
    "    loss = bceloss + omega\n",
    "    frac_tiles = rat_tiles / total_tiles\n",
    "    \n",
    "    mask = labels_vec.cpu().numpy() == (logits_vec>0.5).float().cpu().numpy()\n",
    "    acc = np.mean(mask)\n",
    "    acc_1 = np.mean(mask[labels_vec.cpu().numpy()==1.])\n",
    "    acc_0 = np.mean(mask[labels_vec.cpu().numpy()==0.])\n",
    "    print('Epoch: {0}, Val Loss: {1:0.4f}, Val Omega: {2:0.4f}, Frac of Tiles: {3:0.4f}, Acc: {4:0.4f}, By Label: 0: {5:0.4f}, 1: {6:0.4f}'.format(e, bceloss, omega, frac_tiles, acc, acc_0, acc_1))\n",
    "    return loss, acc, frac_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.6370, Train Omega: 0.0000, Frac of Tiles: 0.4729\n",
      "Epoch: 0, Val Loss: 0.6723, Val Omega: 0.0000, Frac of Tiles: 0.2045, Acc: 0.5610, By Label: 0: 1.0000, 1: 0.0000\n",
      "SAVED BEST LOSS\n",
      "SAVED BEST ACC\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 1, Train Loss: 0.6085, Train Omega: 0.0000, Frac of Tiles: 0.5382\n",
      "Epoch: 1, Val Loss: 0.7144, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.6304, 1: 0.7778\n",
      "Epoch: 2, Train Loss: 0.5630, Train Omega: 0.0000, Frac of Tiles: 0.7100\n",
      "Epoch: 2, Val Loss: 0.6193, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.9783, 1: 0.2500\n",
      "Epoch: 3, Train Loss: 0.5200, Train Omega: 0.0000, Frac of Tiles: 0.8587\n",
      "Epoch: 3, Val Loss: 0.5967, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.9565, 1: 0.2778\n",
      "Epoch: 4, Train Loss: 0.4701, Train Omega: 0.0000, Frac of Tiles: 0.9329\n",
      "Epoch: 4, Val Loss: 0.6243, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.7826, 1: 0.5556\n",
      "Epoch: 5, Train Loss: 0.4282, Train Omega: 0.0000, Frac of Tiles: 0.9609\n",
      "Epoch: 5, Val Loss: 0.5954, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.8478, 1: 0.5000\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 6, Train Loss: 0.4370, Train Omega: 0.0000, Frac of Tiles: 0.9707\n",
      "Epoch: 6, Val Loss: 0.5903, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8478, 1: 0.4722\n",
      "Epoch: 7, Train Loss: 0.4080, Train Omega: 0.0000, Frac of Tiles: 0.9790\n",
      "Epoch: 7, Val Loss: 0.5817, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.9348, 1: 0.3056\n",
      "Epoch: 8, Train Loss: 0.3947, Train Omega: 0.0000, Frac of Tiles: 0.9829\n",
      "Epoch: 8, Val Loss: 0.5933, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Epoch: 9, Train Loss: 0.3941, Train Omega: 0.0000, Frac of Tiles: 0.9860\n",
      "Epoch: 9, Val Loss: 0.5808, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8478, 1: 0.4444\n",
      "Epoch: 10, Train Loss: 0.3971, Train Omega: 0.0000, Frac of Tiles: 0.9891\n",
      "Epoch: 10, Val Loss: 0.6609, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.7195, By Label: 0: 0.6957, 1: 0.7500\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 11, Train Loss: 0.3869, Train Omega: 0.0000, Frac of Tiles: 0.9886\n",
      "Epoch: 11, Val Loss: 0.5834, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8478, 1: 0.4444\n",
      "Epoch: 12, Train Loss: 0.3558, Train Omega: 0.0000, Frac of Tiles: 0.9912\n",
      "Epoch: 12, Val Loss: 0.5882, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "Epoch: 13, Train Loss: 0.3516, Train Omega: 0.0000, Frac of Tiles: 0.9917\n",
      "Epoch: 13, Val Loss: 0.6220, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.7826, 1: 0.5278\n",
      "Epoch: 14, Train Loss: 0.3365, Train Omega: 0.0000, Frac of Tiles: 0.9929\n",
      "Epoch: 14, Val Loss: 0.6100, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6220, By Label: 0: 0.8696, 1: 0.3056\n",
      "Epoch: 15, Train Loss: 0.3289, Train Omega: 0.0000, Frac of Tiles: 0.9930\n",
      "Epoch: 15, Val Loss: 0.6097, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6463, By Label: 0: 0.8261, 1: 0.4167\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 16, Train Loss: 0.3506, Train Omega: 0.0000, Frac of Tiles: 0.9940\n",
      "Epoch: 16, Val Loss: 0.6195, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8261, 1: 0.4722\n",
      "Epoch: 17, Train Loss: 0.3685, Train Omega: 0.0000, Frac of Tiles: 0.9944\n",
      "Epoch: 17, Val Loss: 0.6308, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.7073, By Label: 0: 0.8043, 1: 0.5833\n",
      "Epoch: 18, Train Loss: 0.3464, Train Omega: 0.0000, Frac of Tiles: 0.9944\n",
      "Epoch: 18, Val Loss: 0.6267, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6098, By Label: 0: 0.8478, 1: 0.3056\n",
      "Epoch: 19, Train Loss: 0.3406, Train Omega: 0.0000, Frac of Tiles: 0.9951\n",
      "Epoch: 19, Val Loss: 0.6261, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8043, 1: 0.5278\n",
      "Epoch: 20, Train Loss: 0.3174, Train Omega: 0.0000, Frac of Tiles: 0.9954\n",
      "Epoch: 20, Val Loss: 0.6266, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8261, 1: 0.4722\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 21, Train Loss: 0.3342, Train Omega: 0.0000, Frac of Tiles: 0.9960\n",
      "Epoch: 21, Val Loss: 0.6220, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Epoch: 22, Train Loss: 0.3156, Train Omega: 0.0000, Frac of Tiles: 0.9961\n",
      "Epoch: 22, Val Loss: 0.6242, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Epoch: 23, Train Loss: 0.3119, Train Omega: 0.0000, Frac of Tiles: 0.9963\n",
      "Epoch: 23, Val Loss: 0.6345, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Epoch: 24, Train Loss: 0.3070, Train Omega: 0.0000, Frac of Tiles: 0.9966\n",
      "Epoch: 24, Val Loss: 0.6384, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.8043, 1: 0.5556\n",
      "Epoch: 25, Train Loss: 0.3634, Train Omega: 0.0000, Frac of Tiles: 0.9965\n",
      "Epoch: 25, Val Loss: 0.6479, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.7195, By Label: 0: 0.8043, 1: 0.6111\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 26, Train Loss: 0.2806, Train Omega: 0.0000, Frac of Tiles: 0.9970\n",
      "Epoch: 26, Val Loss: 0.6405, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6341, By Label: 0: 0.8261, 1: 0.3889\n",
      "Epoch: 27, Train Loss: 0.3432, Train Omega: 0.0000, Frac of Tiles: 0.9970\n",
      "Epoch: 27, Val Loss: 0.6799, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6098, By Label: 0: 0.8696, 1: 0.2778\n",
      "Epoch: 28, Train Loss: 0.3235, Train Omega: 0.0000, Frac of Tiles: 0.9974\n",
      "Epoch: 28, Val Loss: 0.6511, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.8043, 1: 0.5556\n",
      "Epoch: 29, Train Loss: 0.3188, Train Omega: 0.0000, Frac of Tiles: 0.9971\n",
      "Epoch: 29, Val Loss: 0.6687, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.7317, By Label: 0: 0.7826, 1: 0.6667\n",
      "Epoch: 30, Train Loss: 0.3047, Train Omega: 0.0000, Frac of Tiles: 0.9975\n",
      "Epoch: 30, Val Loss: 0.6604, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 31, Train Loss: 0.3122, Train Omega: 0.0000, Frac of Tiles: 0.9975\n",
      "Epoch: 31, Val Loss: 0.6735, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.7195, By Label: 0: 0.7826, 1: 0.6389\n",
      "Epoch: 32, Train Loss: 0.3004, Train Omega: 0.0102, Frac of Tiles: 0.9979\n",
      "Epoch: 32, Val Loss: 0.6799, Val Omega: 0.0101, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.7609, 1: 0.6111\n",
      "Epoch: 33, Train Loss: 0.3397, Train Omega: 0.0203, Frac of Tiles: 0.9982\n",
      "Epoch: 33, Val Loss: 0.6905, Val Omega: 0.0202, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.7609, 1: 0.6111\n",
      "Epoch: 34, Train Loss: 0.2974, Train Omega: 0.0305, Frac of Tiles: 0.9982\n",
      "Epoch: 34, Val Loss: 0.6831, Val Omega: 0.0304, Frac of Tiles: 1.0000, Acc: 0.6341, By Label: 0: 0.8261, 1: 0.3889\n",
      "Epoch: 35, Train Loss: 0.3104, Train Omega: 0.0407, Frac of Tiles: 0.9983\n",
      "Epoch: 35, Val Loss: 0.6791, Val Omega: 0.0405, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8043, 1: 0.5000\n",
      "Lambda: 0.0500, Temperature: 9.5000, LR: 0.00001000\n",
      "Epoch: 36, Train Loss: 0.2808, Train Omega: 0.0509, Frac of Tiles: 0.9986\n",
      "Epoch: 36, Val Loss: 0.6870, Val Omega: 0.0506, Frac of Tiles: 1.0000, Acc: 0.6463, By Label: 0: 0.8261, 1: 0.4167\n",
      "Epoch: 37, Train Loss: 0.3137, Train Omega: 0.0610, Frac of Tiles: 0.9986\n",
      "Epoch: 37, Val Loss: 0.6901, Val Omega: 0.0607, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Epoch: 38, Train Loss: 0.3126, Train Omega: 0.0712, Frac of Tiles: 0.9986\n",
      "Epoch: 38, Val Loss: 0.6982, Val Omega: 0.0709, Frac of Tiles: 1.0000, Acc: 0.7073, By Label: 0: 0.7609, 1: 0.6389\n",
      "Epoch: 39, Train Loss: 0.2894, Train Omega: 0.0814, Frac of Tiles: 0.9987\n",
      "Epoch: 39, Val Loss: 0.6916, Val Omega: 0.0810, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8043, 1: 0.5000\n",
      "Epoch: 40, Train Loss: 0.2842, Train Omega: 0.0916, Frac of Tiles: 0.9988\n",
      "Epoch: 40, Val Loss: 0.6943, Val Omega: 0.0911, Frac of Tiles: 1.0000, Acc: 0.7195, By Label: 0: 0.7826, 1: 0.6389\n",
      "Lambda: 0.1000, Temperature: 9.0000, LR: 0.00001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Train Loss: 0.3182, Train Omega: 0.1017, Frac of Tiles: 0.9989\n",
      "Epoch: 41, Val Loss: 0.7033, Val Omega: 0.1012, Frac of Tiles: 1.0000, Acc: 0.7317, By Label: 0: 0.7826, 1: 0.6667\n",
      "Epoch: 42, Train Loss: 0.3198, Train Omega: 0.1119, Frac of Tiles: 0.9989\n",
      "Epoch: 42, Val Loss: 0.7136, Val Omega: 0.1114, Frac of Tiles: 1.0000, Acc: 0.7317, By Label: 0: 0.7609, 1: 0.6944\n",
      "Epoch: 43, Train Loss: 0.2983, Train Omega: 0.1221, Frac of Tiles: 0.9988\n",
      "Epoch: 43, Val Loss: 0.7098, Val Omega: 0.1215, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Epoch: 44, Train Loss: 0.2979, Train Omega: 0.1323, Frac of Tiles: 0.9988\n",
      "Epoch: 44, Val Loss: 0.7078, Val Omega: 0.1316, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Epoch: 45, Train Loss: 0.2932, Train Omega: 0.1425, Frac of Tiles: 0.9990\n",
      "Epoch: 45, Val Loss: 0.7116, Val Omega: 0.1417, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8043, 1: 0.5000\n",
      "Lambda: 0.1500, Temperature: 8.5000, LR: 0.00001000\n",
      "Epoch: 46, Train Loss: 0.2516, Train Omega: 0.1526, Frac of Tiles: 0.9989\n",
      "Epoch: 46, Val Loss: 0.7156, Val Omega: 0.1519, Frac of Tiles: 1.0000, Acc: 0.7073, By Label: 0: 0.7609, 1: 0.6389\n",
      "Epoch: 47, Train Loss: 0.2692, Train Omega: 0.1628, Frac of Tiles: 0.9989\n",
      "Epoch: 47, Val Loss: 0.7147, Val Omega: 0.1620, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8261, 1: 0.4722\n",
      "Epoch: 48, Train Loss: 0.2981, Train Omega: 0.1730, Frac of Tiles: 0.9989\n",
      "Epoch: 48, Val Loss: 0.7236, Val Omega: 0.1721, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Epoch: 49, Train Loss: 0.2636, Train Omega: 0.1831, Frac of Tiles: 0.9988\n",
      "Epoch: 49, Val Loss: 0.7161, Val Omega: 0.1822, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Epoch: 50, Train Loss: 0.3047, Train Omega: 0.1933, Frac of Tiles: 0.9987\n",
      "Epoch: 50, Val Loss: 0.7233, Val Omega: 0.1923, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Lambda: 0.2000, Temperature: 8.0000, LR: 0.00001000\n",
      "Epoch: 51, Train Loss: 0.3193, Train Omega: 0.2034, Frac of Tiles: 0.9987\n",
      "Epoch: 51, Val Loss: 0.7225, Val Omega: 0.2025, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Epoch: 52, Train Loss: 0.2541, Train Omega: 0.2136, Frac of Tiles: 0.9985\n",
      "Epoch: 52, Val Loss: 0.7156, Val Omega: 0.2126, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8043, 1: 0.5278\n",
      "Epoch: 53, Train Loss: 0.2903, Train Omega: 0.2237, Frac of Tiles: 0.9983\n",
      "Epoch: 53, Val Loss: 0.7217, Val Omega: 0.2227, Frac of Tiles: 1.0000, Acc: 0.7195, By Label: 0: 0.7826, 1: 0.6389\n",
      "Epoch: 54, Train Loss: 0.2612, Train Omega: 0.2338, Frac of Tiles: 0.9979\n",
      "Epoch: 54, Val Loss: 0.7559, Val Omega: 0.2328, Frac of Tiles: 1.0000, Acc: 0.7561, By Label: 0: 0.7609, 1: 0.7500\n",
      "Epoch: 55, Train Loss: 0.2778, Train Omega: 0.2438, Frac of Tiles: 0.9971\n",
      "Epoch: 55, Val Loss: 0.7181, Val Omega: 0.2430, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.8043, 1: 0.5556\n",
      "Lambda: 0.2500, Temperature: 7.5000, LR: 0.00001000\n",
      "Epoch: 56, Train Loss: 0.3131, Train Omega: 0.2535, Frac of Tiles: 0.9956\n",
      "Epoch: 56, Val Loss: 0.7892, Val Omega: 0.2531, Frac of Tiles: 1.0000, Acc: 0.7195, By Label: 0: 0.6739, 1: 0.7778\n",
      "Epoch: 57, Train Loss: 0.2947, Train Omega: 0.2613, Frac of Tiles: 0.9870\n",
      "Epoch: 57, Val Loss: 0.7314, Val Omega: 0.2632, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8261, 1: 0.4722\n",
      "Epoch: 58, Train Loss: 0.3066, Train Omega: 0.2071, Frac of Tiles: 0.7707\n",
      "Epoch: 58, Val Loss: 0.6915, Val Omega: 0.0001, Frac of Tiles: 0.0004, Acc: 0.5610, By Label: 0: 1.0000, 1: 0.0000\n",
      "Epoch: 59, Train Loss: 0.3717, Train Omega: 0.1234, Frac of Tiles: 0.4334\n",
      "Epoch: 59, Val Loss: 0.6421, Val Omega: 0.1627, Frac of Tiles: 0.5756, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Epoch: 60, Train Loss: 0.3174, Train Omega: 0.1398, Frac of Tiles: 0.4719\n",
      "Epoch: 60, Val Loss: 0.6584, Val Omega: 0.1765, Frac of Tiles: 0.5909, Acc: 0.6951, By Label: 0: 0.7826, 1: 0.5833\n",
      "Lambda: 0.3000, Temperature: 7.0000, LR: 0.00001000\n",
      "Epoch: 61, Train Loss: 0.3461, Train Omega: 0.1395, Frac of Tiles: 0.4601\n",
      "Epoch: 61, Val Loss: 0.6526, Val Omega: 0.0185, Frac of Tiles: 0.0595, Acc: 0.5976, By Label: 0: 0.9130, 1: 0.1944\n",
      "SAVED BEST LOSS\n",
      "SAVED BEST ACC\n",
      "Epoch: 62, Train Loss: 0.3311, Train Omega: 0.1350, Frac of Tiles: 0.4279\n",
      "Epoch: 62, Val Loss: 0.5930, Val Omega: 0.0862, Frac of Tiles: 0.2709, Acc: 0.6951, By Label: 0: 0.8696, 1: 0.4722\n",
      "SAVED BEST ACC\n",
      "Epoch: 63, Train Loss: 0.3506, Train Omega: 0.1323, Frac of Tiles: 0.4076\n",
      "Epoch: 63, Val Loss: 0.6225, Val Omega: 0.0735, Frac of Tiles: 0.2203, Acc: 0.7073, By Label: 0: 0.8261, 1: 0.5556\n",
      "SAVED BEST ACC\n",
      "Epoch: 64, Train Loss: 0.3656, Train Omega: 0.1277, Frac of Tiles: 0.3772\n",
      "Epoch: 64, Val Loss: 0.6017, Val Omega: 0.0601, Frac of Tiles: 0.1747, Acc: 0.6829, By Label: 0: 0.8696, 1: 0.4444\n",
      "SAVED BEST LOSS\n",
      "Epoch: 65, Train Loss: 0.3717, Train Omega: 0.1252, Frac of Tiles: 0.3653\n",
      "Epoch: 65, Val Loss: 0.6184, Val Omega: 0.0413, Frac of Tiles: 0.1163, Acc: 0.6585, By Label: 0: 0.8696, 1: 0.3889\n",
      "SAVED BEST LOSS\n",
      "Lambda: 0.3500, Temperature: 6.5000, LR: 0.00001000\n",
      "Epoch: 66, Train Loss: 0.3581, Train Omega: 0.1207, Frac of Tiles: 0.3432\n",
      "Epoch: 66, Val Loss: 0.5995, Val Omega: 0.0912, Frac of Tiles: 0.2566, Acc: 0.6951, By Label: 0: 0.8696, 1: 0.4722\n",
      "Epoch: 67, Train Loss: 0.3111, Train Omega: 0.1347, Frac of Tiles: 0.3718\n",
      "Epoch: 67, Val Loss: 0.5912, Val Omega: 0.0870, Frac of Tiles: 0.2381, Acc: 0.6951, By Label: 0: 0.8913, 1: 0.4444\n",
      "Epoch: 68, Train Loss: 0.3688, Train Omega: 0.1227, Frac of Tiles: 0.3252\n",
      "Epoch: 68, Val Loss: 0.5949, Val Omega: 0.0636, Frac of Tiles: 0.1692, Acc: 0.6829, By Label: 0: 0.8696, 1: 0.4444\n",
      "SAVED BEST LOSS\n",
      "Epoch: 69, Train Loss: 0.3457, Train Omega: 0.1200, Frac of Tiles: 0.3147\n",
      "Epoch: 69, Val Loss: 0.6002, Val Omega: 0.0852, Frac of Tiles: 0.2221, Acc: 0.6829, By Label: 0: 0.8478, 1: 0.4722\n",
      "Epoch: 70, Train Loss: 0.3510, Train Omega: 0.1268, Frac of Tiles: 0.3205\n",
      "Epoch: 70, Val Loss: 0.5914, Val Omega: 0.0743, Frac of Tiles: 0.1885, Acc: 0.6829, By Label: 0: 0.8913, 1: 0.4167\n",
      "Lambda: 0.4000, Temperature: 6.0000, LR: 0.00001000\n",
      "Epoch: 71, Train Loss: 0.3292, Train Omega: 0.1301, Frac of Tiles: 0.3255\n",
      "Epoch: 71, Val Loss: 0.5879, Val Omega: 0.0777, Frac of Tiles: 0.1910, Acc: 0.6951, By Label: 0: 0.8913, 1: 0.4444\n",
      "Epoch: 72, Train Loss: 0.3615, Train Omega: 0.1248, Frac of Tiles: 0.3006\n",
      "Epoch: 72, Val Loss: 0.5939, Val Omega: 0.0623, Frac of Tiles: 0.1477, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "SAVED BEST LOSS\n",
      "Epoch: 73, Train Loss: 0.3462, Train Omega: 0.1242, Frac of Tiles: 0.2990\n",
      "Epoch: 73, Val Loss: 0.5845, Val Omega: 0.0703, Frac of Tiles: 0.1625, Acc: 0.6707, By Label: 0: 0.8696, 1: 0.4167\n",
      "SAVED BEST LOSS\n",
      "Epoch: 74, Train Loss: 0.3434, Train Omega: 0.1291, Frac of Tiles: 0.2979\n",
      "Epoch: 74, Val Loss: 0.5912, Val Omega: 0.0527, Frac of Tiles: 0.1178, Acc: 0.6707, By Label: 0: 0.9130, 1: 0.3611\n",
      "SAVED BEST LOSS\n",
      "Epoch: 75, Train Loss: 0.3365, Train Omega: 0.1185, Frac of Tiles: 0.2634\n",
      "Epoch: 75, Val Loss: 0.5846, Val Omega: 0.0661, Frac of Tiles: 0.1461, Acc: 0.6707, By Label: 0: 0.9348, 1: 0.3333\n",
      "Lambda: 0.4500, Temperature: 5.5000, LR: 0.00001000\n",
      "Epoch: 76, Train Loss: 0.3587, Train Omega: 0.1145, Frac of Tiles: 0.2549\n",
      "Epoch: 76, Val Loss: 0.5828, Val Omega: 0.0622, Frac of Tiles: 0.1341, Acc: 0.6707, By Label: 0: 0.9130, 1: 0.3611\n",
      "Epoch: 77, Train Loss: 0.3411, Train Omega: 0.1236, Frac of Tiles: 0.2697\n",
      "Epoch: 77, Val Loss: 0.5843, Val Omega: 0.0607, Frac of Tiles: 0.1291, Acc: 0.6707, By Label: 0: 0.9130, 1: 0.3611\n",
      "Epoch: 78, Train Loss: 0.3408, Train Omega: 0.1242, Frac of Tiles: 0.2614\n",
      "Epoch: 78, Val Loss: 0.5862, Val Omega: 0.0633, Frac of Tiles: 0.1320, Acc: 0.6951, By Label: 0: 0.9130, 1: 0.4167\n",
      "Epoch: 79, Train Loss: 0.3542, Train Omega: 0.1146, Frac of Tiles: 0.2334\n",
      "Epoch: 79, Val Loss: 0.5821, Val Omega: 0.0929, Frac of Tiles: 0.1896, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "Epoch: 80, Train Loss: 0.3590, Train Omega: 0.1164, Frac of Tiles: 0.2350\n",
      "Epoch: 80, Val Loss: 0.5794, Val Omega: 0.0664, Frac of Tiles: 0.1322, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Lambda: 0.5000, Temperature: 5.0000, LR: 0.00001000\n",
      "Epoch: 81, Train Loss: 0.3450, Train Omega: 0.1118, Frac of Tiles: 0.2220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Val Loss: 0.5866, Val Omega: 0.0536, Frac of Tiles: 0.1061, Acc: 0.6707, By Label: 0: 0.9130, 1: 0.3611\n",
      "SAVED BEST LOSS\n",
      "Epoch: 82, Train Loss: 0.3547, Train Omega: 0.1122, Frac of Tiles: 0.2205\n",
      "Epoch: 82, Val Loss: 0.5786, Val Omega: 0.0710, Frac of Tiles: 0.1381, Acc: 0.6707, By Label: 0: 0.9130, 1: 0.3611\n",
      "Epoch: 83, Train Loss: 0.3648, Train Omega: 0.1064, Frac of Tiles: 0.2089\n",
      "Epoch: 83, Val Loss: 0.6024, Val Omega: 0.0385, Frac of Tiles: 0.0731, Acc: 0.6707, By Label: 0: 0.9130, 1: 0.3611\n",
      "Epoch: 84, Train Loss: 0.3427, Train Omega: 0.1129, Frac of Tiles: 0.2137\n",
      "Epoch: 84, Val Loss: 0.5899, Val Omega: 0.0456, Frac of Tiles: 0.0851, Acc: 0.6585, By Label: 0: 0.9130, 1: 0.3333\n",
      "SAVED BEST LOSS\n",
      "Epoch: 85, Train Loss: 0.3817, Train Omega: 0.1038, Frac of Tiles: 0.1936\n",
      "Epoch: 85, Val Loss: 0.5761, Val Omega: 0.0719, Frac of Tiles: 0.1325, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "Lambda: 0.5500, Temperature: 4.5000, LR: 0.00001000\n",
      "Epoch: 86, Train Loss: 0.3636, Train Omega: 0.1065, Frac of Tiles: 0.1892\n",
      "Epoch: 86, Val Loss: 0.5784, Val Omega: 0.0619, Frac of Tiles: 0.1121, Acc: 0.6707, By Label: 0: 0.9130, 1: 0.3611\n",
      "Epoch: 87, Train Loss: 0.3550, Train Omega: 0.1120, Frac of Tiles: 0.2019\n",
      "Epoch: 87, Val Loss: 0.5998, Val Omega: 0.0308, Frac of Tiles: 0.0561, Acc: 0.6585, By Label: 0: 0.9565, 1: 0.2778\n",
      "SAVED BEST LOSS\n",
      "Epoch: 88, Train Loss: 0.3327, Train Omega: 0.1039, Frac of Tiles: 0.1846\n",
      "Epoch: 88, Val Loss: 0.5722, Val Omega: 0.0810, Frac of Tiles: 0.1427, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "Epoch: 89, Train Loss: 0.3215, Train Omega: 0.1157, Frac of Tiles: 0.2005\n",
      "Epoch: 89, Val Loss: 0.5706, Val Omega: 0.0953, Frac of Tiles: 0.1637, Acc: 0.6463, By Label: 0: 0.8478, 1: 0.3889\n",
      "Epoch: 90, Train Loss: 0.3620, Train Omega: 0.1106, Frac of Tiles: 0.1886\n",
      "Epoch: 90, Val Loss: 0.5708, Val Omega: 0.0502, Frac of Tiles: 0.0862, Acc: 0.6585, By Label: 0: 0.9130, 1: 0.3333\n",
      "SAVED BEST LOSS\n",
      "Lambda: 0.6000, Temperature: 4.0000, LR: 0.00001000\n",
      "Epoch: 91, Train Loss: 0.3586, Train Omega: 0.1043, Frac of Tiles: 0.1723\n",
      "Epoch: 91, Val Loss: 0.5696, Val Omega: 0.0612, Frac of Tiles: 0.1036, Acc: 0.6707, By Label: 0: 0.8913, 1: 0.3889\n",
      "Epoch: 92, Train Loss: 0.3494, Train Omega: 0.1023, Frac of Tiles: 0.1724\n",
      "Epoch: 92, Val Loss: 0.5682, Val Omega: 0.0685, Frac of Tiles: 0.1135, Acc: 0.6585, By Label: 0: 0.8913, 1: 0.3611\n",
      "Epoch: 93, Train Loss: 0.3641, Train Omega: 0.1008, Frac of Tiles: 0.1611\n",
      "Epoch: 93, Val Loss: 0.5711, Val Omega: 0.0793, Frac of Tiles: 0.1304, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "Epoch: 94, Train Loss: 0.3314, Train Omega: 0.1063, Frac of Tiles: 0.1694\n",
      "Epoch: 94, Val Loss: 0.5690, Val Omega: 0.0561, Frac of Tiles: 0.0919, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "Epoch: 95, Train Loss: 0.3384, Train Omega: 0.1044, Frac of Tiles: 0.1672\n",
      "Epoch: 95, Val Loss: 0.5686, Val Omega: 0.0635, Frac of Tiles: 0.1028, Acc: 0.6585, By Label: 0: 0.8913, 1: 0.3611\n",
      "Lambda: 0.6500, Temperature: 3.5000, LR: 0.00001000\n",
      "Epoch: 96, Train Loss: 0.3242, Train Omega: 0.1083, Frac of Tiles: 0.1685\n",
      "Epoch: 96, Val Loss: 0.5681, Val Omega: 0.0561, Frac of Tiles: 0.0893, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "Epoch: 97, Train Loss: 0.3676, Train Omega: 0.0981, Frac of Tiles: 0.1487\n",
      "Epoch: 97, Val Loss: 0.5665, Val Omega: 0.0475, Frac of Tiles: 0.0752, Acc: 0.6463, By Label: 0: 0.8913, 1: 0.3333\n",
      "SAVED BEST LOSS\n",
      "Epoch: 98, Train Loss: 0.3735, Train Omega: 0.0883, Frac of Tiles: 0.1356\n",
      "Epoch: 98, Val Loss: 0.5707, Val Omega: 0.0657, Frac of Tiles: 0.1012, Acc: 0.6585, By Label: 0: 0.8696, 1: 0.3889\n",
      "Epoch: 99, Train Loss: 0.3372, Train Omega: 0.1039, Frac of Tiles: 0.1540\n",
      "Epoch: 99, Val Loss: 0.5662, Val Omega: 0.0507, Frac of Tiles: 0.0776, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "Epoch: 100, Train Loss: 0.3375, Train Omega: 0.0988, Frac of Tiles: 0.1519\n",
      "Epoch: 100, Val Loss: 0.5598, Val Omega: 0.0611, Frac of Tiles: 0.0904, Acc: 0.6585, By Label: 0: 0.8696, 1: 0.3889\n",
      "Lambda: 0.7000, Temperature: 3.0000, LR: 0.00001000\n",
      "Epoch: 101, Train Loss: 0.3562, Train Omega: 0.0936, Frac of Tiles: 0.1380\n",
      "Epoch: 101, Val Loss: 0.5574, Val Omega: 0.0547, Frac of Tiles: 0.0809, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "SAVED BEST LOSS\n",
      "Epoch: 102, Train Loss: 0.3608, Train Omega: 0.0921, Frac of Tiles: 0.1348\n",
      "Epoch: 102, Val Loss: 0.5581, Val Omega: 0.0726, Frac of Tiles: 0.1051, Acc: 0.6585, By Label: 0: 0.8696, 1: 0.3889\n",
      "Epoch: 103, Train Loss: 0.3528, Train Omega: 0.0935, Frac of Tiles: 0.1326\n",
      "Epoch: 103, Val Loss: 0.5581, Val Omega: 0.0680, Frac of Tiles: 0.0978, Acc: 0.6585, By Label: 0: 0.8696, 1: 0.3889\n",
      "Epoch: 104, Train Loss: 0.3242, Train Omega: 0.1033, Frac of Tiles: 0.1480\n",
      "Epoch: 104, Val Loss: 0.5587, Val Omega: 0.0712, Frac of Tiles: 0.0999, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "Epoch: 105, Train Loss: 0.3190, Train Omega: 0.0995, Frac of Tiles: 0.1399\n",
      "Epoch: 105, Val Loss: 0.5560, Val Omega: 0.0663, Frac of Tiles: 0.0920, Acc: 0.6707, By Label: 0: 0.8478, 1: 0.4444\n",
      "Lambda: 0.7500, Temperature: 2.5000, LR: 0.00001000\n",
      "Epoch: 106, Train Loss: 0.3409, Train Omega: 0.0923, Frac of Tiles: 0.1294\n",
      "Epoch: 106, Val Loss: 0.5589, Val Omega: 0.0464, Frac of Tiles: 0.0642, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "SAVED BEST LOSS\n",
      "Epoch: 107, Train Loss: 0.3513, Train Omega: 0.0905, Frac of Tiles: 0.1259\n",
      "Epoch: 107, Val Loss: 0.5484, Val Omega: 0.0568, Frac of Tiles: 0.0783, Acc: 0.6463, By Label: 0: 0.8913, 1: 0.3333\n",
      "SAVED BEST LOSS\n",
      "Epoch: 108, Train Loss: 0.3511, Train Omega: 0.0921, Frac of Tiles: 0.1267\n",
      "Epoch: 108, Val Loss: 0.5568, Val Omega: 0.0363, Frac of Tiles: 0.0506, Acc: 0.6707, By Label: 0: 0.9348, 1: 0.3333\n",
      "SAVED BEST LOSS\n",
      "Epoch: 109, Train Loss: 0.3370, Train Omega: 0.0962, Frac of Tiles: 0.1327\n",
      "Epoch: 109, Val Loss: 0.5484, Val Omega: 0.0427, Frac of Tiles: 0.0572, Acc: 0.6585, By Label: 0: 0.8913, 1: 0.3611\n",
      "SAVED BEST LOSS\n",
      "Epoch: 110, Train Loss: 0.3161, Train Omega: 0.0984, Frac of Tiles: 0.1365\n",
      "Epoch: 110, Val Loss: 0.5555, Val Omega: 0.0739, Frac of Tiles: 0.0968, Acc: 0.6829, By Label: 0: 0.8478, 1: 0.4722\n",
      "Lambda: 0.8000, Temperature: 2.0000, LR: 0.00001000\n",
      "Epoch: 111, Train Loss: 0.3339, Train Omega: 0.1001, Frac of Tiles: 0.1313\n",
      "Epoch: 111, Val Loss: 0.5463, Val Omega: 0.0555, Frac of Tiles: 0.0724, Acc: 0.6707, By Label: 0: 0.8913, 1: 0.3889\n",
      "Epoch: 112, Train Loss: 0.3211, Train Omega: 0.0985, Frac of Tiles: 0.1280\n",
      "Epoch: 112, Val Loss: 0.5539, Val Omega: 0.0332, Frac of Tiles: 0.0432, Acc: 0.6463, By Label: 0: 0.8913, 1: 0.3333\n",
      "SAVED BEST LOSS\n",
      "Epoch: 113, Train Loss: 0.3408, Train Omega: 0.0820, Frac of Tiles: 0.1074\n",
      "Epoch: 113, Val Loss: 0.5441, Val Omega: 0.0480, Frac of Tiles: 0.0613, Acc: 0.6707, By Label: 0: 0.8913, 1: 0.3889\n",
      "Epoch: 114, Train Loss: 0.3229, Train Omega: 0.0928, Frac of Tiles: 0.1199\n",
      "Epoch: 114, Val Loss: 0.5449, Val Omega: 0.0567, Frac of Tiles: 0.0707, Acc: 0.6585, By Label: 0: 0.8696, 1: 0.3889\n",
      "Epoch: 115, Train Loss: 0.3443, Train Omega: 0.0856, Frac of Tiles: 0.1098\n",
      "Epoch: 115, Val Loss: 0.5512, Val Omega: 0.0659, Frac of Tiles: 0.0817, Acc: 0.6707, By Label: 0: 0.8913, 1: 0.3889\n",
      "Lambda: 0.8500, Temperature: 1.5000, LR: 0.00001000\n",
      "Epoch: 116, Train Loss: 0.3310, Train Omega: 0.0918, Frac of Tiles: 0.1130\n",
      "Epoch: 116, Val Loss: 0.5511, Val Omega: 0.0668, Frac of Tiles: 0.0807, Acc: 0.6463, By Label: 0: 0.8478, 1: 0.3889\n",
      "Epoch: 117, Train Loss: 0.3118, Train Omega: 0.0932, Frac of Tiles: 0.1126\n",
      "Epoch: 117, Val Loss: 0.5659, Val Omega: 0.0849, Frac of Tiles: 0.1009, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "Epoch: 118, Train Loss: 0.3467, Train Omega: 0.0823, Frac of Tiles: 0.1043\n",
      "Epoch: 118, Val Loss: 0.5630, Val Omega: 0.0762, Frac of Tiles: 0.0908, Acc: 0.6707, By Label: 0: 0.8913, 1: 0.3889\n",
      "Epoch: 119, Train Loss: 0.3124, Train Omega: 0.0944, Frac of Tiles: 0.1158\n",
      "Epoch: 119, Val Loss: 0.5704, Val Omega: 0.0819, Frac of Tiles: 0.0968, Acc: 0.6585, By Label: 0: 0.8696, 1: 0.3889\n",
      "Epoch: 120, Train Loss: 0.3437, Train Omega: 0.0857, Frac of Tiles: 0.1023\n",
      "Epoch: 120, Val Loss: 0.5735, Val Omega: 0.0798, Frac of Tiles: 0.0924, Acc: 0.7073, By Label: 0: 0.8478, 1: 0.5278\n",
      "Lambda: 0.9000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 121, Train Loss: 0.3065, Train Omega: 0.0946, Frac of Tiles: 0.1146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121, Val Loss: 0.5563, Val Omega: 0.0691, Frac of Tiles: 0.0796, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Epoch: 122, Train Loss: 0.3169, Train Omega: 0.0885, Frac of Tiles: 0.1068\n",
      "Epoch: 122, Val Loss: 0.5572, Val Omega: 0.0662, Frac of Tiles: 0.0766, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Epoch: 123, Train Loss: 0.3275, Train Omega: 0.0850, Frac of Tiles: 0.1026\n",
      "Epoch: 123, Val Loss: 0.5480, Val Omega: 0.0486, Frac of Tiles: 0.0565, Acc: 0.6707, By Label: 0: 0.8913, 1: 0.3889\n",
      "Epoch: 124, Train Loss: 0.3094, Train Omega: 0.0852, Frac of Tiles: 0.1026\n",
      "Epoch: 124, Val Loss: 0.5558, Val Omega: 0.0589, Frac of Tiles: 0.0663, Acc: 0.6463, By Label: 0: 0.8478, 1: 0.3889\n",
      "Epoch: 125, Train Loss: 0.2946, Train Omega: 0.0940, Frac of Tiles: 0.1132\n",
      "Epoch: 125, Val Loss: 0.5458, Val Omega: 0.0538, Frac of Tiles: 0.0611, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Lambda: 0.9500, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 126, Train Loss: 0.3019, Train Omega: 0.0915, Frac of Tiles: 0.1045\n",
      "Epoch: 126, Val Loss: 0.5405, Val Omega: 0.0462, Frac of Tiles: 0.0513, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "SAVED BEST LOSS\n",
      "Epoch: 127, Train Loss: 0.2918, Train Omega: 0.0900, Frac of Tiles: 0.0986\n",
      "Epoch: 127, Val Loss: 0.5464, Val Omega: 0.0583, Frac of Tiles: 0.0639, Acc: 0.6707, By Label: 0: 0.8913, 1: 0.3889\n",
      "Epoch: 128, Train Loss: 0.3006, Train Omega: 0.0878, Frac of Tiles: 0.1010\n",
      "Epoch: 128, Val Loss: 0.5584, Val Omega: 0.0688, Frac of Tiles: 0.0740, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "Epoch: 129, Train Loss: 0.2894, Train Omega: 0.0914, Frac of Tiles: 0.1036\n",
      "Epoch: 129, Val Loss: 0.5465, Val Omega: 0.0579, Frac of Tiles: 0.0624, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Epoch: 130, Train Loss: 0.3025, Train Omega: 0.0864, Frac of Tiles: 0.0943\n",
      "Epoch: 130, Val Loss: 0.5607, Val Omega: 0.0687, Frac of Tiles: 0.0726, Acc: 0.6707, By Label: 0: 0.8478, 1: 0.4444\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 131, Train Loss: 0.2812, Train Omega: 0.0946, Frac of Tiles: 0.1019\n",
      "Epoch: 131, Val Loss: 0.5533, Val Omega: 0.0644, Frac of Tiles: 0.0675, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "Epoch: 132, Train Loss: 0.3092, Train Omega: 0.0846, Frac of Tiles: 0.0938\n",
      "Epoch: 132, Val Loss: 0.5528, Val Omega: 0.0587, Frac of Tiles: 0.0617, Acc: 0.6707, By Label: 0: 0.8696, 1: 0.4167\n",
      "Epoch: 133, Train Loss: 0.2921, Train Omega: 0.0927, Frac of Tiles: 0.1003\n",
      "Epoch: 133, Val Loss: 0.5504, Val Omega: 0.0573, Frac of Tiles: 0.0599, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Epoch: 134, Train Loss: 0.2896, Train Omega: 0.0918, Frac of Tiles: 0.1013\n",
      "Epoch: 134, Val Loss: 0.5803, Val Omega: 0.0756, Frac of Tiles: 0.0776, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "Epoch: 135, Train Loss: 0.3101, Train Omega: 0.0863, Frac of Tiles: 0.0950\n",
      "Epoch: 135, Val Loss: 0.5472, Val Omega: 0.0488, Frac of Tiles: 0.0514, Acc: 0.6707, By Label: 0: 0.8696, 1: 0.4167\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 136, Train Loss: 0.3158, Train Omega: 0.0786, Frac of Tiles: 0.0823\n",
      "Epoch: 136, Val Loss: 0.5478, Val Omega: 0.0484, Frac of Tiles: 0.0512, Acc: 0.6707, By Label: 0: 0.8913, 1: 0.3889\n",
      "Epoch: 137, Train Loss: 0.2675, Train Omega: 0.0907, Frac of Tiles: 0.0923\n",
      "Epoch: 137, Val Loss: 0.5638, Val Omega: 0.0653, Frac of Tiles: 0.0674, Acc: 0.6707, By Label: 0: 0.8478, 1: 0.4444\n",
      "Epoch: 138, Train Loss: 0.2951, Train Omega: 0.0893, Frac of Tiles: 0.0972\n",
      "Epoch: 138, Val Loss: 0.5709, Val Omega: 0.0663, Frac of Tiles: 0.0689, Acc: 0.6829, By Label: 0: 0.8696, 1: 0.4444\n",
      "Epoch: 139, Train Loss: 0.2947, Train Omega: 0.0829, Frac of Tiles: 0.0880\n",
      "Epoch: 139, Val Loss: 0.5554, Val Omega: 0.0558, Frac of Tiles: 0.0590, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Epoch: 140, Train Loss: 0.2876, Train Omega: 0.0856, Frac of Tiles: 0.0923\n",
      "Epoch: 140, Val Loss: 0.5760, Val Omega: 0.0640, Frac of Tiles: 0.0654, Acc: 0.7317, By Label: 0: 0.8261, 1: 0.6111\n",
      "SAVED BEST ACC\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 141, Train Loss: 0.2835, Train Omega: 0.0847, Frac of Tiles: 0.0975\n",
      "Epoch: 141, Val Loss: 0.5427, Val Omega: 0.0464, Frac of Tiles: 0.0488, Acc: 0.6951, By Label: 0: 0.9130, 1: 0.4167\n",
      "Epoch: 142, Train Loss: 0.2695, Train Omega: 0.0857, Frac of Tiles: 0.0922\n",
      "Epoch: 142, Val Loss: 0.5580, Val Omega: 0.0596, Frac of Tiles: 0.0615, Acc: 0.6707, By Label: 0: 0.8478, 1: 0.4444\n",
      "Epoch: 143, Train Loss: 0.2955, Train Omega: 0.0825, Frac of Tiles: 0.0903\n",
      "Epoch: 143, Val Loss: 0.5769, Val Omega: 0.0652, Frac of Tiles: 0.0682, Acc: 0.6829, By Label: 0: 0.8913, 1: 0.4167\n",
      "Epoch: 144, Train Loss: 0.2948, Train Omega: 0.0841, Frac of Tiles: 0.0936\n",
      "Epoch: 144, Val Loss: 0.5804, Val Omega: 0.0594, Frac of Tiles: 0.0623, Acc: 0.6585, By Label: 0: 0.8478, 1: 0.4167\n",
      "Epoch: 145, Train Loss: 0.2926, Train Omega: 0.0793, Frac of Tiles: 0.0907\n",
      "Epoch: 145, Val Loss: 0.5708, Val Omega: 0.0518, Frac of Tiles: 0.0559, Acc: 0.6951, By Label: 0: 0.9130, 1: 0.4167\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 146, Train Loss: 0.2954, Train Omega: 0.0736, Frac of Tiles: 0.0840\n",
      "Epoch: 146, Val Loss: 0.6151, Val Omega: 0.0723, Frac of Tiles: 0.0755, Acc: 0.7195, By Label: 0: 0.8478, 1: 0.5556\n",
      "Epoch: 147, Train Loss: 0.2787, Train Omega: 0.0867, Frac of Tiles: 0.0928\n",
      "Epoch: 147, Val Loss: 0.6314, Val Omega: 0.0788, Frac of Tiles: 0.0825, Acc: 0.6951, By Label: 0: 0.8478, 1: 0.5000\n",
      "Epoch: 148, Train Loss: 0.2763, Train Omega: 0.0848, Frac of Tiles: 0.0931\n",
      "Epoch: 148, Val Loss: 0.5696, Val Omega: 0.0490, Frac of Tiles: 0.0514, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Epoch: 149, Train Loss: 0.2517, Train Omega: 0.0876, Frac of Tiles: 0.0947\n",
      "Epoch: 149, Val Loss: 0.5862, Val Omega: 0.0594, Frac of Tiles: 0.0618, Acc: 0.6829, By Label: 0: 0.8478, 1: 0.4722\n",
      "Epoch: 150, Train Loss: 0.2540, Train Omega: 0.0928, Frac of Tiles: 0.1005\n",
      "Epoch: 150, Val Loss: 0.5755, Val Omega: 0.0538, Frac of Tiles: 0.0571, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 151, Train Loss: 0.2980, Train Omega: 0.0716, Frac of Tiles: 0.0768\n",
      "Epoch: 151, Val Loss: 0.5772, Val Omega: 0.0532, Frac of Tiles: 0.0556, Acc: 0.6951, By Label: 0: 0.8478, 1: 0.5000\n",
      "Epoch: 152, Train Loss: 0.2780, Train Omega: 0.0812, Frac of Tiles: 0.0917\n",
      "Epoch: 152, Val Loss: 0.5789, Val Omega: 0.0532, Frac of Tiles: 0.0555, Acc: 0.7073, By Label: 0: 0.8478, 1: 0.5278\n",
      "Epoch: 153, Train Loss: 0.2957, Train Omega: 0.0733, Frac of Tiles: 0.0795\n",
      "Epoch: 153, Val Loss: 0.5818, Val Omega: 0.0545, Frac of Tiles: 0.0577, Acc: 0.6951, By Label: 0: 0.8696, 1: 0.4722\n",
      "Epoch: 154, Train Loss: 0.2897, Train Omega: 0.0730, Frac of Tiles: 0.0801\n",
      "Epoch: 154, Val Loss: 0.6043, Val Omega: 0.0660, Frac of Tiles: 0.0694, Acc: 0.6951, By Label: 0: 0.8696, 1: 0.4722\n",
      "Epoch: 155, Train Loss: 0.2824, Train Omega: 0.0812, Frac of Tiles: 0.0881\n",
      "Epoch: 155, Val Loss: 0.6099, Val Omega: 0.0721, Frac of Tiles: 0.0735, Acc: 0.7317, By Label: 0: 0.8478, 1: 0.5833\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 156, Train Loss: 0.2382, Train Omega: 0.0882, Frac of Tiles: 0.0955\n",
      "Epoch: 156, Val Loss: 0.5789, Val Omega: 0.0614, Frac of Tiles: 0.0631, Acc: 0.6829, By Label: 0: 0.8696, 1: 0.4444\n",
      "Epoch: 157, Train Loss: 0.2720, Train Omega: 0.0766, Frac of Tiles: 0.0821\n",
      "Epoch: 157, Val Loss: 0.5656, Val Omega: 0.0502, Frac of Tiles: 0.0512, Acc: 0.7195, By Label: 0: 0.8478, 1: 0.5556\n",
      "Epoch: 158, Train Loss: 0.2612, Train Omega: 0.0812, Frac of Tiles: 0.0911\n",
      "Epoch: 158, Val Loss: 0.5613, Val Omega: 0.0452, Frac of Tiles: 0.0483, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Epoch: 159, Train Loss: 0.2887, Train Omega: 0.0703, Frac of Tiles: 0.0771\n",
      "Epoch: 159, Val Loss: 0.5740, Val Omega: 0.0448, Frac of Tiles: 0.0463, Acc: 0.7195, By Label: 0: 0.8478, 1: 0.5556\n",
      "Epoch: 160, Train Loss: 0.2807, Train Omega: 0.0721, Frac of Tiles: 0.0791\n",
      "Epoch: 160, Val Loss: 0.5788, Val Omega: 0.0534, Frac of Tiles: 0.0563, Acc: 0.6951, By Label: 0: 0.9130, 1: 0.4167\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 161, Train Loss: 0.2678, Train Omega: 0.0746, Frac of Tiles: 0.0843\n",
      "Epoch: 161, Val Loss: 0.6228, Val Omega: 0.0709, Frac of Tiles: 0.0725, Acc: 0.7073, By Label: 0: 0.8478, 1: 0.5278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 162, Train Loss: 0.2736, Train Omega: 0.0780, Frac of Tiles: 0.0846\n",
      "Epoch: 162, Val Loss: 0.5918, Val Omega: 0.0582, Frac of Tiles: 0.0607, Acc: 0.6951, By Label: 0: 0.8696, 1: 0.4722\n",
      "Epoch: 163, Train Loss: 0.2214, Train Omega: 0.0886, Frac of Tiles: 0.0930\n",
      "Epoch: 163, Val Loss: 0.6036, Val Omega: 0.0575, Frac of Tiles: 0.0602, Acc: 0.6829, By Label: 0: 0.8913, 1: 0.4167\n",
      "Epoch: 164, Train Loss: 0.2648, Train Omega: 0.0707, Frac of Tiles: 0.0780\n",
      "Epoch: 164, Val Loss: 0.6193, Val Omega: 0.0623, Frac of Tiles: 0.0635, Acc: 0.7195, By Label: 0: 0.8478, 1: 0.5556\n",
      "Epoch: 165, Train Loss: 0.2800, Train Omega: 0.0722, Frac of Tiles: 0.0815\n",
      "Epoch: 165, Val Loss: 0.5821, Val Omega: 0.0494, Frac of Tiles: 0.0527, Acc: 0.6829, By Label: 0: 0.9130, 1: 0.3889\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 166, Train Loss: 0.2622, Train Omega: 0.0764, Frac of Tiles: 0.0858\n",
      "Epoch: 166, Val Loss: 0.5944, Val Omega: 0.0534, Frac of Tiles: 0.0550, Acc: 0.7317, By Label: 0: 0.8478, 1: 0.5833\n",
      "Epoch: 167, Train Loss: 0.2368, Train Omega: 0.0785, Frac of Tiles: 0.0861\n",
      "Epoch: 167, Val Loss: 0.5711, Val Omega: 0.0490, Frac of Tiles: 0.0521, Acc: 0.6951, By Label: 0: 0.8696, 1: 0.4722\n",
      "Epoch: 168, Train Loss: 0.2439, Train Omega: 0.0748, Frac of Tiles: 0.0803\n",
      "Epoch: 168, Val Loss: 0.5907, Val Omega: 0.0489, Frac of Tiles: 0.0506, Acc: 0.7317, By Label: 0: 0.8478, 1: 0.5833\n",
      "Epoch: 169, Train Loss: 0.2547, Train Omega: 0.0751, Frac of Tiles: 0.0811\n",
      "Epoch: 169, Val Loss: 0.5912, Val Omega: 0.0549, Frac of Tiles: 0.0572, Acc: 0.7195, By Label: 0: 0.8696, 1: 0.5278\n",
      "Epoch: 170, Train Loss: 0.2410, Train Omega: 0.0795, Frac of Tiles: 0.0855\n",
      "Epoch: 170, Val Loss: 0.5885, Val Omega: 0.0510, Frac of Tiles: 0.0525, Acc: 0.7073, By Label: 0: 0.8478, 1: 0.5278\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 171, Train Loss: 0.2598, Train Omega: 0.0716, Frac of Tiles: 0.0784\n",
      "Epoch: 171, Val Loss: 0.5847, Val Omega: 0.0438, Frac of Tiles: 0.0458, Acc: 0.7195, By Label: 0: 0.8696, 1: 0.5278\n",
      "Epoch: 172, Train Loss: 0.2327, Train Omega: 0.0703, Frac of Tiles: 0.0773\n",
      "Epoch: 172, Val Loss: 0.6000, Val Omega: 0.0502, Frac of Tiles: 0.0524, Acc: 0.7073, By Label: 0: 0.8696, 1: 0.5000\n",
      "Epoch: 173, Train Loss: 0.2405, Train Omega: 0.0699, Frac of Tiles: 0.0769\n",
      "Epoch: 173, Val Loss: 0.5861, Val Omega: 0.0459, Frac of Tiles: 0.0482, Acc: 0.7195, By Label: 0: 0.8696, 1: 0.5278\n",
      "Epoch: 174, Train Loss: 0.2639, Train Omega: 0.0707, Frac of Tiles: 0.0761\n",
      "Epoch: 174, Val Loss: 0.6147, Val Omega: 0.0587, Frac of Tiles: 0.0603, Acc: 0.6951, By Label: 0: 0.8261, 1: 0.5278\n",
      "Epoch: 175, Train Loss: 0.2448, Train Omega: 0.0749, Frac of Tiles: 0.0841\n",
      "Epoch: 175, Val Loss: 0.5796, Val Omega: 0.0448, Frac of Tiles: 0.0457, Acc: 0.7073, By Label: 0: 0.8261, 1: 0.5556\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 176, Train Loss: 0.2638, Train Omega: 0.0737, Frac of Tiles: 0.0788\n",
      "Epoch: 176, Val Loss: 0.6078, Val Omega: 0.0547, Frac of Tiles: 0.0568, Acc: 0.6829, By Label: 0: 0.8696, 1: 0.4444\n",
      "Epoch: 177, Train Loss: 0.2708, Train Omega: 0.0700, Frac of Tiles: 0.0734\n",
      "Epoch: 177, Val Loss: 0.6016, Val Omega: 0.0532, Frac of Tiles: 0.0539, Acc: 0.7195, By Label: 0: 0.8261, 1: 0.5833\n",
      "Epoch: 178, Train Loss: 0.2634, Train Omega: 0.0676, Frac of Tiles: 0.0736\n",
      "Epoch: 178, Val Loss: 0.5945, Val Omega: 0.0500, Frac of Tiles: 0.0524, Acc: 0.6951, By Label: 0: 0.8696, 1: 0.4722\n",
      "Epoch: 179, Train Loss: 0.2672, Train Omega: 0.0645, Frac of Tiles: 0.0721\n",
      "Epoch: 179, Val Loss: 0.6074, Val Omega: 0.0510, Frac of Tiles: 0.0533, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Epoch: 180, Train Loss: 0.2789, Train Omega: 0.0636, Frac of Tiles: 0.0701\n",
      "Epoch: 180, Val Loss: 0.6127, Val Omega: 0.0514, Frac of Tiles: 0.0525, Acc: 0.7073, By Label: 0: 0.8261, 1: 0.5556\n",
      "Lambda: 1.0000, Temperature: 1.0000, LR: 0.00001000\n",
      "Epoch: 181, Train Loss: 0.2528, Train Omega: 0.0686, Frac of Tiles: 0.0769\n",
      "Epoch: 181, Val Loss: 0.5945, Val Omega: 0.0469, Frac of Tiles: 0.0493, Acc: 0.7073, By Label: 0: 0.8696, 1: 0.5000\n",
      "Epoch: 182, Train Loss: 0.2475, Train Omega: 0.0682, Frac of Tiles: 0.0773\n",
      "Epoch: 182, Val Loss: 0.6223, Val Omega: 0.0511, Frac of Tiles: 0.0525, Acc: 0.7073, By Label: 0: 0.8261, 1: 0.5556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-230a1d4ddbb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     training_loop_rationales_gs(e, step_size, optimizer, gen, enc, pool_fn, train_embeddings, train_jpgs_to_slide, \n\u001b[0;32m---> 10\u001b[0;31m                                 train_labels, criterion, n_samples_train, sample_weight, lamb1, lamb2, temp)\n\u001b[0m\u001b[1;32m     11\u001b[0m     loss, acc, frac = validation_loop_rationales_gs(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, \n\u001b[1;32m     12\u001b[0m                                               val_jpgs_to_slide, val_labels, criterion, n_samples_val, lamb1, lamb2, temp)\n",
      "\u001b[0;32m<ipython-input-12-329f19a089d6>\u001b[0m in \u001b[0;36mtraining_loop_rationales_gs\u001b[0;34m(e, step_size, optimizer, gen, enc, pool_fn, train_embeddings, train_jpgs_to_slide, train_labels, criterion, n_samples, sample_weight, lamb1, lamb2, temp)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mbceloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbceloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0momega\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 1e8\n",
    "best_acc = 0.0\n",
    "frac_threshold = 0.5\n",
    "path_acc_gen = '/n/tcga_models/COAD_rationale_model_gen_5_10_acc.pt'\n",
    "path_acc_enc = '/n/tcga_models/COAD_rationale_model_enc_5_10_acc.pt'\n",
    "path_loss_gen = '/n/tcga_models/COAD_rationale_model_gen_5_10.pt'\n",
    "path_loss_enc = '/n/tcga_models/COAD_rationale_model_enc_5_10.pt'\n",
    "for e in range(500):\n",
    "    training_loop_rationales_gs(e, step_size, optimizer, gen, enc, pool_fn, train_embeddings, train_jpgs_to_slide, \n",
    "                                train_labels, criterion, n_samples_train, sample_weight, lamb1, lamb2, temp)\n",
    "    loss, acc, frac = validation_loop_rationales_gs(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, \n",
    "                                              val_jpgs_to_slide, val_labels, criterion, n_samples_val, lamb1, lamb2, temp)\n",
    "    if loss < best_loss and frac < frac_threshold:\n",
    "        torch.save(gen.state_dict(), path_loss_gen)\n",
    "        torch.save(enc.state_dict(), path_loss_enc)\n",
    "        best_loss = loss\n",
    "        print('SAVED BEST LOSS')\n",
    "    if acc > best_acc and frac < frac_threshold:\n",
    "        torch.save(gen.state_dict(), path_acc_gen)\n",
    "        torch.save(enc.state_dict(), path_acc_enc)\n",
    "        best_acc = acc\n",
    "        print('SAVED BEST ACC')\n",
    "    if e > 30:\n",
    "        lamb1 += 0.01\n",
    "        temp -= 0.1\n",
    "    temp = np.max([temp, 1.0])\n",
    "    lamb1 = np.min([lamb1, 1.0])\n",
    "    if e % 5 == 0:\n",
    "        print('Lambda: {0:0.4f}, Temperature: {1:0.4f}, LR: {2:0.8f}'.format(lamb1, temp, optimizer.state_dict()['param_groups'][0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
