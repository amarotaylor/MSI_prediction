{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import model_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = '/n/data_labeled_histopathology_images/COAD/train.pkl'\n",
    "with open(pickle_file, 'rb') as f: \n",
    "    train_embeddings, train_labels, train_jpgs_to_slide = pickle.load(f)\n",
    "    \n",
    "pickle_file = '/n/data_labeled_histopathology_images/COAD/val.pkl'\n",
    "with open(pickle_file, 'rb') as f: \n",
    "    val_embeddings, val_labels, val_jpgs_to_slide = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20): \n",
    "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "    U = torch.rand(shape,dtype=torch.float32,device='cuda')\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature): \n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape)\n",
    "    return F.softmax( y / temperature,dim=1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"\n",
    "    Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        temperature: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if hard:\n",
    "        y = torch.argmax(logits,dim=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2048\n",
    "hidden_size = 2048\n",
    "output_size_gen = 2\n",
    "output_size_enc = 1\n",
    "\n",
    "gen = model_utils.Generator(input_size, hidden_size, output_size_gen, dropout=0.5)\n",
    "enc = model_utils.Encoder(input_size, hidden_size, output_size_enc, pool_fn, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 10\n",
    "lamb1 = 0\n",
    "lamb2 = 0\n",
    "temp = 10\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "params = list(enc.parameters()) + list(gen.parameters())\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(params, lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_val = val_jpgs_to_slide.max()\n",
    "n_samples_train = train_jpgs_to_slide.max()\n",
    "idxs_train = np.linspace(0,n_samples_train,n_samples_train+1,dtype=int)\n",
    "labels_to_idxs_train = np.concatenate([(train_labels[train_jpgs_to_slide==i]).unique().numpy() for i in idxs_train])\n",
    "weights = 1/np.sum(labels_to_idxs_train==0),1/np.sum(labels_to_idxs_train==1)\n",
    "sample_weight = [weights[l] for l in labels_to_idxs_train]\n",
    "sample_weight = sample_weight/np.sum(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = train_embeddings.cuda()\n",
    "val_embeddings = val_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (d): Dropout(p=0.5)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.cuda()\n",
    "enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsm = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_loop_vars(step_size):\n",
    "    logits_vec = torch.zeros((step_size+1,1)).cuda()\n",
    "    labels_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    znorm_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    zdist_vec = torch.zeros_like(logits_vec).cuda()\n",
    "    batch_idx = 0\n",
    "    return logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_rationales_gs(e, step_size, optimizer, gen, enc, pool_fn, train_embeddings, train_jpgs_to_slide, \n",
    "                                train_labels, criterion, n_samples, sample_weight, lamb1, lamb2, temp):\n",
    "    gen.train()\n",
    "    enc.train()\n",
    "    \n",
    "    logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(step_size)\n",
    "    track_loss = 0   \n",
    "    track_omega = 0\n",
    "    rat_tiles = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    idxs_train = np.linspace(0,n_samples,n_samples+1,dtype=int)\n",
    "    idexs = np.random.choice(idxs_train,size=n_samples.numpy(),p=sample_weight)\n",
    "    \n",
    "    for idx in idexs:\n",
    "        slide = train_embeddings[train_jpgs_to_slide==idx] \n",
    "        labels_vec[batch_idx] = train_labels[train_jpgs_to_slide==idx].unique().float().cuda()\n",
    "        \n",
    "        preds = gen(slide) \n",
    "        logits = lsm(preds)\n",
    "        sample = gumbel_softmax(logits, temperature=temp)\n",
    "        rationale = slide * sample[:,1].unsqueeze(1)\n",
    "        \n",
    "        # predict class based on rationales\n",
    "        logits = enc(rationale)\n",
    "        logits_vec[batch_idx] = logits\n",
    "        \n",
    "        znorm = torch.sum(sample[:,1])\n",
    "        znorm_vec[batch_idx] = znorm / sample.shape[0]\n",
    "        \n",
    "        rat_tiles += znorm.detach().cpu().numpy()\n",
    "        total_tiles += sample.shape[0]\n",
    "        \n",
    "        zdist = torch.sum(torch.abs(sample[:-1,1] - sample[1:,1]))\n",
    "        zdist_vec[batch_idx] = zdist / sample.shape[0]\n",
    "        \n",
    "        if batch_idx == step_size:\n",
    "            # compute loss and regularization term\n",
    "            omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / step_size\n",
    "            bceloss = criterion(logits_vec, labels_vec)\n",
    "            loss = bceloss + omega\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            optimizer.zero_grad()\n",
    "            track_loss += bceloss.detach().cpu().numpy()\n",
    "            track_omega += omega.detach().cpu().numpy()\n",
    "            logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(step_size)\n",
    "        else:\n",
    "            batch_idx += 1\n",
    "            \n",
    "    omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / batch_idx\n",
    "    bceloss = criterion(logits_vec, labels_vec)\n",
    "    loss = bceloss + omega\n",
    "    loss.backward()\n",
    "    track_loss += bceloss.detach().cpu().numpy()\n",
    "    track_loss = track_loss * (float(step_size) / float(n_samples))\n",
    "    track_omega += omega.detach().cpu().numpy()\n",
    "    track_omega = track_omega * (float(step_size) / float(n_samples))\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "       \n",
    "    frac_tiles = rat_tiles / total_tiles\n",
    "    print('Epoch: {0}, Train Loss: {1:0.4f}, Train Omega: {2:0.4f}, Frac of Tiles: {3:0.4f}'.format(e, track_loss, \n",
    "                                                                                                   track_omega, \n",
    "                                                                                                   frac_tiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop_rationales_gs(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, val_jpgs_to_slide, \n",
    "                                  val_labels, criterion, n_samples, lamb1, lamb2, temp):\n",
    "    gen.eval()\n",
    "    enc.eval()\n",
    "    \n",
    "    logits_vec, labels_vec, znorm_vec, zdist_vec, batch_idx = initialize_loop_vars(n_samples)\n",
    "    rat_tiles = 0\n",
    "    total_tiles = 0\n",
    "    \n",
    "    idexs = np.linspace(0,n_samples,n_samples+1,dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for idx in idexs:\n",
    "            slide = val_embeddings[val_jpgs_to_slide==idx] # num_tiles x 2048\n",
    "            labels_vec[idx] = val_labels[val_jpgs_to_slide==idx].unique().float().cuda()\n",
    "\n",
    "            preds = gen(slide)\n",
    "            sample = torch.argmax(preds, dim=1).float()\n",
    "            rationale = slide * sample.unsqueeze(1)\n",
    "\n",
    "            # predict class based on rationales\n",
    "            logits = enc(rationale)\n",
    "            logits_vec[idx] = logits\n",
    "\n",
    "            znorm = torch.sum(sample)\n",
    "            znorm_vec[idx] = znorm / sample.shape[0]\n",
    "\n",
    "            rat_tiles += znorm.detach().cpu().numpy()\n",
    "            total_tiles += sample.shape[0]\n",
    "\n",
    "            zdist = torch.sum(torch.abs(sample[:-1] - sample[1:]))\n",
    "            zdist_vec[idx] = zdist / sample.shape[0]\n",
    "\n",
    "    # compute loss and regularization term\n",
    "    omega = ((lamb1 * znorm_vec.sum()) + (lamb2 * zdist_vec.sum())) / n_samples\n",
    "    bceloss = criterion(logits_vec, labels_vec)\n",
    "    loss = bceloss + omega\n",
    "    frac_tiles = rat_tiles / total_tiles\n",
    "    \n",
    "    mask = labels_vec.cpu().numpy() == (logits_vec>0.5).float().cpu().numpy()\n",
    "    acc = np.mean(mask)\n",
    "    acc_1 = np.mean(mask[labels_vec.cpu().numpy()==1.])\n",
    "    acc_0 = np.mean(mask[labels_vec.cpu().numpy()==0.])\n",
    "    print('Epoch: {0}, Val Loss: {1:0.4f}, Val Omega: {2:0.4f}, Frac of Tiles: {3:0.4f}, Acc: {4:0.4f}, By Label: 0: {5:0.4f}, 1: {6:0.4f}'.format(e, bceloss, omega, frac_tiles, acc, acc_0, acc_1))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.5549, Train Omega: 0.0000, Frac of Tiles: 0.7587\n",
      "Epoch: 0, Val Loss: 0.6300, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.9130, 1: 0.3333\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 1, Train Loss: 0.4968, Train Omega: 0.0000, Frac of Tiles: 0.8939\n",
      "Epoch: 1, Val Loss: 0.6100, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.5854, By Label: 0: 1.0000, 1: 0.0556\n",
      "Epoch: 2, Train Loss: 0.4895, Train Omega: 0.0000, Frac of Tiles: 0.9403\n",
      "Epoch: 2, Val Loss: 0.5999, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.9348, 1: 0.3611\n",
      "Epoch: 3, Train Loss: 0.4565, Train Omega: 0.0000, Frac of Tiles: 0.9590\n",
      "Epoch: 3, Val Loss: 0.5969, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8696, 1: 0.4167\n",
      "Epoch: 4, Train Loss: 0.4355, Train Omega: 0.0000, Frac of Tiles: 0.9730\n",
      "Epoch: 4, Val Loss: 0.5806, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.9130, 1: 0.3611\n",
      "Epoch: 5, Train Loss: 0.4414, Train Omega: 0.0000, Frac of Tiles: 0.9783\n",
      "Epoch: 5, Val Loss: 0.5830, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6463, By Label: 0: 0.9348, 1: 0.2778\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 6, Train Loss: 0.3898, Train Omega: 0.0000, Frac of Tiles: 0.9828\n",
      "Epoch: 6, Val Loss: 0.5933, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6220, By Label: 0: 0.9348, 1: 0.2222\n",
      "Epoch: 7, Train Loss: 0.3889, Train Omega: 0.0000, Frac of Tiles: 0.9864\n",
      "Epoch: 7, Val Loss: 0.5823, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.9348, 1: 0.3611\n",
      "Epoch: 8, Train Loss: 0.3519, Train Omega: 0.0000, Frac of Tiles: 0.9889\n",
      "Epoch: 8, Val Loss: 0.5871, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6463, By Label: 0: 0.8478, 1: 0.3889\n",
      "Epoch: 9, Train Loss: 0.3641, Train Omega: 0.0000, Frac of Tiles: 0.9899\n",
      "Epoch: 9, Val Loss: 0.5907, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6463, By Label: 0: 0.8696, 1: 0.3611\n",
      "Epoch: 10, Train Loss: 0.3512, Train Omega: 0.0000, Frac of Tiles: 0.9911\n",
      "Epoch: 10, Val Loss: 0.6000, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Lambda: 0.0000, Temperature: 10.0000, LR: 0.00001000\n",
      "Epoch: 11, Train Loss: 0.3583, Train Omega: 0.0000, Frac of Tiles: 0.9928\n",
      "Epoch: 11, Val Loss: 0.5982, Val Omega: 0.0000, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Epoch: 12, Train Loss: 0.3789, Train Omega: 0.0101, Frac of Tiles: 0.9934\n",
      "Epoch: 12, Val Loss: 0.6156, Val Omega: 0.0101, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Epoch: 13, Train Loss: 0.3545, Train Omega: 0.0203, Frac of Tiles: 0.9942\n",
      "Epoch: 13, Val Loss: 0.6154, Val Omega: 0.0202, Frac of Tiles: 1.0000, Acc: 0.6829, By Label: 0: 0.8261, 1: 0.5000\n",
      "Epoch: 14, Train Loss: 0.3577, Train Omega: 0.0304, Frac of Tiles: 0.9948\n",
      "Epoch: 14, Val Loss: 0.6136, Val Omega: 0.0304, Frac of Tiles: 1.0000, Acc: 0.6585, By Label: 0: 0.8261, 1: 0.4444\n",
      "Epoch: 15, Train Loss: 0.3838, Train Omega: 0.0405, Frac of Tiles: 0.9949\n",
      "Epoch: 15, Val Loss: 0.6183, Val Omega: 0.0405, Frac of Tiles: 1.0000, Acc: 0.6463, By Label: 0: 0.8261, 1: 0.4167\n",
      "Lambda: 0.0500, Temperature: 9.5000, LR: 0.00001000\n",
      "Epoch: 16, Train Loss: 0.3226, Train Omega: 0.0507, Frac of Tiles: 0.9957\n",
      "Epoch: 16, Val Loss: 0.6248, Val Omega: 0.0506, Frac of Tiles: 1.0000, Acc: 0.6341, By Label: 0: 0.8478, 1: 0.3611\n",
      "Epoch: 17, Train Loss: 0.3509, Train Omega: 0.0609, Frac of Tiles: 0.9962\n",
      "Epoch: 17, Val Loss: 0.6321, Val Omega: 0.0607, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.8261, 1: 0.5278\n",
      "Epoch: 18, Train Loss: 0.3326, Train Omega: 0.0710, Frac of Tiles: 0.9958\n",
      "Epoch: 18, Val Loss: 0.6218, Val Omega: 0.0709, Frac of Tiles: 1.0000, Acc: 0.6463, By Label: 0: 0.8261, 1: 0.4167\n",
      "Epoch: 19, Train Loss: 0.3359, Train Omega: 0.0812, Frac of Tiles: 0.9961\n",
      "Epoch: 19, Val Loss: 0.6464, Val Omega: 0.0810, Frac of Tiles: 1.0000, Acc: 0.7195, By Label: 0: 0.7826, 1: 0.6389\n",
      "Epoch: 20, Train Loss: 0.3042, Train Omega: 0.0913, Frac of Tiles: 0.9963\n",
      "Epoch: 20, Val Loss: 0.6343, Val Omega: 0.0911, Frac of Tiles: 1.0000, Acc: 0.7317, By Label: 0: 0.8261, 1: 0.6111\n",
      "Lambda: 0.1000, Temperature: 9.0000, LR: 0.00001000\n",
      "Epoch: 21, Train Loss: 0.3230, Train Omega: 0.1015, Frac of Tiles: 0.9963\n",
      "Epoch: 21, Val Loss: 0.6393, Val Omega: 0.1012, Frac of Tiles: 1.0000, Acc: 0.7439, By Label: 0: 0.8043, 1: 0.6667\n",
      "Epoch: 22, Train Loss: 0.3239, Train Omega: 0.1116, Frac of Tiles: 0.9961\n",
      "Epoch: 22, Val Loss: 0.6272, Val Omega: 0.1114, Frac of Tiles: 1.0000, Acc: 0.6220, By Label: 0: 0.8261, 1: 0.3611\n",
      "Epoch: 23, Train Loss: 0.3316, Train Omega: 0.1217, Frac of Tiles: 0.9959\n",
      "Epoch: 23, Val Loss: 0.6236, Val Omega: 0.1215, Frac of Tiles: 1.0000, Acc: 0.7073, By Label: 0: 0.8261, 1: 0.5556\n",
      "Epoch: 24, Train Loss: 0.3329, Train Omega: 0.1318, Frac of Tiles: 0.9958\n",
      "Epoch: 24, Val Loss: 0.6196, Val Omega: 0.1316, Frac of Tiles: 1.0000, Acc: 0.6707, By Label: 0: 0.8261, 1: 0.4722\n",
      "Epoch: 25, Train Loss: 0.3043, Train Omega: 0.1420, Frac of Tiles: 0.9954\n",
      "Epoch: 25, Val Loss: 0.6334, Val Omega: 0.1417, Frac of Tiles: 1.0000, Acc: 0.6951, By Label: 0: 0.8261, 1: 0.5278\n",
      "Lambda: 0.1500, Temperature: 8.5000, LR: 0.00001000\n",
      "Epoch: 26, Train Loss: 0.3167, Train Omega: 0.1520, Frac of Tiles: 0.9947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ce7f4b40e28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 train_labels, criterion, n_samples_train, sample_weight, lamb1, lamb2, temp)\n\u001b[1;32m      4\u001b[0m     loss, acc = validation_loop_rationales_gs(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, \n\u001b[0;32m----> 5\u001b[0;31m                                               val_jpgs_to_slide, val_labels, criterion, n_samples_val, lamb1, lamb2, temp)\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlamb1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-857cb17c7d78>\u001b[0m in \u001b[0;36mvalidation_loop_rationales_gs\u001b[0;34m(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, val_jpgs_to_slide, val_labels, criterion, n_samples, lamb1, lamb2, temp)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mznorm_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mznorm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mrat_tiles\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mznorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mtotal_tiles\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    training_loop_rationales_gs(e, step_size, optimizer, gen, enc, pool_fn, train_embeddings, train_jpgs_to_slide, \n",
    "                                train_labels, criterion, n_samples_train, sample_weight, lamb1, lamb2, temp)\n",
    "    loss, acc = validation_loop_rationales_gs(e, step_size, scheduler, gen, enc, pool_fn, val_embeddings, \n",
    "                                              val_jpgs_to_slide, val_labels, criterion, n_samples_val, lamb1, lamb2, temp)\n",
    "    if e > 10:\n",
    "        lamb1 += 0.01\n",
    "        temp -= 0.1\n",
    "    temp = np.max([temp, 1.0])\n",
    "    lamb1 = np.min([lamb1, 1.0])\n",
    "    if e % 5 == 0:\n",
    "        print('Lambda: {0:0.4f}, Temperature: {1:0.4f}, LR: {2:0.8f}'.format(lamb1, temp, optimizer.state_dict()['param_groups'][0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
