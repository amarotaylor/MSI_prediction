{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import accimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms, set_image_backend, get_image_backend\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import train_utils\n",
    "import data_utils\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "set_image_backend('accimage')\n",
    "import data_utils\n",
    "import train_utils\n",
    "root_dir_coad = '/n/mounted-data-drive/COAD/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_train, sa_val = data_utils.load_COAD_train_val_sa_pickle()\n",
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "magnification = '10.0'\n",
    "batch_type = 'slide'\n",
    "\n",
    "train_transform = train_utils.transform_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCGADataset_tiled_slides(Dataset):\n",
    "    \"\"\"\n",
    "    TCGA slide dataset. Each slide is linked to its tiles via a label.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_annotations, root_dir, transform=None, loader=data_utils.default_loader, magnification='5.0'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sample_annot (dict): dictionary of sample names and their respective labels.\n",
    "            root_dir (string): directory containing all of the samples and their respective images.\n",
    "            transform (callable, optional): optional transform to be applied on the images of a sample.\n",
    "            loader specifies image backend: use accimage\n",
    "            magnification: tile magnification\n",
    "        \"\"\"\n",
    "        self.sample_names = list(sample_annotations.keys())\n",
    "        self.sample_labels = list(sample_annotations.values())\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "        self.magnification = magnification\n",
    "        self.batch_type = batch_type\n",
    "        self.img_dirs = [self.root_dir + sample_name + '.svs/' \\\n",
    "                         + sample_name + '_files/' + self.magnification for sample_name in self.sample_names]\n",
    "        self.jpegs = [os.listdir(img_dir) for img_dir in self.img_dirs]\n",
    "        self.all_jpegs = []\n",
    "        self.all_labels = []\n",
    "        self.jpg_to_sample = []\n",
    "        self.coords = []\n",
    "        \n",
    "        for idx,(im_dir,label,l) in enumerate(zip(self.img_dirs,self.sample_labels,self.jpegs)):\n",
    "            sample_coords = []\n",
    "            for jpeg in l:\n",
    "                # build tile dataset\n",
    "                self.all_jpegs.append(im_dir+'/'+jpeg)\n",
    "                # label for each tile\n",
    "                self.all_labels.append(label)\n",
    "                # tracks slide membership at a tile level\n",
    "                self.jpg_to_sample.append(idx)\n",
    "                # store tile coordinates\n",
    "                x,y = jpeg[:-5].split('_') # 'X_Y.jpeg'\n",
    "                x,y = int(x), int(y)\n",
    "                self.coords.append(torch.tensor([x,y]))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_jpegs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.loader(self.all_jpegs[idx])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        if image.shape[1] < 256 or image.shape[2] < 256:\n",
    "            image = data_utils.pad_tensor_up_to(image,256,256,channels_last=False)\n",
    "        return image, self.all_labels[idx], self.coords[idx],self.jpg_to_sample[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TCGADataset_tiled_slides(sa_train, root_dir, transform=train_transform, magnification='5.0')\n",
    "train_loader = DataLoader(train_set, batch_size=512, pin_memory=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_file = '/n/tcga_models/resnet18_WGD_10x.pt'\n",
    "device = torch.device('cuda', 0)\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(2048, output_shape, bias=True)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc: storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "resnet.fc = nn.Linear(2048, 2048, bias=False)\n",
    "resnet.fc.weight.data = torch.eye(2048)\n",
    "resnet.cuda(device=device)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_level_classification_layer = nn.Linear(2048,1)\n",
    "slide_level_classification_layer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0\n",
    "learning_rate = 1e-4\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(slide_level_classification_layer.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, min_lr=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# track number of slides seen\n",
    "p_update = torch.tensor(0.,device=device)\n",
    "# store embeddings, labels, and memberships\n",
    "embeddings = []\n",
    "slide_labels = []\n",
    "slide_membership = []\n",
    "step = 0\n",
    "batches = 0\n",
    "\n",
    "for batch,labels,coords,idxs in train_loader:\n",
    "    # get embeddings\n",
    "    batch,labels,coords,idxs = batch.cuda(),labels.cuda(),coords.cuda(),idxs.cuda()\n",
    "    if len(embeddings) == 0:\n",
    "        current_slide = torch.min(idxs)\n",
    "    # append each batched results\n",
    "    embeddings.extend(resnet(batch))\n",
    "    slide_membership.extend(idxs)\n",
    "    slide_labels.extend(labels)\n",
    "    \n",
    "    p_update += 0.0001\n",
    "    batches+=1\n",
    "    \n",
    "    if torch.rand(1,device=device) < p_update:\n",
    "        slide_membership = torch.stack(slide_membership)\n",
    "        slides = list(set(torch.squeeze(slide_membership).detach().cpu().numpy()))\n",
    "        loss = torch.tensor(0.,device=device)\n",
    "        embeddings = torch.stack(embeddings)\n",
    "        slide_labels = torch.stack(slide_labels)\n",
    "        for slide in slides:\n",
    "            pooled = pool_fn(embeddings[slide_membership == slide,:])\n",
    "            logits = slide_level_classification_layer(pooled)\n",
    "            loss += criterion(logits,slide_labels[slide_membership == slide][0].float().view(-1))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    \n",
    "        embeddings = []\n",
    "        slide_labels = []\n",
    "        print(step,batches,torch.max(slide_membership).detach().cpu().numpy())\n",
    "        slide_membership = []\n",
    "        step+=1\n",
    "        batches = 0\n",
    "        p_update = torch.tensor(0.,device=device)\n",
    "        \n",
    "slide_membership = torch.stack(slide_membership)\n",
    "slides = list(set(torch.squeeze(slide_membership).detach().cpu().numpy()))\n",
    "loss = torch.tensor(0.,device=device)\n",
    "embeddings = torch.stack(embeddings)\n",
    "slide_labels = torch.stack(slide_labels)\n",
    "for slide in slides:\n",
    "    pooled = pool_fn(embeddings[slide_membership == slide,:])\n",
    "    logits = slide_level_classification_layer(pooled)\n",
    "    loss += criterion(logits,slide_labels[slide_membership == slide][0].float().view(-1))\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(torch.stack(slide_membership).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
