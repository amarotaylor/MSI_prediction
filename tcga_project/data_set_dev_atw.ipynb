{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import accimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms, set_image_backend, get_image_backend\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import train_utils\n",
    "import data_utils\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "set_image_backend('accimage')\n",
    "import data_utils\n",
    "import train_utils\n",
    "root_dir_coad = '/n/mounted-data-drive/COAD/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_train, sa_val = data_utils.load_COAD_train_val_sa_pickle()\n",
    "root_dir = '/n/mounted-data-drive/COAD/'\n",
    "magnification = '10.0'\n",
    "batch_type = 'slide'\n",
    "\n",
    "train_transform = train_utils.transform_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCGADataset_tiled_slides(Dataset):\n",
    "    \"\"\"\n",
    "    TCGA slide dataset. Each slide is linked to its tiles via a label.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_annotations, root_dir, transform=None, loader=data_utils.default_loader, magnification='5.0'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sample_annot (dict): dictionary of sample names and their respective labels.\n",
    "            root_dir (string): directory containing all of the samples and their respective images.\n",
    "            transform (callable, optional): optional transform to be applied on the images of a sample.\n",
    "            loader specifies image backend: use accimage\n",
    "            magnification: tile magnification\n",
    "        \"\"\"\n",
    "        self.sample_names = list(sample_annotations.keys())\n",
    "        self.sample_labels = list(sample_annotations.values())\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "        self.magnification = magnification\n",
    "        self.batch_type = batch_type\n",
    "        self.img_dirs = [self.root_dir + sample_name + '.svs/' \\\n",
    "                         + sample_name + '_files/' + self.magnification for sample_name in self.sample_names]\n",
    "        self.jpegs = [os.listdir(img_dir) for img_dir in self.img_dirs]\n",
    "        self.all_jpegs = []\n",
    "        self.all_labels = []\n",
    "        self.jpg_to_sample = []\n",
    "        self.coords = []\n",
    "        \n",
    "        for idx,(im_dir,label,l) in enumerate(zip(self.img_dirs,self.sample_labels,self.jpegs)):\n",
    "            sample_coords = []\n",
    "            for jpeg in l:\n",
    "                # build tile dataset\n",
    "                self.all_jpegs.append(im_dir+'/'+jpeg)\n",
    "                # label for each tile\n",
    "                self.all_labels.append(label)\n",
    "                # tracks slide membership at a tile level\n",
    "                self.jpg_to_sample.append(idx)\n",
    "                # store tile coordinates\n",
    "                x,y = jpeg[:-5].split('_') # 'X_Y.jpeg'\n",
    "                x,y = int(x), int(y)\n",
    "                self.coords.append(torch.tensor([x,y]))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_jpegs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.loader(self.all_jpegs[idx])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        if image.shape[1] < 256 or image.shape[2] < 256:\n",
    "            image = data_utils.pad_tensor_up_to(image,256,256,channels_last=False)\n",
    "        return image, self.all_labels[idx], self.coords[idx],self.jpg_to_sample[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TCGADataset_tiled_slides(sa_train, root_dir, transform=train_transform, magnification='5.0')\n",
    "train_loader = DataLoader(train_set, batch_size=512, pin_memory=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_file = '/n/tcga_models/resnet18_WGD_10x.pt'\n",
    "device = torch.device('cuda', 0)\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(2048, output_shape, bias=True)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc: storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "resnet.fc = nn.Linear(2048, 2048, bias=False)\n",
    "resnet.fc.weight.data = torch.eye(2048)\n",
    "resnet.cuda(device=device)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pool_and_classify(\n",
       "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v\n",
    "\n",
    "class pool_and_classify(nn.Module):\n",
    "    def __init__(self, pool_fn, n_inputs, n_outputs):\n",
    "        super(pool_and_classify, self).__init__()\n",
    "        self.fc = nn.Linear(n_inputs,n_outputs)\n",
    "        self.pool = pool_fn\n",
    "    def forward(self,h,embeddings,slides):\n",
    "        embeddings = [torch.masked_select(embeddings,(slide_membership == slide).view(-1,1)).view(-1,2048) for slide in slides]\n",
    "        pooled = [self.pool(h) for h in embeddings]\n",
    "        logits = self.fc(torch.stack(pooled))\n",
    "        return logits\n",
    "    \n",
    "slide_level_classification_layer = pool_and_classify(pool_fn,2048,1)\n",
    "slide_level_classification_layer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0\n",
    "learning_rate = 1e-4\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(slide_level_classification_layer.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, min_lr=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1fdc4d76eb52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mp_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mslide_membership\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslide_membership\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mslides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslide_membership\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "# track number of slides seen\n",
    "p_update = torch.tensor(0.,device=device)\n",
    "# store embeddings, labels, and memberships\n",
    "embeddings = []\n",
    "slide_membership = []\n",
    "step = 0\n",
    "batches = 0\n",
    "slide_labels = torch.tensor(train_set.sample_labels, device=device)\n",
    "for batch,labels,coords,idxs in train_loader:\n",
    "    # get embeddings\n",
    "    batch,labels,coords,idxs = batch.cuda(),labels.cuda(),coords.cuda(),idxs.cuda()\n",
    "    if len(embeddings) == 0:\n",
    "        current_slide = torch.min(idxs)\n",
    "    # append each batched results\n",
    "    embeddings.extend(resnet(batch))\n",
    "    slide_membership.extend(idxs)\n",
    "    \n",
    "    p_update += 0.0001\n",
    "    batches+=1\n",
    "    \n",
    "    if torch.rand(1,device=device) < p_update:\n",
    "        slide_membership = torch.stack(slide_membership)\n",
    "        slides = torch.unique(torch.tensor(slide_membership),sorted=True)\n",
    "        loss = torch.tensor(0.,device=device)\n",
    "        embeddings = torch.stack(embeddings)\n",
    "        slide_labels = torch.stack(slide_labels)\n",
    "        torch.index_select(slide_labels,0,torch.tensor(slides,device=device))\n",
    "        for slide in slides:\n",
    "            logits = slide_level_classification_layer(embeddings[slide_membership == slide,:])\n",
    "            loss += criterion(logits,slide_labels[slide_membership == slide][0].float().view(-1))\n",
    "        break    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    \n",
    "        embeddings = []\n",
    "        slide_labels = []\n",
    "        print(step,batches,torch.max(slide_membership).detach().cpu().numpy())\n",
    "        slide_membership = []\n",
    "        step+=1\n",
    "        batches = 0\n",
    "        p_update = torch.tensor(0.,device=device)\n",
    "        \n",
    "slide_membership = torch.stack(slide_membership)\n",
    "slides = list(set(torch.squeeze(slide_membership).detach().cpu().numpy()))\n",
    "loss = torch.tensor(0.,device=device)\n",
    "embeddings = torch.stack(embeddings)\n",
    "slide_labels = torch.stack(slide_labels)\n",
    "for slide in slides:\n",
    "    pooled = pool_fn(embeddings[slide_membership == slide,:])\n",
    "    logits = slide_level_classification_layer(pooled)\n",
    "    loss += criterion(logits,slide_labels[slide_membership == slide][0].float().view(-1))\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_em = [torch.masked_select(embeddings,slide_membership == slide).view(-1,2048) for slide in slides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_labels = torch.tensor(train_set.sample_labels, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([324])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([73728])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_membership.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1145, 0.1475, 0.1570,  ..., 0.7910, 1.0237, 0.9910],\n",
       "        [0.6682, 0.7999, 0.7649,  ..., 0.3765, 0.2672, 0.3765],\n",
       "        [1.1181, 1.2128, 0.7971,  ..., 1.4313, 0.9947, 1.1140],\n",
       "        ...,\n",
       "        [1.0344, 0.8466, 0.8967,  ..., 0.4271, 0.6927, 0.4957],\n",
       "        [0.8224, 0.7230, 1.0966,  ..., 0.9042, 1.0291, 0.9912],\n",
       "        [1.1212, 1.5049, 1.3071,  ..., 0.4562, 0.4927, 0.5448]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
