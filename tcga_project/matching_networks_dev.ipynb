{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables\n",
    "classification = 'WGD'\n",
    "magnification = '10.0'\n",
    "output_size = 1\n",
    "device = torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image file paths\n",
    "root_dir = '/n/mounted-data-drive/'\n",
    "batch_one = ['COAD', 'BRCA', 'UCEC']\n",
    "batch_two_orig = ['BLCA', 'KIRC', 'READ', 'HNSC', 'LUSC', 'LIHC', 'LUAD', 'STAD']\n",
    "if magnification == '10.0':\n",
    "    batch_two = [b + '_10x' for b in batch_two_orig]\n",
    "elif magnification == '5.0':\n",
    "    batch_two = [b + '_5x' for b in batch_two_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# get sample annotations\n",
    "# NOTE: ONLY FOR WGD\n",
    "wgd_path = 'ALL_WGD_TABLE.xlsx'\n",
    "wgd_raw = pd.read_excel(wgd_path)\n",
    "#wgd_raw.head(3)\n",
    "\n",
    "batch_all_orig = batch_one + batch_two_orig\n",
    "wgd_filtered = wgd_raw.loc[wgd_raw['Type'].isin(batch_all_orig)]\n",
    "#wgd_filtered.head(3)\n",
    "\n",
    "wgd_filtered.loc[wgd_filtered['Genome_doublings'].values == 2, 'Genome_doublings'] = 1\n",
    "\n",
    "wgd_filtered.set_index('Sample', inplace=True)\n",
    "#wgd_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Samples with Images and Labels:\n",
      "COAD      Num Images:   433  Num Labels:   433  Overlap:   406\n",
      "BRCA      Num Images: 1,054  Num Labels: 1,048  Overlap:   998\n",
      "UCEC      Num Images:   505  Num Labels:   517  Overlap:   477\n",
      "BLCA_10x  Num Images:   387  Num Labels:   402  Overlap:   377\n",
      "KIRC_10x  Num Images:   508  Num Labels:   483  Overlap:   459\n",
      "READ_10x  Num Images:   157  Num Labels:   155  Overlap:   143\n",
      "HNSC_10x  Num Images:   365  Num Labels:   512  Overlap:   351\n",
      "LUSC_10x  Num Images:   479  Num Labels:   482  Overlap:   460\n",
      "LIHC_10x  Num Images:   365  Num Labels:   362  Overlap:   351\n",
      "LUAD_10x  Num Images:   466  Num Labels:   503  Overlap:   448\n",
      "STAD_10x  Num Images:   373  Num Labels:   427  Overlap:   358\n"
     ]
    }
   ],
   "source": [
    "# get sample annotations for all cancer types\n",
    "sa_trains = []\n",
    "sa_vals = []\n",
    "batch_all = batch_one + batch_two\n",
    "\n",
    "print('Num Samples with Images and Labels:')\n",
    "for cancer in batch_all:\n",
    "    sa_train, sa_val = data_utils.process_WGD_data(root_dir='/n/mounted-data-drive/', cancer_type=cancer, \n",
    "                                                   wgd_path=None, wgd_raw = wgd_filtered)\n",
    "    sa_trains.append(sa_train)\n",
    "    sa_vals.append(sa_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sample annotations in a pickle\n",
    "pickle_file = 'tcga_wgd_sa_all.pkl'\n",
    "with open(pickle_file, 'wb') as f: \n",
    "        pickle.dump([batch_all, sa_trains, sa_vals], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample annotations pickle\n",
    "batch_all, sa_trains, sa_vals = data_utils.load_COAD_train_val_sa_pickle(pickle_file=pickle_file, \n",
    "                                                                         return_all_cancers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Datasets\n",
    "train_sets = []\n",
    "val_sets = []\n",
    "\n",
    "train_transform = train_utils.transform_train\n",
    "val_transform = train_utils.transform_validation\n",
    "\n",
    "for i in range(len(batch_all)):\n",
    "    train_set = data_utils.TCGADataset_tiles(sa_trains[i], \n",
    "                                             root_dir + batch_all[i] + '/', \n",
    "                                             transform=train_transform, \n",
    "                                             magnification=magnification, \n",
    "                                             batch_type='tile')\n",
    "    val_set = data_utils.TCGADataset_tiles(sa_vals[i], \n",
    "                                           root_dir + batch_all[i] + '/', \n",
    "                                           transform=val_transform, \n",
    "                                           magnification=magnification, \n",
    "                                           batch_type='tile')\n",
    "    train_sets.append(train_set)\n",
    "    val_sets.append(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Tiles:\n",
      "COAD      Train:    942,176              Val:  247,402\n",
      "          Train: (0) 0.6246, (1) 0.3754  Val: (0) 0.6002 (1) 0.3998\n",
      "BRCA      Train:  1,891,942              Val:  511,924\n",
      "          Train: (0) 0.5509, (1) 0.4491  Val: (0) 0.5445 (1) 0.4555\n",
      "UCEC      Train:  1,570,751              Val:  417,222\n",
      "          Train: (0) 0.8230, (1) 0.1770  Val: (0) 0.7520 (1) 0.2480\n",
      "BLCA_10x  Train:  1,102,415              Val:  289,763\n",
      "          Train: (0) 0.3706, (1) 0.6294  Val: (0) 0.4747 (1) 0.5253\n",
      "KIRC_10x  Train:  1,175,609              Val:  323,656\n",
      "          Train: (0) 0.7961, (1) 0.2039  Val: (0) 0.8145 (1) 0.1855\n",
      "READ_10x  Train:    293,972              Val:   79,397\n",
      "          Train: (0) 0.4715, (1) 0.5285  Val: (0) 0.4454 (1) 0.5546\n",
      "HNSC_10x  Train:    985,504              Val:  219,335\n",
      "          Train: (0) 0.6641, (1) 0.3359  Val: (0) 0.7502 (1) 0.2498\n",
      "LUSC_10x  Train:  1,018,233              Val:  250,477\n",
      "          Train: (0) 0.4718, (1) 0.5282  Val: (0) 0.3744 (1) 0.6256\n",
      "LIHC_10x  Train:    967,194              Val:  237,645\n",
      "          Train: (0) 0.6969, (1) 0.3031  Val: (0) 0.6099 (1) 0.3901\n",
      "LUAD_10x  Train:    944,024              Val:  248,300\n",
      "          Train: (0) 0.3555, (1) 0.6445  Val: (0) 0.3931 (1) 0.6069\n",
      "STAD_10x  Train:    857,105              Val:  215,682\n",
      "          Train: (0) 0.5788, (1) 0.4212  Val: (0) 0.6236 (1) 0.3764\n"
     ]
    }
   ],
   "source": [
    "print('Num Tiles:')\n",
    "for cancer, tset, vset in zip(batch_all, train_sets, val_sets):\n",
    "    print('{0:<8}  Train: {1:>10,d}              Val: {2:>8,d}'.format(cancer, tset.__len__(), vset.__len__()))\n",
    "    print('          Train: (0) {0:0.4f}, (1) {1:0.4f}  Val: (0) {2:0.4f} (1) {3:0.4f}'.format(np.mean(np.array(tset.all_labels) == 0),\n",
    "                                                                                              np.mean(np.array(tset.all_labels) == 1),\n",
    "                                                                                              np.mean(np.array(vset.all_labels) == 0),\n",
    "                                                                                              np.mean(np.array(vset.all_labels) == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model file paths\n",
    "if classification == 'WGD':\n",
    "    if magnification == '10.0':\n",
    "        #sa_file = '/n/tcga_models/resnet18_WGD_10x_sa.pkl'\n",
    "        state_dict_file = '/n/tcga_models/resnet18_WGD_10x.pt'\n",
    "    elif magnification == '5.0':\n",
    "        #sa_file = '/n/tcga_models/resnet18_WGD_v04_sa.pkl'\n",
    "        state_dict_file = '/n/tcga_models/resnet18_WGD_v04.pt'\n",
    "elif classification == 'MSI':\n",
    "    if magnification == '10.0':\n",
    "        #sa_file = '/n/tcga_models/resnet18_MSI_singlelabel_10x_sa.pkl'\n",
    "        state_dict_file = '/n/tcga_models/resnet18_MSI_singlelabel_10x.pt'\n",
    "    elif magnification == '5.0':\n",
    "        #sa_file = '/n/tcga_models/resnet18_MSI_singlelabel_v02_sa.pkl'\n",
    "        state_dict_file = '/n/tcga_models/resnet18_MSI_singlelabel_v02.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding network and freeze layers\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(2048, output_shape, bias=True)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc: storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "resnet.fc = nn.Linear(2048, 2048, bias=False)\n",
    "resnet.fc.weight.data=torch.eye(2048)\n",
    "resnet.cuda(device=device)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# initialize fully-connected final layer \n",
    "final_embed_layer = nn.Linear(2048, 2048)\n",
    "final_embed_layer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define few-shot learning params\n",
    "n_support = 5 # number of training examples in the support set\n",
    "n_query = 20 # number of training examples in the query set\n",
    "n_task = 4 # number of 'tasks' to sample from each cancer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
