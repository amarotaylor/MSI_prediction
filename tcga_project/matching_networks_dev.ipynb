{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables\n",
    "classification = 'WGD'\n",
    "magnification = '10.0'\n",
    "output_shape = 1\n",
    "device = torch.device('cuda', 0)\n",
    "root_dir = '/n/mounted-data-drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Images - DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image file paths\n",
    "batch_one = ['COAD', 'BRCA', 'UCEC']\n",
    "batch_two_orig = ['BLCA', 'KIRC', 'READ', 'HNSC', 'LUSC', 'LIHC', 'LUAD', 'STAD']\n",
    "if magnification == '10.0':\n",
    "    batch_two = [b + '_10x' for b in batch_two_orig]\n",
    "elif magnification == '5.0':\n",
    "    batch_two = [b + '_5x' for b in batch_two_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample annotations\n",
    "# NOTE: ONLY FOR WGD\n",
    "wgd_path = 'ALL_WGD_TABLE.xlsx'\n",
    "wgd_raw = pd.read_excel(wgd_path)\n",
    "#wgd_raw.head(3)\n",
    "\n",
    "batch_all_orig = batch_one + batch_two_orig\n",
    "wgd_filtered = wgd_raw.loc[wgd_raw['Type'].isin(batch_all_orig)]\n",
    "#wgd_filtered.head(3)\n",
    "\n",
    "wgd_filtered.loc[wgd_filtered['Genome_doublings'].values == 2, 'Genome_doublings'] = 1\n",
    "\n",
    "wgd_filtered.set_index('Sample', inplace=True)\n",
    "#wgd_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample annotations for all cancer types\n",
    "# split samples into two sets of train/val\n",
    "sa_trains1 = []\n",
    "sa_vals1 = []\n",
    "sa_trains2 = []\n",
    "sa_vals2 = []\n",
    "batch_all = batch_one + batch_two\n",
    "\n",
    "print('Num Samples with Images and Labels:')\n",
    "for cancer in batch_all:\n",
    "    sa_train1, sa_val1, sa_train2, sa_val2 = data_utils.process_WGD_data(root_dir='/n/mounted-data-drive/', \n",
    "                                                                         cancer_type=cancer, \n",
    "                                                                         wgd_path=None, \n",
    "                                                                         split_in_two=True, \n",
    "                                                                         print_overlap=True, \n",
    "                                                                         wgd_raw=wgd_filtered)\n",
    "    sa_trains1.append(sa_train1)\n",
    "    sa_vals1.append(sa_val1)\n",
    "    sa_trains2.append(sa_train2)\n",
    "    sa_vals2.append(sa_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sample annotations in a pickle\n",
    "pickle_file = '/home/sxchao/MSI_prediction/tcga_project/tcga_wgd_sa_all.pkl'\n",
    "with open(pickle_file, 'wb') as f: \n",
    "    pickle.dump([batch_all, sa_trains1, sa_vals1, sa_trains2, sa_vals2], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample annotations pickle\n",
    "pickle_file = '/home/sxchao/MSI_prediction/tcga_project/tcga_wgd_sa_all.pkl'\n",
    "batch_all, _, _, sa_trains, sa_vals = data_utils.load_COAD_train_val_sa_pickle(pickle_file=pickle_file, \n",
    "                                                                               return_all_cancers=True, \n",
    "                                                                               split_in_two=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Datasets\n",
    "train_sets = []\n",
    "val_sets = []\n",
    "\n",
    "#train_transform = train_utils.transform_train\n",
    "val_transform = train_utils.transform_validation\n",
    "\n",
    "train_cancers = ['COAD', 'BRCA', 'READ_10x', 'LUSC_10x', 'BLCA_10x', 'LUAD_10x', 'STAD_10x', 'HNSC_10x']\n",
    "#val_cancers = ['UCEC', 'LIHC_10x', 'KIRC_10x']\n",
    "\n",
    "#train_cancers = ['READ_10x', 'LUSC_10x']\n",
    "val_cancers = ['COAD', 'BRCA', 'READ_10x', 'LUSC_10x', 'BLCA_10x', 'LUAD_10x', 'STAD_10x', 'HNSC_10x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COAD BRCA READ_10x LUSC_10x BLCA_10x LUAD_10x STAD_10x HNSC_10x "
     ]
    }
   ],
   "source": [
    "for i in range(len(train_cancers)):\n",
    "    print(train_cancers[i], end=' ')\n",
    "    train_set = data_utils.TCGADataset_tiles(sa_vals[batch_all.index(train_cancers[i])], \n",
    "                                             root_dir + train_cancers[i] + '/', \n",
    "                                             transform=val_transform, \n",
    "                                             magnification=magnification, \n",
    "                                             batch_type='tile')\n",
    "    train_sets.append(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COAD BRCA READ_10x LUSC_10x BLCA_10x LUAD_10x STAD_10x HNSC_10x "
     ]
    }
   ],
   "source": [
    "for j in range(len(val_cancers)):\n",
    "    print(val_cancers[j], end=' ')\n",
    "    val_set = data_utils.TCGADataset_tiles(sa_vals[batch_all.index(val_cancers[j])], \n",
    "                                           root_dir + val_cancers[j] + '/', \n",
    "                                           transform=val_transform, \n",
    "                                           magnification=magnification, \n",
    "                                           batch_type='tile')\n",
    "    val_sets.append(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Tiles:\n",
      "COAD      Train:    110,517              Val:  110,517\n",
      "          Train: (0) 0.5520, (1) 0.4480  Val: (0) 0.5520 (1) 0.4480\n",
      "BRCA      Train:    241,890              Val:  241,890\n",
      "          Train: (0) 0.5481, (1) 0.4519  Val: (0) 0.5481 (1) 0.4519\n",
      "UCEC      Train:     34,856              Val:   34,856\n",
      "          Train: (0) 0.6451, (1) 0.3549  Val: (0) 0.6451 (1) 0.3549\n",
      "BLCA_10x  Train:    131,765              Val:  131,765\n",
      "          Train: (0) 0.4012, (1) 0.5988  Val: (0) 0.4012 (1) 0.5988\n",
      "KIRC_10x  Train:    144,962              Val:  144,962\n",
      "          Train: (0) 0.4888, (1) 0.5112  Val: (0) 0.4888 (1) 0.5112\n",
      "READ_10x  Train:    143,084              Val:  143,084\n",
      "          Train: (0) 0.3579, (1) 0.6421  Val: (0) 0.3579 (1) 0.6421\n",
      "HNSC_10x  Train:    101,878              Val:  101,878\n",
      "          Train: (0) 0.7181, (1) 0.2819  Val: (0) 0.7181 (1) 0.2819\n",
      "LUSC_10x  Train:    110,304              Val:  110,304\n",
      "          Train: (0) 0.6168, (1) 0.3832  Val: (0) 0.6168 (1) 0.3832\n"
     ]
    }
   ],
   "source": [
    "print('Num Tiles:')\n",
    "for cancer, tset, vset in zip(batch_all, train_sets, val_sets):\n",
    "    print('{0:<8}  Train: {1:>10,d}              Val: {2:>8,d}'.format(cancer, tset.__len__(), vset.__len__()))\n",
    "    print('          Train: (0) {0:0.4f}, (1) {1:0.4f}  Val: (0) {2:0.4f} (1) {3:0.4f}'.format(np.mean(np.array(tset.all_labels) == 0),\n",
    "                                                                                              np.mean(np.array(tset.all_labels) == 1),\n",
    "                                                                                              np.mean(np.array(vset.all_labels) == 0),\n",
    "                                                                                              np.mean(np.array(vset.all_labels) == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model file paths\n",
    "if classification == 'WGD':\n",
    "    if magnification == '10.0':\n",
    "        #sa_file = '/n/tcga_models/resnet18_WGD_10x_sa.pkl'\n",
    "        #state_dict_file = '/n/tcga_models/resnet18_WGD_10x.pt'\n",
    "        sa_file = '/home/sxchao/MSI_prediction/tcga_project/tcga_wgd_sa_all.pkl'\n",
    "        state_dict_file = '/n/tcga_models/resnet18_WGD_all_10x.pt'\n",
    "    elif magnification == '5.0':\n",
    "        #sa_file = '/n/tcga_models/resnet18_WGD_v04_sa.pkl'\n",
    "        state_dict_file = '/n/tcga_models/resnet18_WGD_v04.pt'\n",
    "elif classification == 'MSI':\n",
    "    if magnification == '10.0':\n",
    "        #sa_file = '/n/tcga_models/resnet18_MSI_singlelabel_10x_sa.pkl'\n",
    "        state_dict_file = '/n/tcga_models/resnet18_MSI_singlelabel_10x.pt'\n",
    "    elif magnification == '5.0':\n",
    "        #sa_file = '/n/tcga_models/resnet18_MSI_singlelabel_v02_sa.pkl'\n",
    "        state_dict_file = '/n/tcga_models/resnet18_MSI_singlelabel_v02.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=2048, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load embedding network\n",
    "\n",
    "# alternative 1\n",
    "#resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# alternative 2\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(2048, output_shape, bias=True)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc: storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "\n",
    "# freeze layers\n",
    "resnet.fc = Identity()\n",
    "resnet.cuda(device=device)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# initialize fully-connected final layer \n",
    "final_embed_layer = nn.Linear(2048, 2048)\n",
    "final_embed_layer.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return torch.stack([d[i][0] for d in self.datasets]), torch.cat([torch.tensor(d[i][1]).view(-1) for d in self.datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "support_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ConcatDataset(*train_sets), \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=24, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "lsm = nn.LogSoftmax(dim=1)\n",
    "criterion = nn.BCELoss()\n",
    "#optimizer = torch.optim.Adam(resnet.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(final_embed_layer.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, verbose=True, min_lr=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (tiles, labels) in enumerate(train_loader):  \n",
    "    labels = labels.cuda().float().transpose(0,1)    \n",
    "    # flatten batch_size x num_cancer_types \n",
    "    batch = tiles.cuda().transpose(0,1).reshape(batch_size * len(train_cancers), 3, 256, 256)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50000, NLL: 11.282670, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 50100, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 50200, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch   202: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 50300, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch   303: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 50400, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 50500, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 50600, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 50700, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 50800, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 50900, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51000, NLL: 11.282670, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51100, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51200, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51300, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51400, NLL: 11.282670, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51500, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51600, NLL: 11.282670, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51700, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51800, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 51900, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52000, NLL: 11.282670, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52100, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52200, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52300, NLL: 11.282670, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52400, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52500, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52600, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52700, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52800, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 52900, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53000, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53100, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53200, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53300, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53400, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53500, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53600, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53700, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53800, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 53900, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 54000, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 54100, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n",
      "Epoch: 54200, NLL: 11.282666, Acc: 0.5917, By Label: 0: 0.6115, 1: 0.5714\n"
     ]
    }
   ],
   "source": [
    "for e in range(50000,100000):\n",
    "    # forward pass\n",
    "    output = resnet(batch)\n",
    "    \n",
    "    # un-flatten num_cancer_types x batch_size\n",
    "    cancers_by_feats = torch.stack(torch.chunk(output, len(train_cancers)))    \n",
    "        \n",
    "    # split feats, labels into support, query sets\n",
    "    feats_support = cancers_by_feats[:, :support_size, :]\n",
    "    feats_support = feats_support.reshape(support_size * len(train_cancers), 2048)\n",
    "    feats_support = final_embed_layer(feats_support)\n",
    "    feats_support = torch.stack(torch.chunk(feats_support, len(train_cancers)))    \n",
    "    feats_query = cancers_by_feats[:, support_size:, :]    \n",
    "    labels_support = labels[:,:support_size]\n",
    "    labels_query = labels[:,support_size:]\n",
    "    \n",
    "    # get preds    \n",
    "    scores = lsm(torch.bmm(feats_support, feats_query.transpose(1,2))).exp()\n",
    "    preds = torch.bmm(labels_support.unsqueeze(1), scores).squeeze(1)\n",
    "    clamped_preds = torch.clamp(preds, 0, 1)\n",
    "    \n",
    "    # calc loss, backprop, step    \n",
    "    loss = criterion(clamped_preds, labels_query)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if e > 50000:\n",
    "        scheduler.step(loss)\n",
    "    \n",
    "    all_preds = (preds.contiguous().view(-1) > 0.5).float().detach().cpu().numpy()\n",
    "    all_labels = labels_query.contiguous().view(-1).float().detach().cpu().numpy()\n",
    "    acc = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "    \n",
    "    d = {'label': all_labels, 'pred': all_preds}\n",
    "    df = pd.DataFrame(data = d)\n",
    "    df['correct_tile'] = df['label'] == df['pred']\n",
    "    tile_acc_by_label = ', '.join([str(i) + ': ' + str(float(df.groupby(['label'])['correct_tile'].mean()[i]))[:6] for i in range(2)])\n",
    "    \n",
    "    #if step % 5 == 0:\n",
    "    #    print('Epoch: {0}, Step: {1}, Train NLL: {2:0.6f}'.format(e, step, loss.detach().cpu().numpy()))\n",
    "    if e % 100 == 0:\n",
    "        print('Epoch: {0}, NLL: {1:0.6f}, Acc: {2:0.4f}, By Label: {3}'.format(e, loss.detach().cpu().numpy(), acc, tile_acc_by_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = (\n",
    "            queries.unsqueeze(1).expand(queries.shape[0], support.shape[0], -1) -\n",
    "            support.unsqueeze(0).expand(queries.shape[0], support.shape[0], -1)\n",
    ").pow(2).sum(dim=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
