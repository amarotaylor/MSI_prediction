{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import model_utils\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "set_image_backend('accimage')\n",
    "device = torch.device('cuda', 0)\n",
    "\n",
    "# load sample annotations pickle\n",
    "pickle_file = '/home/sxchao/MSI_prediction/tcga_project/tcga_wgd_sa_all.pkl'\n",
    "batch_all, _, _, sa_trains, sa_vals = data_utils.load_COAD_train_val_sa_pickle(pickle_file=pickle_file,\n",
    "                                                                               return_all_cancers=True, \n",
    "                                                                               split_in_two=True)\n",
    "# normalize and tensorify jpegs\n",
    "val_transform = train_utils.transform_validation\n",
    "\n",
    "# initialize Datasets\n",
    "val_sets = []\n",
    "val_cancers = ['UCEC', 'LIHC_10x', 'KIRC_10x']\n",
    "magnification = '10.0'\n",
    "root_dir = '/n/mounted-data-drive/'\n",
    "for j in range(len(val_cancers)):\n",
    "    val_set = data_utils.TCGADataset_tiles(sa_vals[batch_all.index(val_cancers[j])], \n",
    "                                           root_dir + val_cancers[j] + '/', \n",
    "                                           transform=val_transform, \n",
    "                                           magnification=magnification, \n",
    "                                           batch_type='tile', \n",
    "                                           return_jpg_to_sample=False)\n",
    "    val_sets.append(val_set)\n",
    "\n",
    "# get DataLoaders    \n",
    "batch_size_val = 400\n",
    "n_workers = 16\n",
    "val_loaders = [torch.utils.data.DataLoader(val_set, \n",
    "                                        batch_size=batch_size_val, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=n_workers, \n",
    "                                        pin_memory=True) for val_set in val_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model args\n",
    "state_dict_file_resnet = '/n/tcga_models/resnet18_WGD_all_10x.pt'\n",
    "state_dict_file_maml = '/n/tcga_models/maml_WGD_10x.pt'\n",
    "input_size = 2048\n",
    "hidden_size = 512\n",
    "output_size = 1\n",
    "\n",
    "# initialize trained resnet\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(2048, output_size, bias=True)\n",
    "saved_state = torch.load(state_dict_file_resnet, map_location=lambda storage, loc: storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "\n",
    "# freeze layers\n",
    "resnet.fc = model_utils.Identity()\n",
    "resnet.cuda(device=device)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# initialize theta_global\n",
    "    net = model_utils.FeedForward(input_size, hidden_size, output_size)\n",
    "    saved_state = torch.load(state_dict_file_maml, map_location=lambda storage, loc: storage)\n",
    "    net.load_state_dict(saved_state)\n",
    "    net.cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_global = [p.detach().clone() for p in net.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()\n",
    "num_steps = 10\n",
    "#num_tasks = int(tiles.shape[1])\n",
    "#for t in range(num_tasks):\n",
    "all_losses = np.zeros((num_steps, len(val_loaders[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-2\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.40835369 2.39327192 1.88396323 1.16007531 2.9691081  1.44426143\n",
      " 2.31927657 1.47561824 1.64480376 2.49590039]\n",
      "1 [2.91604567 2.35293484 1.48522079 1.24402201 1.47609496 1.37248039\n",
      " 1.42844009 1.47775078 1.53149951 1.61077023]\n",
      "2 [3.60560536 2.67700076 2.13805628 1.88240087 1.64428353 1.90511072\n",
      " 1.53838599 1.32009089 1.27488601 1.29028821]\n",
      "3 [3.43054748 2.74031782 2.99221635 2.16195297 2.35563302 2.15735507\n",
      " 2.13523865 2.30686164 2.52127194 2.96156406]\n",
      "4 [3.34314322 2.60716343 2.19643068 1.7887404  1.59041154 1.52957809\n",
      " 1.43499696 1.43333352 1.56361604 1.75463784]\n",
      "5 [2.84291625 1.94059026 1.56476462 1.87794495 1.52578425 1.29282808\n",
      " 1.31949794 1.44589794 1.61176729 1.51044679]\n",
      "6 [3.37569094 2.36453891 2.05386066 2.25720811 2.48291969 2.21124959\n",
      " 2.17643833 2.50970149 3.1274159  3.1077466 ]\n",
      "7 [3.92434812 3.7060132  2.56321526 1.97081971 1.68264306 1.80115235\n",
      " 1.54789543 1.66056061 1.59020591 1.5105679 ]\n",
      "8 [4.02207708 2.50514436 2.18152356 1.85807455 2.22588515 1.75629973\n",
      " 1.68750608 1.74521124 2.30862474 2.00028396]\n",
      "9 [4.51316929 4.48668671 2.8163867  2.5736227  2.29956388 2.70463991\n",
      " 2.4567523  2.74718261 2.97676516 3.01774955]\n",
      "10 [3.42705631 2.51173544 2.60660338 2.098454   2.02225852 1.93612361\n",
      " 2.15494633 1.95334744 2.05576587 2.13439417]\n",
      "11 [4.55201769 3.53814745 3.39628172 2.75636339 2.77347898 2.35576963\n",
      " 2.78026295 2.62207079 3.38953185 3.66961145]\n",
      "12 [3.58100152 2.66917109 2.68803883 2.7087276  2.88748956 2.87144852\n",
      " 3.01955628 3.03824997 3.10252976 3.07554936]\n",
      "13 [2.69889021 1.99166763 2.13629889 2.02376795 1.82776272 1.6946032\n",
      " 1.8271209  1.67134976 1.8428669  1.86869574]\n",
      "14 [4.34529877 3.40056133 3.11479545 2.77709842 2.72776246 2.32640624\n",
      " 2.15925956 2.18084598 2.62766957 3.14888692]\n",
      "15 [3.73572326 2.53185797 2.40429235 2.55826426 2.4885118  1.85531616\n",
      " 1.90458488 1.82737517 2.45476913 2.5113101 ]\n",
      "16 [3.33931422 2.34951496 2.01120329 3.08184004 1.71839309 1.73768079\n",
      " 1.86879349 1.96376586 2.17369032 2.43826628]\n",
      "17 [3.85639119 3.03145337 2.67645931 2.17319107 2.01958847 2.15444636\n",
      " 2.09896183 2.21075845 2.26439619 2.74198914]\n",
      "18 [3.39220142 2.77690053 2.80045199 1.98149109 1.8015846  2.53547573\n",
      " 1.9047451  1.82947242 1.936777   2.04862905]\n",
      "19 [4.06763554 2.60182858 1.99961913 1.87799406 1.55936241 1.54537559\n",
      " 1.64390373 1.95953858 1.87695706 1.76236689]\n",
      "20 [3.75968313 3.57516646 3.47527146 2.54865956 2.45213056 2.66456866\n",
      " 3.34568834 2.99863148 2.76110029 2.81626368]\n",
      "21 [3.12778282 2.40090466 1.95014405 1.66767573 1.69589734 1.63454103\n",
      " 1.71388876 1.92445385 2.2784586  2.63532782]\n",
      "22 [2.92531967 1.94341612 1.85649979 1.42676783 1.52009177 1.44784379\n",
      " 1.691571   1.98818016 2.56070399 2.35523558]\n",
      "23 [3.73258471 2.90205646 2.53205371 2.28948092 2.28321433 2.21973944\n",
      " 2.12620878 2.10199547 2.0604465  2.11548281]\n",
      "24 [4.26180172 3.49206471 2.92592978 2.73334837 2.58039904 2.57832336\n",
      " 2.55230188 2.61476302 2.62228847 2.63373041]\n",
      "25 [3.22442675 2.38702106 2.22558975 1.73896241 2.09430861 2.05072951\n",
      " 2.01659775 2.0618062  2.14754701 2.25137806]\n",
      "26 [3.15134931 2.67426515 3.16505456 2.19704533 2.28669238 2.51961088\n",
      " 3.11885858 2.61338615 2.76863337 2.71715045]\n",
      "27 [3.36562681 2.67156243 2.3240521  2.12645531 2.13856101 2.2460742\n",
      " 2.34675193 2.45858455 2.5835278  2.69781828]\n",
      "28 [3.57642961 2.66805601 2.35173678 2.05050635 2.47173166 2.42072964\n",
      " 2.48927021 2.70583177 2.72229767 2.68410659]\n",
      "29 [4.24270821 3.00446439 1.97015357 1.64084053 1.8532809  1.86260509\n",
      " 1.79154336 2.00298691 2.24510622 2.45588994]\n",
      "30 [3.16495109 2.34311485 1.89001882 1.84824562 1.99577236 2.12638426\n",
      " 2.29092765 2.48929787 2.62981796 2.66038942]\n",
      "31 [3.52400994 2.51002121 1.87849975 1.67317557 1.72221589 1.49416697\n",
      " 1.69671655 2.00106215 2.25474644 2.35985494]\n",
      "32 [4.78474617 3.60004807 3.36792016 3.36162996 3.42915225 2.9810884\n",
      " 2.82134151 3.06597161 3.22245049 2.97845697]\n",
      "33 [4.72300529 3.56785512 2.99752402 2.29824591 1.96273184 2.2066133\n",
      " 1.58711863 2.01688027 1.46911812 1.79116464]\n",
      "34 [3.66750884 2.88729358 3.09934115 2.44821119 2.50030017 2.45665216\n",
      " 2.86643863 2.36841083 2.58580565 2.65316105]\n",
      "35 [2.63809776 1.94880426 1.80611598 1.47628367 1.31370437 1.29097474\n",
      " 1.26128829 1.45828879 1.49118912 1.71727622]\n",
      "36 [3.77741718 3.5944128  3.4547863  2.11056995 2.06740952 2.4190352\n",
      " 2.70447993 2.0578692  1.93418312 2.09209323]\n",
      "37 [4.30820274 3.06883907 2.35769892 2.03982139 2.02764869 2.03001428\n",
      " 2.10404253 2.14971757 2.11814332 2.12607908]\n",
      "38 [3.05585694 2.44520402 2.40574598 2.14096212 2.08462214 1.90700316\n",
      " 2.1139884  2.32841206 2.71495414 2.47209764]\n",
      "39 [3.65854716 2.62290716 2.21512246 1.97135162 1.72270322 1.53377855\n",
      " 1.56267416 1.71072185 1.69277644 1.65938437]\n",
      "40 [3.55165267 2.82565808 3.16537738 2.00307465 1.99623251 2.20084476\n",
      " 2.7782836  2.18940568 2.13427448 2.15024805]\n",
      "41 [4.23653173 2.97112846 2.55032468 1.85057282 1.78744507 1.7800386\n",
      " 1.71091044 1.79048717 1.83411086 2.06164885]\n",
      "42 [2.85195804 1.97752225 1.90709722 1.51514494 1.30044985 1.39685774\n",
      " 1.62746263 1.2750386  1.2524271  1.36630201]\n",
      "43 [4.01149416 3.40240598 2.74962878 2.54719138 2.07357788 1.95129752\n",
      " 1.95786798 1.98558426 1.95111823 2.00999236]\n",
      "44 [4.13660383 3.00594044 2.71172261 2.39264274 2.19959593 2.04999328\n",
      " 1.94287252 2.04314399 2.04105425 2.08380175]\n",
      "45 [2.67640805 2.11957431 1.84522867 2.11698699 1.91872275 2.15070653\n",
      " 2.08087373 2.22447944 2.41701174 2.64010859]\n",
      "46 [4.32321501 3.32809734 2.56162953 2.38510489 2.60129571 2.35612941\n",
      " 2.18882251 2.20590329 2.42222691 2.5047133 ]\n",
      "47 [3.35078359 2.68434191 3.8322804  2.37095046 2.29468131 2.18652081\n",
      " 2.95362306 2.63990593 2.48375964 2.63600516]\n",
      "48 [3.23578739 2.78606439 2.34648514 2.10639834 2.08192325 1.7686758\n",
      " 1.34968507 1.31065249 1.28057289 1.40590572]\n",
      "49 [3.11111045 2.41501856 2.02600098 2.02925658 2.11547136 2.32133555\n",
      " 2.35826254 2.4654727  2.6136775  2.75548029]\n",
      "50 [2.76236129 2.19768357 2.17103767 2.12349868 2.37421417 2.39727521\n",
      " 2.42503309 2.4921813  2.57690978 2.64578795]\n",
      "51 [3.2566359  2.36958528 2.284693   1.7483393  1.62957168 1.52641618\n",
      " 1.56448615 1.54654849 1.69256735 1.81467187]\n",
      "52 [3.37967896 2.96605468 2.89143872 1.2013545  1.07123458 1.57312846\n",
      " 1.35950696 1.62444115 1.60286605 1.79475951]\n",
      "53 [4.23433018 3.08417463 2.54716969 2.56507897 2.43795872 2.43533897\n",
      " 2.19345617 2.4782548  2.16860294 1.89470732]\n",
      "54 [3.50245786 2.97707391 2.28495932 1.86666787 1.89713955 2.14457464\n",
      " 1.83913755 1.76460218 1.73323739 1.78418136]\n",
      "55 [3.97177768 3.15568447 2.73726916 1.98857021 1.93884003 2.26311302\n",
      " 2.47511244 2.70058966 2.58210588 2.44117188]\n",
      "56 [3.90454578 2.8443377  2.73006892 3.34920645 3.09116101 2.50529695\n",
      " 2.45188689 3.11965299 3.02202916 2.53316069]\n",
      "57 [3.06258607 2.27611566 2.22842383 2.06781268 1.9382242  1.90563476\n",
      " 2.1175952  1.85868406 2.17656159 1.98800933]\n",
      "58 [2.61808538 2.50963616 2.85306096 1.69989407 1.76356316 2.22479558\n",
      " 2.06961727 2.12773228 2.23643684 2.39136076]\n",
      "59 [3.37797594 2.62080932 2.35108972 1.79232144 1.71750379 1.61447906\n",
      " 1.54015684 1.52324677 1.59142089 1.65465176]\n",
      "60 [4.22615767 2.63914323 2.25255156 2.47439075 2.13613343 1.44432616\n",
      " 2.10751009 2.90097404 3.22629786 2.91740346]\n",
      "61 [2.57097578 1.87201262 1.9923979  1.35877669 1.52301967 1.29701042\n",
      " 1.35432279 1.45771146 1.67553842 1.84694362]\n",
      "62 [4.49886274 3.38438416 3.40095544 2.2593224  2.17676973 2.11561036\n",
      " 2.25784659 2.34705114 2.54037905 2.62249804]\n",
      "63 [2.83607388 2.32101464 3.0096755  1.75882411 2.01903415 1.75033844\n",
      " 2.03274965 2.35113025 1.97346246 2.11544251]\n",
      "64 [3.06681323 2.31243205 2.30023408 1.92103028 2.2904489  2.81955266\n",
      " 2.46280813 2.19712305 2.13567233 2.13260245]\n",
      "65 [3.35712099 3.16296005 2.76412201 2.5493896  2.47879672 2.38019729\n",
      " 2.30234647 2.36262989 2.26135135 2.27841926]\n",
      "66 [3.3724339  2.73621511 2.99986601 2.44924259 2.34645939 2.86985397\n",
      " 2.79801679 3.03466749 3.39547205 3.42599773]\n",
      "67 [4.12955189 3.46074152 2.74576187 1.99566376 1.93651366 1.79285216\n",
      " 1.76031625 1.66712093 1.73514795 1.84134567]\n",
      "68 [3.37214231 2.5371623  2.28946328 2.20994353 2.15866995 2.07524729\n",
      " 2.188936   2.29121375 2.38954115 2.34960771]\n",
      "69 [3.33146548 2.79877353 2.98711061 1.98757565 2.06781316 2.02013302\n",
      " 2.57411194 2.28742361 2.00432253 1.90756392]\n",
      "70 [3.49800897 2.57980466 2.22377181 1.8928299  1.77414429 1.45941496\n",
      " 1.53619707 1.4293654  1.58949745 1.5120362 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 [3.09142399 2.17179132 1.8532275  1.45565712 1.48311865 1.31265819\n",
      " 1.28105462 1.27485251 1.33091807 1.38645113]\n",
      "72 [2.83916974 1.93725979 2.33901358 1.74084878 1.99729228 1.35335124\n",
      " 1.92469811 1.63140774 1.9698602  2.26310349]\n",
      "73 [3.68423843 3.22030544 2.15728879 2.17349648 2.44706416 2.82055259\n",
      " 2.21734619 1.9856838  1.99940765 2.33087611]\n",
      "74 [4.00544071 3.31622481 3.56498098 2.44904518 2.4576354  2.56261969\n",
      " 2.55616808 2.35766864 2.70202756 2.38553953]\n",
      "75 [4.51596498 3.85257125 3.1304419  2.68331861 2.60273862 2.52270055\n",
      " 2.49925137 2.41657186 2.47303438 2.58869839]\n",
      "76 [4.0538826  3.19204712 2.67170811 2.07694983 2.05416059 1.62882507\n",
      " 1.64636064 1.7064023  1.76038444 1.80221903]\n",
      "77 [3.36139393 2.24078679 2.54530573 1.80162823 1.83813691 1.74567354\n",
      " 2.08612919 1.97456634 2.48521614 2.3241539 ]\n",
      "78 [3.45928097 3.48326159 3.4006896  2.56680965 2.24646664 2.59157801\n",
      " 2.32789803 2.09280872 2.15281439 2.20636582]\n",
      "79 [3.90829301 3.18331289 3.04693151 2.67625427 3.23664451 2.25808716\n",
      " 1.94212031 1.90572906 2.04460406 2.56953192]\n",
      "80 [3.79603267 2.71336532 2.31524038 2.12313223 2.34627461 2.33932424\n",
      " 2.4945066  2.57301641 2.81732607 3.00307798]\n",
      "81 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-90827cc46c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moutput_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mall_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#net.linear1.weight.data = theta_global[0].clone()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_num,(tiles,labels) in enumerate(val_loaders[0]):\n",
    "    print(batch_num, end=' ')\n",
    "    \n",
    "    tiles, labels = tiles.cuda(), labels.cuda().float()\n",
    "    idx = int(tiles.shape[0] / 2)\n",
    "    inputs_a = tiles[:idx,:,:,:]\n",
    "    inputs_b = tiles[idx:,:,:,:]\n",
    "    labels_a = labels[:idx].unsqueeze(1)\n",
    "    labels_b = labels[idx:].unsqueeze(1)    \n",
    "    \n",
    "    for step in range(num_steps):        \n",
    "        optimizer.zero_grad()\n",
    "        if step == 0:\n",
    "            net.eval()\n",
    "            output_b = net(resnet(inputs_b))\n",
    "            loss = criterion(output_b, labels_b)\n",
    "            all_losses[step, batch_num] = loss.detach().cpu().item()\n",
    "        else:\n",
    "            # first forward pass, step \n",
    "            net.train()\n",
    "            output_a = net(resnet(inputs_a))\n",
    "            loss = criterion(output_a, labels_a)\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "            # second forward pass   \n",
    "            net.eval()\n",
    "            output_b = net(resnet(inputs_b))\n",
    "            loss = criterion(output_b, labels_b)\n",
    "            all_losses[step, batch_num] = loss.detach().cpu().item()\n",
    "    print(all_losses[:,batch_num])\n",
    "    #net.linear1.weight.data = theta_global[0].clone()\n",
    "    #net.linear1.bias.data = theta_global[1].clone()\n",
    "    #net.linear2.weight.data = theta_global[2].clone()\n",
    "    #net.linear2.bias.data = theta_global[3].clone()\n",
    "    net.update_params(theta_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
