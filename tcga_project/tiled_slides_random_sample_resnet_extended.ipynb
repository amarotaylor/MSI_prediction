{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import accimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms, set_image_backend, get_image_backend\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import train_utils\n",
    "import data_utils\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "set_image_backend('accimage')\n",
    "from data_utils import *\n",
    "import train_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "state_dict_file = '/n/tcga_models/resnet18_WGD_all_10x.pt'\n",
    "input_size = 2048\n",
    "hidden_size = 512\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, gated=True):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.gated = gated\n",
    "        self.V = nn.Linear(input_size, hidden_size)\n",
    "        self.U = nn.Linear(input_size, hidden_size)\n",
    "        self.w = nn.Linear(hidden_size, output_size)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "        self.linear_layer = nn.Linear(input_size,1)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        if self.gated == True:\n",
    "            a = self.sm(self.w(self.tanh(self.V(h)) * self.sigm(self.U(h))))\n",
    "        else:\n",
    "            a = self.sm(self.w(self.tanh(self.V(h))))\n",
    "        z = torch.sum(a*h,dim=0)\n",
    "        logits = self.linear_layer(z)\n",
    "        return logits,a\n",
    "\n",
    "\n",
    "# initialize trained resnet\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(2048, output_size, bias=True)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc: storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "device = torch.device('cuda',1)\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "attend_and_pool = Attention(input_size, hidden_size, output_size)\n",
    "resnet.fc = attend_and_pool\n",
    "resnet.cuda(device=device)\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(resnet.fc.parameters(), lr = 1e-5)\n",
    "train_cancers = ['READ_10x']\n",
    "val_cancers = ['READ_10x']\n",
    "\n",
    "root_dir = '/n/mounted-data-drive/'\n",
    "magnification = '10.0'\n",
    "criterion=nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCGA_random_tiles_sampler(Dataset):\n",
    "    \"\"\"TCGA dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_annotations, root_dir, transform=None, loader=default_loader, \n",
    "                 magnification='5.0', tile_batch_size = 256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sample_annot (dict): dictionary of sample names and their respective labels.\n",
    "            root_dir (string): directory containing all of the samples and their respective images.\n",
    "            transform (callable, optional): optional transform to be applied on the images of a sample.\n",
    "        \"\"\"\n",
    "        self.sample_names = list(sample_annotations.keys())\n",
    "        self.sample_labels = list(sample_annotations.values())\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "        self.magnification = magnification\n",
    "        self.img_dirs = [self.root_dir + sample_name + '.svs/' \\\n",
    "                         + sample_name + '_files/' + self.magnification for sample_name in self.sample_names]\n",
    "        self.jpegs = [os.listdir(img_dir) for img_dir in self.img_dirs]\n",
    "        self.all_jpegs = []\n",
    "        self.all_labels = []\n",
    "        self.jpg_to_sample = []\n",
    "        self.coords = []\n",
    "        self.tile_batch_size = tile_batch_size\n",
    "        for idx,(im_dir,label,l) in enumerate(zip(self.img_dirs,self.sample_labels,self.jpegs)):\n",
    "            sample_coords = []\n",
    "            for jpeg in l:\n",
    "                self.all_jpegs.append(im_dir+'/'+jpeg)\n",
    "                self.all_labels.append(label)\n",
    "                self.jpg_to_sample.append(idx)\n",
    "                x,y = jpeg[:-5].split('_') # 'X_Y.jpeg'\n",
    "                x,y = int(x), int(y)\n",
    "                sample_coords.append(torch.tensor([x,y]))\n",
    "            self.coords.append(torch.stack(sample_coords))\n",
    "                \n",
    "            \n",
    "    def __len__(self):\n",
    "        ''' number of slides: jpegs is a list of lists '''\n",
    "        return len(self.jpegs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_tiles = []\n",
    "        tiles_batch = []\n",
    "        perm = torch.randperm(len(self.jpegs[idx]))\n",
    "        \n",
    "        if len(self.jpegs[idx]) > self.tile_batch_size:\n",
    "            idxs = perm[:self.tile_batch_size]\n",
    "        else: \n",
    "            idxs = range(len(self.jpegs[idx]))\n",
    "            \n",
    "        for tile_num in idxs:\n",
    "            im = self.jpegs[idx][tile_num]\n",
    "            path = self.img_dirs[idx] + '/' + im\n",
    "            image = self.loader(path)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "            if image.shape[1] < 256 or image.shape[2] < 256:\n",
    "                image = pad_tensor_up_to(image,256,256,channels_last=False)\n",
    "            tiles_batch.append(image)\n",
    "\n",
    "        # create batch of random tiles\n",
    "        slide = torch.stack(tiles_batch)\n",
    "\n",
    "        label = self.sample_labels[idx]\n",
    "        coords = torch.stack([self.coords[idx][i] for i in idxs])\n",
    "        return slide, label, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_train, sa_val = data_utils.load_COAD_train_val_sa_pickle('/n/tcga_models/resnet18_WGD_v04_sa.pkl')\n",
    "pickle_file = '/home/sxchao/MSI_prediction/tcga_project/tcga_wgd_sa_all.pkl'\n",
    "batch_all, sa_train1, sa_val1, sa_train2, sa_val2 = data_utils.load_COAD_train_val_sa_pickle(pickle_file=pickle_file,\n",
    "                                                                               return_all_cancers=True, \n",
    "                                                                               split_in_two=True)\n",
    "\n",
    "sa_trains = [dict(sa_train1[idx], **sa_train2[idx]) for idx,_ in enumerate(sa_train1)]\n",
    "sa_vals = [dict(sa_val1[idx], **sa_val2[idx]) for idx,_ in enumerate(sa_val1)]\n",
    "\n",
    "\n",
    "train_transform = train_utils.transform_train\n",
    "val_transform = train_utils.transform_validation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "magnification = '10.0'\n",
    "root_dir = '/n/mounted-data-drive/'\n",
    "\n",
    "train_sets = []\n",
    "val_sets = []\n",
    "\n",
    "for i in range(len(train_cancers)):\n",
    "    train_set = data_utils.TCGA_random_tiles_sampler(sa_trains[batch_all.index(train_cancers[i])], \n",
    "                                             root_dir + train_cancers[i] + '/', \n",
    "                                             transform=train_transform, \n",
    "                                             magnification=magnification,tile_batch_size=1024)\n",
    "    train_sets.append(train_set)    \n",
    "\n",
    "for j in range(len(val_cancers)):\n",
    "    val_set = data_utils.TCGA_random_tiles_sampler(sa_vals[batch_all.index(val_cancers[j])], \n",
    "                                           root_dir + val_cancers[j] + '/', \n",
    "                                           transform=val_transform, \n",
    "                                           magnification=magnification,tile_batch_size=1024)\n",
    "    val_sets.append(val_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=1,shuffle=True,num_workers=16, \n",
    "                                            pin_memory=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set,batch_size=1,shuffle=True,num_workers=16, \n",
    "                                            pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_random_sampling(e,train_loader,device,criterion,resnet,optimizer):\n",
    "    for step,(slide, label, coords) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        slide,label = slide.squeeze(0).cuda(device),label.cuda(device)\n",
    "        logits,_ = resnet(slide)\n",
    "        loss = criterion(logits,label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step%30 == 0:\n",
    "            print('Epoch: {0}, Step: {1}, Train NLL: {2:0.4f}'.format(e, step, loss.detach().cpu().numpy()))\n",
    "    del slide, label, loss, logits, _\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop_for_random_sampler(e,val_loader,device,criterion,resnet):\n",
    "    pred_batch = []\n",
    "    true_label = []\n",
    "    torch.cuda.empty_cache()\n",
    "    loss = torch.tensor(0.0,device=device)\n",
    "    with torch.no_grad():\n",
    "        for step,(slide, label, coords) in enumerate(val_loader):\n",
    "            slide,label = slide.squeeze(0).cuda(device),label.cuda(device)\n",
    "            logits,_ = resnet(slide)\n",
    "            loss += criterion(logits,label.float())\n",
    "            pred_batch.append(torch.sigmoid(logits).detach().cpu().numpy()>0.5)\n",
    "            true_label.append(label.detach().cpu().numpy())\n",
    "            \n",
    "            del slide,label,logits,_\n",
    "            torch.cuda.empty_cache()\n",
    "    #scheduler.step(loss)\n",
    "    pred_batch = np.array(pred_batch)\n",
    "    true_label = np.array(true_label)\n",
    "    acc = np.mean(pred_batch==true_label)\n",
    "    acc_1 = np.mean(pred_batch[true_label==1])\n",
    "    acc_0 = np.mean(1-pred_batch[true_label==0])\n",
    "    loss = loss.detach().cpu().numpy()\n",
    "    \n",
    "    print('Epoch: {0}, Val Total NLL: {1:0.4f}, Val Accuracy: {2:0.2f} \\\n",
    "           Class Accuracy: WGD = {3:0.2f}, Diploid = {4:0.2f}'\\\n",
    "              .format(e,loss,acc,acc_1,acc_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Train NLL: 0.5961\n",
      "Epoch: 0, Step: 30, Train NLL: 0.6327\n",
      "Epoch: 0, Step: 60, Train NLL: 0.6081\n",
      "Epoch: 0, Step: 90, Train NLL: 0.6123\n",
      "Epoch: 0, Val Total NLL: 21.1557, Val Accuracy: 0.34            Class Accuracy: WGD = 1.00, Diploid = 0.00\n",
      "Epoch: 1, Step: 0, Train NLL: 0.7902\n",
      "Epoch: 1, Step: 30, Train NLL: 0.8381\n",
      "Epoch: 1, Step: 60, Train NLL: 0.6370\n",
      "Epoch: 1, Step: 90, Train NLL: 0.6121\n",
      "Epoch: 1, Val Total NLL: 21.2448, Val Accuracy: 0.34            Class Accuracy: WGD = 1.00, Diploid = 0.00\n",
      "Epoch: 2, Step: 0, Train NLL: 0.7884\n"
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    training_loop_random_sampling(e,train_loader,device,criterion,resnet,optim)\n",
    "    validation_loop_for_random_sampler(e,val_loader,device,criterion,resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
