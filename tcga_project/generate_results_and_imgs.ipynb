{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, set_image_backend\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import model_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = '/n/data_labeled_histopathology_images/COAD/train.pkl'\n",
    "with open(pickle_file, 'rb') as f: \n",
    "    train_embeddings, train_labels, train_jpgs_to_slide = pickle.load(f)\n",
    "    \n",
    "pickle_file = '/n/data_labeled_histopathology_images/COAD/val.pkl'\n",
    "with open(pickle_file, 'rb') as f: \n",
    "    val_embeddings, val_labels, val_jpgs_to_slide = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2048\n",
    "hidden_size = 2048\n",
    "output_size = 1\n",
    "criterion = nn.BCELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_file = '/n/tcga_models/resnet18_WGD_10x.pt'\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(input_size, output_size)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc:storage)\n",
    "resnet.load_state_dict(saved_state)\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1, bias=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_fc = resnet.fc\n",
    "n_samples = val_jpgs_to_slide.max() + 1\n",
    "probs_vec = torch.zeros((n_samples, 1)).cuda()\n",
    "labels_vec = torch.zeros_like(probs_vec).cuda()\n",
    "val_embeddings = val_embeddings.cuda()\n",
    "resnet_fc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_fc.eval()\n",
    "for idx in range(n_samples):\n",
    "    with torch.no_grad():\n",
    "        slide = val_embeddings[val_jpgs_to_slide==idx]\n",
    "        labels_vec[idx] = val_labels[val_jpgs_to_slide==idx].unique().float().cuda()\n",
    "        probs_vec[idx] = torch.mean((torch.sigmoid(resnet_fc(slide)) > 0.5).float()) # mean pooling the tile-level predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6194, device='cuda:0') tensor(0.4929, device='cuda:0') 0.7240338164251208\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(probs_vec, labels_vec) # input, target\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels_vec.detach().cpu().numpy(), probs_vec.detach().cpu().numpy()) # y_true, y_score\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(torch.mean(loss), torch.median(loss), roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (V): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (w): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  (sigm): Sigmoid()\n",
       "  (tanh): Tanh()\n",
       "  (sm): Softmax()\n",
       "  (linear_layer): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_file = '/n/tcga_models/COAD_attention_model_reworked_5_8_acc.pt'\n",
    "net = model_utils.Attention(input_size, hidden_size, output_size)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc:storage)\n",
    "net.load_state_dict(saved_state)\n",
    "for p in net.parameters():\n",
    "    p.requires_grad = False\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in range(n_samples):\n",
    "        slide = val_embeddings[val_jpgs_to_slide==idx]\n",
    "        labels_vec[idx] = val_labels[val_jpgs_to_slide==idx].unique().float().cuda()\n",
    "        probs_vec[idx] = torch.mean((torch.sigmoid(net(slide)) > 0.5).float()) # mean pooling the tile-level predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.0762, device='cuda:0') tensor(-0., device='cuda:0') 0.7445652173913043\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(probs_vec, labels_vec) # input, target\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels_vec.detach().cpu().numpy(), probs_vec.detach().cpu().numpy()) # y_true, y_score\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(torch.mean(loss), torch.median(loss), roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (d): Dropout(p=0.5)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (linear2): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_file = '/n/tcga_models/COAD_rationale_model_gen_5_9_acc.pt'\n",
    "output_size = 2\n",
    "gen = model_utils.Generator(input_size, hidden_size, output_size, dropout=0.5)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc:storage)\n",
    "gen.load_state_dict(saved_state)\n",
    "for p in gen.parameters():\n",
    "    p.requires_grad = False\n",
    "gen.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (d): Dropout(p=0.5)\n",
       "  (m): ReLU()\n",
       "  (linear1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_file = '/n/tcga_models/COAD_rationale_model_enc_5_9_acc.pt'\n",
    "output_size = 1\n",
    "enc = model_utils.Encoder(input_size, hidden_size, output_size, pool_fn, dropout=0.5)\n",
    "saved_state = torch.load(state_dict_file, map_location=lambda storage, loc:storage)\n",
    "enc.load_state_dict(saved_state)\n",
    "for p in enc.parameters():\n",
    "    p.requires_grad = False\n",
    "enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.eval()\n",
    "enc.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in range(n_samples):\n",
    "        slide = val_embeddings[val_jpgs_to_slide==idx]\n",
    "        labels_vec[idx] = val_labels[val_jpgs_to_slide==idx].unique().float().cuda()\n",
    "\n",
    "        preds = gen(slide)\n",
    "        sample = torch.argmax(preds, dim=1).float()\n",
    "        rationale = slide * sample.unsqueeze(1)\n",
    "\n",
    "        logits = enc(rationale)\n",
    "        probs_vec[idx] = torch.mean((torch.sigmoid(logits) > 0.5).float()) # mean pooling the tile-level predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.7502, device='cuda:0') tensor(-0., device='cuda:0') 0.7288647342995169\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(probs_vec, labels_vec) # input, target\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels_vec.detach().cpu().numpy(), probs_vec.detach().cpu().numpy()) # y_true, y_score\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(torch.mean(loss), torch.median(loss), roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
