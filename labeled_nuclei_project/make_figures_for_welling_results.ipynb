{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import models\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_auc(valid_loader, net, criterion, pool_fn):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        for slide,label in valid_loader:\n",
    "            slide.squeeze_()\n",
    "            slide, label = slide.cuda(), label.cuda()\n",
    "            output = net(slide)\n",
    "            pool = pool_fn(output).unsqueeze(0)\n",
    "            output = net.classification_layer(pool)\n",
    "            loss = criterion(output, label)\n",
    "            logits.append(output[0][1].detach().cpu().numpy())\n",
    "            total_loss += loss.detach().cpu().numpy()\n",
    "            labels.extend(label.float().cpu().numpy())\n",
    "            preds.append(torch.argmax(output).float().detach().cpu().numpy())\n",
    "    \n",
    "        acc = np.mean(np.array(labels) == np.array(preds))\n",
    "    logits = np.stack(logits)\n",
    "    labels = np.array(labels)\n",
    "    auc = roc_auc_score(labels, logits)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.COAD_dataset(data_utils.COAD_TRAIN)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True , pin_memory=True)\n",
    "\n",
    "valid = data_utils.COAD_dataset(data_utils.COAD_VALID)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=1, shuffle=True , pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean pooling benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conv_layers = 2\n",
    "n_fc_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = [512,512]\n",
    "dropout=0.5\n",
    "net = models.ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "weight_decay = 0.0005\n",
    "def pool_fn(x):\n",
    "    v = torch.mean(x,0)\n",
    "    return v\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train NLL: 34.3639\n",
      "Epoch: 0, Val NLL: 34.9153, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 1, Train NLL: 33.9725\n",
      "Epoch: 1, Val NLL: 35.0570, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 2, Train NLL: 34.0094\n",
      "Epoch: 2, Val NLL: 34.9143, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 3, Train NLL: 33.8674\n",
      "Epoch: 3, Val NLL: 35.0414, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 4, Train NLL: 33.8154\n",
      "Epoch: 4, Val NLL: 34.8646, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 5, Train NLL: 33.9091\n",
      "Epoch: 5, Val NLL: 34.8800, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 6, Train NLL: 33.7869\n",
      "Epoch: 6, Val NLL: 34.9257, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 7, Train NLL: 34.1032\n",
      "Epoch: 7, Val NLL: 34.8426, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 8, Train NLL: 33.7169\n",
      "Epoch: 8, Val NLL: 35.3528, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 9, Train NLL: 33.6053\n",
      "Epoch: 9, Val NLL: 34.8635, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 10, Train NLL: 33.9253\n",
      "Epoch: 10, Val NLL: 34.9161, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 11, Train NLL: 33.4934\n",
      "Epoch: 11, Val NLL: 34.5739, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 12, Train NLL: 33.7216\n",
      "Epoch: 12, Val NLL: 34.9832, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 13, Train NLL: 33.5717\n",
      "Epoch: 13, Val NLL: 34.4517, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 14, Train NLL: 33.3281\n",
      "Epoch: 14, Val NLL: 34.5947, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 15, Train NLL: 32.7126\n",
      "Epoch: 15, Val NLL: 33.3927, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 16, Train NLL: 32.2279\n",
      "Epoch: 16, Val NLL: 36.7698, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 17, Train NLL: 31.6490\n",
      "Epoch: 17, Val NLL: 32.5919, Val Acc: 0.6000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 18, Train NLL: 30.6735\n",
      "Epoch: 18, Val NLL: 31.4547, Val Acc: 0.6600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 19, Train NLL: 28.1712\n",
      "Epoch: 19, Val NLL: 27.8100, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 20, Train NLL: 25.6718\n",
      "Epoch: 20, Val NLL: 25.2965, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 21, Train NLL: 25.9226\n",
      "Epoch: 21, Val NLL: 25.9388, Val Acc: 0.7600\n",
      "LR = 0.0001\n",
      "Epoch: 22, Train NLL: 22.6010\n",
      "Epoch: 22, Val NLL: 23.7746, Val Acc: 0.7000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 23, Train NLL: 20.0590\n",
      "Epoch: 23, Val NLL: 26.3494, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 24, Train NLL: 20.0044\n",
      "Epoch: 24, Val NLL: 28.3588, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 25, Train NLL: 21.2727\n",
      "Epoch: 25, Val NLL: 23.2906, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 26, Train NLL: 20.4930\n",
      "Epoch: 26, Val NLL: 24.4365, Val Acc: 0.6800\n",
      "LR = 0.0001\n",
      "Epoch: 27, Train NLL: 17.7528\n",
      "Epoch: 27, Val NLL: 24.7326, Val Acc: 0.6800\n",
      "LR = 0.0001\n",
      "Epoch: 28, Train NLL: 21.0248\n",
      "Epoch: 28, Val NLL: 28.2488, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 29, Train NLL: 17.6801\n",
      "Epoch: 29, Val NLL: 24.0578, Val Acc: 0.7000\n",
      "LR = 0.0001\n",
      "Epoch: 30, Train NLL: 17.7284\n",
      "Epoch: 30, Val NLL: 23.2864, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 31, Train NLL: 16.7738\n",
      "Epoch: 31, Val NLL: 24.5298, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 32, Train NLL: 17.8983\n",
      "Epoch: 32, Val NLL: 23.0199, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 33, Train NLL: 15.0363\n",
      "Epoch: 33, Val NLL: 25.7608, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 34, Train NLL: 15.8237\n",
      "Epoch: 34, Val NLL: 23.8936, Val Acc: 0.7200\n",
      "LR = 0.0001\n",
      "Epoch: 35, Train NLL: 17.0512\n",
      "Epoch: 35, Val NLL: 26.8000, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 36, Train NLL: 17.5496\n",
      "Epoch: 36, Val NLL: 22.5358, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 37, Train NLL: 15.1726\n",
      "Epoch: 37, Val NLL: 22.8108, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 38, Train NLL: 16.5415\n",
      "Epoch: 38, Val NLL: 23.1141, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 39, Train NLL: 14.5874\n",
      "Epoch: 39, Val NLL: 30.6250, Val Acc: 0.7200\n",
      "LR = 0.0001\n",
      "Epoch: 40, Train NLL: 15.8498\n",
      "Epoch: 40, Val NLL: 22.9989, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 41, Train NLL: 16.4705\n",
      "Epoch: 41, Val NLL: 22.6711, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 42, Train NLL: 13.9294\n",
      "Epoch: 42, Val NLL: 25.2433, Val Acc: 0.7200\n",
      "LR = 0.0001\n",
      "Epoch: 43, Train NLL: 15.1239\n",
      "Epoch: 43, Val NLL: 21.7783, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 44, Train NLL: 15.9532\n",
      "Epoch: 44, Val NLL: 21.5995, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 45, Train NLL: 13.9950\n",
      "Epoch: 45, Val NLL: 22.4659, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 46, Train NLL: 14.5871\n",
      "Epoch: 46, Val NLL: 21.2240, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 47, Train NLL: 16.3783\n",
      "Epoch: 47, Val NLL: 21.8139, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 48, Train NLL: 15.8396\n",
      "Epoch: 48, Val NLL: 21.5350, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 49, Train NLL: 14.8311\n",
      "Epoch: 49, Val NLL: 22.0198, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 50, Train NLL: 14.1838\n",
      "Epoch: 50, Val NLL: 21.3882, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 51, Train NLL: 13.7889\n",
      "Epoch: 51, Val NLL: 20.9866, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 52, Train NLL: 12.6299\n",
      "Epoch: 52, Val NLL: 23.8942, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 53, Train NLL: 12.1305\n",
      "Epoch: 53, Val NLL: 20.7364, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 54, Train NLL: 11.8616\n",
      "Epoch: 54, Val NLL: 21.6748, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 55, Train NLL: 15.0305\n",
      "Epoch: 55, Val NLL: 20.2538, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 56, Train NLL: 11.2574\n",
      "Epoch: 56, Val NLL: 21.1792, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 57, Train NLL: 12.5544\n",
      "Epoch: 57, Val NLL: 20.6187, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 58, Train NLL: 12.5660\n",
      "Epoch: 58, Val NLL: 20.0447, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 59, Train NLL: 11.2827\n",
      "Epoch: 59, Val NLL: 22.2910, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 60, Train NLL: 14.1528\n",
      "Epoch: 60, Val NLL: 20.9814, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 61, Train NLL: 12.0278\n",
      "Epoch: 61, Val NLL: 20.6165, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 62, Train NLL: 12.0405\n",
      "Epoch: 62, Val NLL: 22.1314, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 63, Train NLL: 12.1107\n",
      "Epoch: 63, Val NLL: 19.9087, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 64, Train NLL: 10.5537\n",
      "Epoch: 64, Val NLL: 19.6258, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 65, Train NLL: 10.0480\n",
      "Epoch: 65, Val NLL: 20.0765, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 66, Train NLL: 10.4535\n",
      "Epoch: 66, Val NLL: 19.7543, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 67, Train NLL: 10.5069\n",
      "Epoch: 67, Val NLL: 19.2389, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 68, Train NLL: 10.8822\n",
      "Epoch: 68, Val NLL: 21.5215, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 69, Train NLL: 9.5151\n",
      "Epoch: 69, Val NLL: 19.2783, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 70, Train NLL: 9.6904\n",
      "Epoch: 70, Val NLL: 21.5210, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 71, Train NLL: 10.4913\n",
      "Epoch: 71, Val NLL: 19.6242, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 72, Train NLL: 11.4826\n",
      "Epoch: 72, Val NLL: 19.4508, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 73, Train NLL: 9.2291\n",
      "Epoch: 73, Val NLL: 19.8030, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 74, Train NLL: 9.6925\n",
      "Epoch: 74, Val NLL: 19.2197, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 75, Train NLL: 8.5433\n",
      "Epoch: 75, Val NLL: 20.1955, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 76, Train NLL: 9.4392\n",
      "Epoch: 76, Val NLL: 19.1244, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 77, Train NLL: 8.2782\n",
      "Epoch: 77, Val NLL: 20.9930, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 78, Train NLL: 9.3251\n",
      "Epoch: 78, Val NLL: 20.3190, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 79, Train NLL: 8.9829\n",
      "Epoch: 79, Val NLL: 19.2105, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 80, Train NLL: 10.3006\n",
      "Epoch: 80, Val NLL: 22.5193, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 81, Train NLL: 7.4410\n",
      "Epoch: 81, Val NLL: 19.8568, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 82, Train NLL: 7.3481\n",
      "Epoch: 82, Val NLL: 19.9044, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 83, Train NLL: 8.8046\n",
      "Epoch: 83, Val NLL: 22.3641, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 84, Train NLL: 9.4002\n",
      "Epoch: 84, Val NLL: 18.8451, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 85, Train NLL: 6.9581\n",
      "Epoch: 85, Val NLL: 19.8490, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 86, Train NLL: 7.6785\n",
      "Epoch: 86, Val NLL: 26.5253, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 87, Train NLL: 8.7325\n",
      "Epoch: 87, Val NLL: 18.7903, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 88, Train NLL: 6.8193\n",
      "Epoch: 88, Val NLL: 20.2624, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 89, Train NLL: 7.7092\n",
      "Epoch: 89, Val NLL: 19.5632, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 90, Train NLL: 7.8020\n",
      "Epoch: 90, Val NLL: 21.0073, Val Acc: 0.8400\n",
      "LR = 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91, Train NLL: 7.3497\n",
      "Epoch: 91, Val NLL: 21.8068, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 92, Train NLL: 6.2891\n",
      "Epoch: 92, Val NLL: 20.3960, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 93, Train NLL: 6.6234\n",
      "Epoch: 93, Val NLL: 19.8022, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 94, Train NLL: 8.7323\n",
      "Epoch: 94, Val NLL: 18.8535, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 95, Train NLL: 6.8555\n",
      "Epoch: 95, Val NLL: 19.4858, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 96, Train NLL: 6.0016\n",
      "Epoch: 96, Val NLL: 20.4090, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 97, Train NLL: 4.8574\n",
      "Epoch: 97, Val NLL: 22.4974, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 98, Train NLL: 5.9782\n",
      "Epoch: 98, Val NLL: 20.3884, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 99, Train NLL: 4.9569\n",
      "Epoch: 99, Val NLL: 21.0085, Val Acc: 0.8800\n",
      "LR = 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1e8\n",
    "for e in range(100):\n",
    "    train_utils.embedding_training_loop(e, train_loader, net, criterion, optimizer,pool_fn)\n",
    "    loss = train_utils.embedding_validation_loop(e, valid_loader, net, criterion,pool_fn)\n",
    "    print('LR = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if loss < best_loss:\n",
    "        torch.save(net.state_dict(),'convnet_mean_embed.pt')\n",
    "        best_loss = loss\n",
    "        print('WROTE MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9172077922077922"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pool_fn(x):\n",
    "    v = torch.mean(x,0)\n",
    "    return v\n",
    "\n",
    "state_dict = torch.load('convnet_mean_embed.pt')\n",
    "net.load_state_dict(state_dict)\n",
    "net = net.cuda()\n",
    "compute_auc(valid_loader, net, criterion,pool_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Attention Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = 512\n",
    "hidden_size = 512\n",
    "output_size = 1\n",
    "attn = models.Attention(input_size, hidden_size, output_size)\n",
    "attn.cuda()\n",
    "pool_fn = models.pool(attn)\n",
    "\n",
    "n_conv_layers = 2\n",
    "n_fc_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = [512,512]\n",
    "dropout=0.5\n",
    "net = models.ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "net = net.cuda()\n",
    "\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters = [p for p in net.parameters()]\n",
    "parameters.extend([p for p in attn.parameters()])\n",
    "optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train NLL: 34.2017\n",
      "Epoch: 0, Val NLL: 35.2774, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 1, Train NLL: 34.5259\n",
      "Epoch: 1, Val NLL: 34.7277, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 2, Train NLL: 33.9126\n",
      "Epoch: 2, Val NLL: 35.0359, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 3, Train NLL: 33.9082\n",
      "Epoch: 3, Val NLL: 34.9596, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 4, Train NLL: 33.8870\n",
      "Epoch: 4, Val NLL: 34.7221, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 5, Train NLL: 33.9272\n",
      "Epoch: 5, Val NLL: 35.0649, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 6, Train NLL: 33.8881\n",
      "Epoch: 6, Val NLL: 34.7648, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 7, Train NLL: 33.7409\n",
      "Epoch: 7, Val NLL: 34.9427, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 8, Train NLL: 33.6009\n",
      "Epoch: 8, Val NLL: 34.6753, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 9, Train NLL: 33.8874\n",
      "Epoch: 9, Val NLL: 34.7165, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 10, Train NLL: 33.3413\n",
      "Epoch: 10, Val NLL: 34.1933, Val Acc: 0.5600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 11, Train NLL: 32.9587\n",
      "Epoch: 11, Val NLL: 34.5491, Val Acc: 0.4600\n",
      "LR = 0.0001\n",
      "Epoch: 12, Train NLL: 33.6145\n",
      "Epoch: 12, Val NLL: 34.0887, Val Acc: 0.5000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 13, Train NLL: 32.4752\n",
      "Epoch: 13, Val NLL: 33.2712, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 14, Train NLL: 31.5077\n",
      "Epoch: 14, Val NLL: 31.7577, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 15, Train NLL: 28.6268\n",
      "Epoch: 15, Val NLL: 45.1582, Val Acc: 0.4400\n",
      "LR = 0.0001\n",
      "Epoch: 16, Train NLL: 29.3900\n",
      "Epoch: 16, Val NLL: 36.0798, Val Acc: 0.5400\n",
      "LR = 0.0001\n",
      "Epoch: 17, Train NLL: 25.9055\n",
      "Epoch: 17, Val NLL: 24.7528, Val Acc: 0.7600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 18, Train NLL: 20.9235\n",
      "Epoch: 18, Val NLL: 25.1094, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 19, Train NLL: 24.1126\n",
      "Epoch: 19, Val NLL: 27.1799, Val Acc: 0.7200\n",
      "LR = 0.0001\n",
      "Epoch: 20, Train NLL: 18.2114\n",
      "Epoch: 20, Val NLL: 25.1224, Val Acc: 0.7600\n",
      "LR = 0.0001\n",
      "Epoch: 21, Train NLL: 21.5136\n",
      "Epoch: 21, Val NLL: 26.5342, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 22, Train NLL: 16.0854\n",
      "Epoch: 22, Val NLL: 23.3062, Val Acc: 0.7600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 23, Train NLL: 18.9905\n",
      "Epoch: 23, Val NLL: 22.9083, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 24, Train NLL: 15.4451\n",
      "Epoch: 24, Val NLL: 22.7508, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 25, Train NLL: 15.3336\n",
      "Epoch: 25, Val NLL: 23.6363, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 26, Train NLL: 16.7707\n",
      "Epoch: 26, Val NLL: 21.6390, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 27, Train NLL: 13.5671\n",
      "Epoch: 27, Val NLL: 21.2201, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 28, Train NLL: 16.5064\n",
      "Epoch: 28, Val NLL: 23.3018, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 29, Train NLL: 13.4177\n",
      "Epoch: 29, Val NLL: 22.1780, Val Acc: 0.7400\n",
      "LR = 0.0001\n",
      "Epoch: 30, Train NLL: 10.9159\n",
      "Epoch: 30, Val NLL: 29.9702, Val Acc: 0.7600\n",
      "LR = 0.0001\n",
      "Epoch: 31, Train NLL: 14.7209\n",
      "Epoch: 31, Val NLL: 21.9628, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 32, Train NLL: 16.9575\n",
      "Epoch: 32, Val NLL: 21.0142, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 33, Train NLL: 15.1902\n",
      "Epoch: 33, Val NLL: 20.0543, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 34, Train NLL: 12.8418\n",
      "Epoch: 34, Val NLL: 19.6612, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 35, Train NLL: 9.9689\n",
      "Epoch: 35, Val NLL: 21.5961, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 36, Train NLL: 11.3203\n",
      "Epoch: 36, Val NLL: 20.5228, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 37, Train NLL: 13.0146\n",
      "Epoch: 37, Val NLL: 21.3806, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 38, Train NLL: 15.8417\n",
      "Epoch: 38, Val NLL: 21.5296, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 39, Train NLL: 11.6998\n",
      "Epoch: 39, Val NLL: 19.2263, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 40, Train NLL: 14.6697\n",
      "Epoch: 40, Val NLL: 23.2179, Val Acc: 0.7600\n",
      "LR = 0.0001\n",
      "Epoch: 41, Train NLL: 15.2718\n",
      "Epoch: 41, Val NLL: 25.5164, Val Acc: 0.7600\n",
      "LR = 0.0001\n",
      "Epoch: 42, Train NLL: 14.1073\n",
      "Epoch: 42, Val NLL: 18.7448, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 43, Train NLL: 10.1726\n",
      "Epoch: 43, Val NLL: 20.9483, Val Acc: 0.7600\n",
      "LR = 0.0001\n",
      "Epoch: 44, Train NLL: 13.1440\n",
      "Epoch: 44, Val NLL: 20.2943, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 45, Train NLL: 11.3715\n",
      "Epoch: 45, Val NLL: 19.0052, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 46, Train NLL: 10.5259\n",
      "Epoch: 46, Val NLL: 19.6306, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 47, Train NLL: 9.8321\n",
      "Epoch: 47, Val NLL: 21.7591, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 48, Train NLL: 10.3568\n",
      "Epoch: 48, Val NLL: 17.8550, Val Acc: 0.9000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 49, Train NLL: 9.5132\n",
      "Epoch: 49, Val NLL: 18.5315, Val Acc: 0.9000\n",
      "LR = 0.0001\n",
      "Epoch: 50, Train NLL: 11.7668\n",
      "Epoch: 50, Val NLL: 20.7694, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 51, Train NLL: 11.1354\n",
      "Epoch: 51, Val NLL: 17.3985, Val Acc: 0.9000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 52, Train NLL: 9.0842\n",
      "Epoch: 52, Val NLL: 17.3510, Val Acc: 0.9000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 53, Train NLL: 8.7696\n",
      "Epoch: 53, Val NLL: 21.6536, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 54, Train NLL: 8.3225\n",
      "Epoch: 54, Val NLL: 21.5494, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 55, Train NLL: 9.9232\n",
      "Epoch: 55, Val NLL: 23.3367, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 56, Train NLL: 10.2761\n",
      "Epoch: 56, Val NLL: 17.1451, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 57, Train NLL: 8.7489\n",
      "Epoch: 57, Val NLL: 23.0917, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 58, Train NLL: 14.0720\n",
      "Epoch: 58, Val NLL: 18.1113, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 59, Train NLL: 11.2543\n",
      "Epoch: 59, Val NLL: 17.6532, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 60, Train NLL: 11.1520\n",
      "Epoch: 60, Val NLL: 17.2019, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 61, Train NLL: 13.1972\n",
      "Epoch: 61, Val NLL: 17.5451, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 62, Train NLL: 9.3301\n",
      "Epoch: 62, Val NLL: 16.9553, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 63, Train NLL: 10.8004\n",
      "Epoch: 63, Val NLL: 18.5996, Val Acc: 0.8000\n",
      "LR = 0.0001\n",
      "Epoch: 64, Train NLL: 9.0086\n",
      "Epoch: 64, Val NLL: 17.9635, Val Acc: 0.9000\n",
      "LR = 0.0001\n",
      "Epoch: 65, Train NLL: 7.5353\n",
      "Epoch: 65, Val NLL: 17.9415, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 66, Train NLL: 10.1669\n",
      "Epoch: 66, Val NLL: 17.9407, Val Acc: 0.9000\n",
      "LR = 0.0001\n",
      "Epoch: 67, Train NLL: 8.2767\n",
      "Epoch: 67, Val NLL: 16.8945, Val Acc: 0.9000\n",
      "LR = 0.0001\n",
      "WROTE MODEL\n",
      "Epoch: 68, Train NLL: 7.7180\n",
      "Epoch: 68, Val NLL: 17.9280, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 69, Train NLL: 7.3309\n",
      "Epoch: 69, Val NLL: 21.9807, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 70, Train NLL: 7.7769\n",
      "Epoch: 70, Val NLL: 17.7228, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 71, Train NLL: 6.7803\n",
      "Epoch: 71, Val NLL: 17.9228, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 72, Train NLL: 5.6776\n",
      "Epoch: 72, Val NLL: 23.0689, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 73, Train NLL: 11.7806\n",
      "Epoch: 73, Val NLL: 18.7126, Val Acc: 0.9000\n",
      "LR = 0.0001\n",
      "Epoch: 74, Train NLL: 8.0043\n",
      "Epoch: 74, Val NLL: 18.1427, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 75, Train NLL: 7.7007\n",
      "Epoch: 75, Val NLL: 22.0371, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 76, Train NLL: 6.6785\n",
      "Epoch: 76, Val NLL: 17.3758, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 77, Train NLL: 12.2931\n",
      "Epoch: 77, Val NLL: 21.8092, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 78, Train NLL: 5.5413\n",
      "Epoch: 78, Val NLL: 20.3217, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 79, Train NLL: 8.3879\n",
      "Epoch: 79, Val NLL: 20.3994, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 80, Train NLL: 6.1907\n",
      "Epoch: 80, Val NLL: 26.0813, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 81, Train NLL: 6.4642\n",
      "Epoch: 81, Val NLL: 22.4523, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 82, Train NLL: 7.2689\n",
      "Epoch: 82, Val NLL: 21.3793, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 83, Train NLL: 5.5035\n",
      "Epoch: 83, Val NLL: 22.6598, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 84, Train NLL: 13.6430\n",
      "Epoch: 84, Val NLL: 23.8560, Val Acc: 0.7800\n",
      "LR = 0.0001\n",
      "Epoch: 85, Train NLL: 8.4734\n",
      "Epoch: 85, Val NLL: 17.9716, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 86, Train NLL: 7.3911\n",
      "Epoch: 86, Val NLL: 20.5830, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 87, Train NLL: 5.6305\n",
      "Epoch: 87, Val NLL: 28.6511, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 88, Train NLL: 5.9929\n",
      "Epoch: 88, Val NLL: 22.1134, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 89, Train NLL: 7.7492\n",
      "Epoch: 89, Val NLL: 18.8770, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 90, Train NLL: 4.6469\n",
      "Epoch: 90, Val NLL: 20.9030, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 91, Train NLL: 6.0262\n",
      "Epoch: 91, Val NLL: 23.5397, Val Acc: 0.8200\n",
      "LR = 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Train NLL: 7.0301\n",
      "Epoch: 92, Val NLL: 21.6809, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 93, Train NLL: 5.0907\n",
      "Epoch: 93, Val NLL: 20.2760, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 94, Train NLL: 4.6036\n",
      "Epoch: 94, Val NLL: 23.3069, Val Acc: 0.8400\n",
      "LR = 0.0001\n",
      "Epoch: 95, Train NLL: 12.0253\n",
      "Epoch: 95, Val NLL: 20.7364, Val Acc: 0.8200\n",
      "LR = 0.0001\n",
      "Epoch: 96, Train NLL: 7.5517\n",
      "Epoch: 96, Val NLL: 17.3303, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 97, Train NLL: 5.3005\n",
      "Epoch: 97, Val NLL: 18.8591, Val Acc: 0.8800\n",
      "LR = 0.0001\n",
      "Epoch: 98, Train NLL: 5.2294\n",
      "Epoch: 98, Val NLL: 20.5604, Val Acc: 0.8600\n",
      "LR = 0.0001\n",
      "Epoch: 99, Train NLL: 3.4743\n",
      "Epoch: 99, Val NLL: 19.1279, Val Acc: 0.8800\n",
      "LR = 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1e8\n",
    "for e in range(100):\n",
    "    train_utils.embedding_training_loop(e, train_loader, net, criterion, optimizer,pool_fn)\n",
    "    loss = train_utils.embedding_validation_loop(e, valid_loader, net, criterion,pool_fn)\n",
    "    print('LR = {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    if loss < best_loss:\n",
    "        torch.save(net.state_dict(),'convnet_attention_embed.pt')\n",
    "        torch.save(attn.state_dict(),'attention_pool_embed.pt')\n",
    "        best_loss = loss\n",
    "        print('WROTE MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9464285714285714"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_dict = torch.load('convnet_attention_embed.pt')\n",
    "net.load_state_dict(state_dict)\n",
    "net = net.cuda()\n",
    "\n",
    "input_size = 512\n",
    "hidden_size = 512\n",
    "output_size = 1\n",
    "attn = models.Attention(input_size, hidden_size, output_size)\n",
    "state_dict = torch.load('attention_pool_embed.pt')\n",
    "attn.load_state_dict(state_dict)\n",
    "attn.cuda()\n",
    "pool_fn = models.pool(attn)\n",
    "\n",
    "\n",
    "compute_auc(valid_loader, net, criterion,pool_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Rationales Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10703\n"
     ]
    }
   ],
   "source": [
    "n_conv_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = 512\n",
    "n_rnn_layers = 2\n",
    "dropout=0.5\n",
    "gen = models.Generator(n_conv_layers, kernel_size, n_conv_filters, hidden_size, n_rnn_layers, dropout=dropout)\n",
    "#state_dict = torch.load('/home/sxchao/MSI_prediction/labeled_nuclei_project/generator.pt')\n",
    "#gen.load_state_dict(state_dict)\n",
    "gen = gen.cuda()\n",
    "\n",
    "n_conv_layers = 2\n",
    "n_fc_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = [512,512]\n",
    "dropout=0.5\n",
    "enc = models.ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "#state_dict = torch.load('/home/sxchao/MSI_prediction/labeled_nuclei_project/encoder.pt')\n",
    "#enc.load_state_dict(state_dict)\n",
    "end = enc.cuda()\n",
    "def pool_fn(x):\n",
    "    v = torch.mean(x,0)\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "lamb1 = 0\n",
    "lamb2 = 0\n",
    "xent = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "temp = 5\n",
    "params = list(enc.parameters()) + list(gen.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-6)\n",
    "total_val_tiles = 0\n",
    "\n",
    "for slide,label in valid_loader:\n",
    "    total_val_tiles += slide.shape[1]\n",
    "    \n",
    "print(total_val_tiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 18.840653, Train Omega: 0.0000, Fraction of Tiles: 0.9847\n",
      "Epoch: 0, Val Loss: 24.5958, Val Acc: 0.8200, Fraction of Tiles: 1.0000, Total Tiles: 10703.0\n",
      "========== Train Set ==========\n",
      "Epoch: 0, Val Loss: 18.0476, Val Acc: 0.8571, Fraction of Tiles: 1.0000, Total Tiles: 9659.0\n",
      "Lambda: 0.0000000, LR: 0.0001000, Temperature: 5.0000000\n",
      "Epoch: 1, Train Loss: 18.166777, Train Omega: 0.0000, Fraction of Tiles: 0.9857\n",
      "Epoch: 1, Val Loss: 24.4054, Val Acc: 0.8000, Fraction of Tiles: 1.0000, Total Tiles: 10703.0\n",
      "Epoch: 2, Train Loss: 19.524276, Train Omega: 0.0000, Fraction of Tiles: 0.9863\n",
      "Epoch: 2, Val Loss: 25.2501, Val Acc: 0.7400, Fraction of Tiles: 1.0000, Total Tiles: 10703.0\n",
      "Epoch: 3, Train Loss: 20.246536, Train Omega: 0.0000, Fraction of Tiles: 0.9866\n",
      "Epoch: 3, Val Loss: 32.6224, Val Acc: 0.6600, Fraction of Tiles: 1.0000, Total Tiles: 10703.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-872d346205c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#best_val_frac = 0.10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrationales_training_loop_GS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac_tiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_tiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrationales_validation_loop_GS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MSI_prediction/labeled_nuclei_project/train_utils.py\u001b[0m in \u001b[0;36mrationales_training_loop_GS\u001b[0;34m(e, train_loader, gen, enc, pool_fn, lamb1, lamb2, xent, learning_rate, optimizer, temp)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrationale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mlogits_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# compute loss and regularization term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#best_val_loss = 1e8 \n",
    "#best_val_frac = 0.10\n",
    "for e in range(500):\n",
    "    train_utils.rationales_training_loop_GS(e, train_loader, gen, enc, pool_fn, lamb1, lamb2, xent, learning_rate, optimizer,temp)\n",
    "    loss, frac_tiles, total_tiles = train_utils.rationales_validation_loop_GS(e, valid_loader, gen, enc, pool_fn, xent, scheduler)\n",
    "    if e > 50:\n",
    "        lamb1 += 0.001\n",
    "        temp -= 0.25\n",
    "    temp = np.max([temp,1])\n",
    "    lamb1 = np.min([lamb1,0.4])\n",
    "    if e % 5 == 0:\n",
    "        print('========== Train Set ==========')\n",
    "        _, _, _ = train_utils.rationales_validation_loop_GS(e, train_loader, gen, enc, pool_fn, xent, scheduler)\n",
    "        print('Lambda: {0:0.7f}, LR: {1:0.7f}, Temperature: {2:0.7f}'.format(lamb1, optimizer.state_dict()['param_groups'][0]['lr'], temp))\n",
    "    if loss < best_val_loss and frac_tiles < best_val_frac and total_tiles == total_val_tiles:\n",
    "        best_val_loss = loss\n",
    "        torch.save(gen.state_dict(),'generator.pt')\n",
    "        torch.save(enc.state_dict(),'encoder.pt')\n",
    "        print('WROTE MODEL!')\n",
    "    #if frac_tiles < 0.9:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conv_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = 512\n",
    "n_rnn_layers = 2\n",
    "dropout=0.5\n",
    "gen = models.Generator(n_conv_layers, kernel_size, n_conv_filters, hidden_size, n_rnn_layers, dropout=dropout)\n",
    "state_dict = torch.load('generator_retrain.pt')\n",
    "gen.load_state_dict(state_dict)\n",
    "gen = gen.cuda()\n",
    "\n",
    "n_conv_layers = 2\n",
    "n_fc_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = [512,512]\n",
    "dropout=0.5\n",
    "enc = models.ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "state_dict = torch.load('encoder_retrain.pt')\n",
    "enc.load_state_dict(state_dict)\n",
    "enc = enc.cuda()\n",
    "\n",
    "\n",
    "#compute_auc(valid_loader, net, criterion,pool_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rationales_auc(valid_loader, gen, enc, pool_fn):\n",
    "    gen.eval()\n",
    "    enc.eval()\n",
    "    \n",
    "    labels = []\n",
    "    logits = []\n",
    "\n",
    "    for slide,label in valid_loader:\n",
    "        slide, label = slide.squeeze(0).cuda(), label.cuda()\n",
    "\n",
    "        prez = gen(slide)\n",
    "        z = torch.argmax(prez, dim=2).squeeze(0)\n",
    "        rationale = slide[z==1,:,:,:]\n",
    "\n",
    "        output = enc(rationale)\n",
    "        pool = pool_fn(output)\n",
    "        y_hat = enc.classification_layer(pool)\n",
    "        logits.append(y_hat[1].detach().cpu().numpy())\n",
    "        labels.extend(label.float().cpu().numpy())\n",
    "\n",
    "    \n",
    "    auc = roc_auc_score(labels, logits)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9561688311688312"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_rationales_auc(valid_loader, gen, enc, pool_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Val Loss: 16.6692, Val Acc: 0.8800, Fraction of Tiles: 0.0126, Total Tiles: 10703.0\n"
     ]
    }
   ],
   "source": [
    "loss, frac_tiles, total_tiles = train_utils.rationales_validation_loop_GS(e, valid_loader, gen, enc, pool_fn, xent, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
