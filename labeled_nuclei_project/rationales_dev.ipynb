{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import models\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = data_utils.COAD_dataset(data_utils.COAD_DEV)\n",
    "dev_loader = torch.utils.data.DataLoader(dev, batch_size=1, shuffle=True , pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_conv_layers, kernel_size, n_conv_filters, hidden_size, n_rnn_layers, dropout=0.5):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_conv_layers = n_conv_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_conv_filters = n_conv_filters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_rnn_layers = n_rnn_layers\n",
    "        self.conv_layers = []\n",
    "        self.m = nn.MaxPool2d(2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "         \n",
    "        in_channels = 3        \n",
    "        for layer in range(self.n_conv_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, self.n_conv_filters[layer], self.kernel_size[layer]))\n",
    "            self.conv_layers.append(self.relu)\n",
    "            self.conv_layers.append(self.m)\n",
    "            in_channels = self.n_conv_filters[layer]\n",
    "        self.conv = nn.Sequential(*self.conv_layers)\n",
    "        in_channels = in_channels * 25\n",
    "\n",
    "        self.lstm = nn.LSTM(in_channels, self.hidden_size, self.n_rnn_layers, batch_first=True, \n",
    "                            dropout=dropout, bidirectional=True) \n",
    "        in_channels = hidden_size * 2\n",
    "        self.classification_layer = nn.Linear(in_channels, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.conv(x)\n",
    "        embed = embed.view(1,x.shape[0],-1)\n",
    "        self.lstm.flatten_parameters()\n",
    "        output, hidden = self.lstm(embed)\n",
    "        y = self.classification_layer(output)\n",
    "        return y\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 36, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(36, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(1200, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (classification_layer): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_conv_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = 512\n",
    "n_rnn_layers = 2\n",
    "dropout=0.5\n",
    "gen = Generator(n_conv_layers, kernel_size, n_conv_filters, hidden_size, n_rnn_layers, dropout=dropout)\n",
    "gen.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (n): Dropout(p=0.5)\n",
       "  (relu): ReLU()\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 36, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(36, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1200, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5)\n",
       "  )\n",
       "  (classification_layer): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_conv_layers = 2\n",
    "n_fc_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = [512,512]\n",
    "dropout=0.5\n",
    "enc = models.ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb1 = 0.01\n",
    "lamb2 = 0.01\n",
    "xent = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsm = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(enc.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(slide, gen, num_samples):\n",
    "    zis = []\n",
    "    grads = []\n",
    "    all_grads = []\n",
    "    for p in gen.parameters():\n",
    "        start = [num_samples]\n",
    "        start.extend(list(p.shape))\n",
    "        all_grads.append(torch.zeros(start, device='cuda'))\n",
    "        grads.append(torch.zeros(p.shape, device='cuda'))\n",
    "        \n",
    "    for sample in range(num_samples):\n",
    "        preds = gen(slide)\n",
    "        logits = lsm(preds).squeeze(0)\n",
    "        b = torch.distributions.bernoulli.Bernoulli(logits=logits[:,1])\n",
    "        zi = b.sample() #zis = b.sample(torch.Size([batch_size]))\n",
    "        zis.append(zi)\n",
    "\n",
    "        logprobs = b.log_prob(zi).sum()\n",
    "        logprobs.backward()\n",
    "\n",
    "        for idx,p in enumerate(gen.parameters()):\n",
    "            all_grads[idx][sample] = p.grad\n",
    "\n",
    "        gen.zero_grad()\n",
    "        \n",
    "    return zis, grads, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationales_training(e, train_loader, gen, enc, pool_fn, num_samples, lamb1, lamb2, xent,\n",
    "                        learning_rate, optimizer):\n",
    "    gen.train()\n",
    "    enc.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for slide,label in train_loader:\n",
    "        slide,label = slide.squeeze(0).cuda(),label.cuda()\n",
    "        zis, grads, all_grads = sampler(slide, gen, num_samples)\n",
    "        zis = torch.stack(zis)\n",
    "        \n",
    "        rationales = [slide[zi==1,:,:,:] for zi in zis]\n",
    "        sampled_rationales = torch.cat(rationales,dim=0)\n",
    "        outputs = enc(sampled_rationales)\n",
    "        \n",
    "        lens = zis.sum(dim=1)\n",
    "        indexs = torch.cat([torch.zeros(1),torch.cumsum(lens,0).cpu()]).int()\n",
    "        outputs = [outputs[indexs[n]:indexs[n+1]] for n,ix in enumerate(indexs[:-1])]\n",
    "        \n",
    "        pool = torch.stack([pool_fn(o).unsqueeze(0) for o in outputs])\n",
    "        y_hat = enc.classification_layer(pool.squeeze(1))\n",
    "        \n",
    "        znorm = torch.norm(zis.float(), p=1, dim=1)\n",
    "        zdist = torch.sum(torch.abs(zis[:,:-1] - zis[:,1:]), dim=1)\n",
    "        omega = (lamb1 * znorm) + (lamb2 * zdist)\n",
    "        cost = xent(y_hat, label.repeat(num_samples)) + omega\n",
    "        \n",
    "        for sample in range(num_samples):\n",
    "            for idx,p in enumerate(gen.parameters()):\n",
    "                grads[idx] += cost[sample] * all_grads[idx][sample] \n",
    "        \n",
    "        for idx,p in enumerate(gen.parameters()):\n",
    "            p.data = p.data - learning_rate * (grads[idx] / float(num_samples))\n",
    "            \n",
    "        loss = cost.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "    print('Epoch: {0}, Train Loss: {1:0.4f}'.format(e, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 318.7721\n"
     ]
    }
   ],
   "source": [
    "e = 0\n",
    "rationales_training(e, dev_loader, gen, enc, pool_fn, num_samples, lamb1, lamb2, xent, learning_rate, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
