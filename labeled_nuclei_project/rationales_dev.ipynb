{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import data_utils\n",
    "import train_utils\n",
    "import models\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = data_utils.COAD_dataset(data_utils.COAD_DEV)\n",
    "dev_loader = torch.utils.data.DataLoader(dev, batch_size=1, shuffle=True , pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_conv_layers, kernel_size, n_conv_filters, hidden_size, n_rnn_layers, dropout=0.5):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_conv_layers = n_conv_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_conv_filters = n_conv_filters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_rnn_layers = n_rnn_layers\n",
    "        self.conv_layers = []\n",
    "        self.m = nn.MaxPool2d(2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "         \n",
    "        in_channels = 3        \n",
    "        for layer in range(self.n_conv_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, self.n_conv_filters[layer], self.kernel_size[layer]))\n",
    "            self.conv_layers.append(self.relu)\n",
    "            self.conv_layers.append(self.m)\n",
    "            in_channels = self.n_conv_filters[layer]\n",
    "        self.conv = nn.Sequential(*self.conv_layers)\n",
    "        in_channels = in_channels * 25\n",
    "\n",
    "        self.lstm = nn.LSTM(in_channels, self.hidden_size, self.n_rnn_layers, batch_first=True, \n",
    "                            dropout=dropout, bidirectional=True) \n",
    "        in_channels = hidden_size * 2\n",
    "        self.classification_layer = nn.Linear(in_channels, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.conv(x)\n",
    "        embed = embed.view(1,x.shape[0],-1)\n",
    "        self.lstm.flatten_parameters()\n",
    "        output, hidden = self.lstm(embed)\n",
    "        y = self.classification_layer(output)\n",
    "        return y\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slide,label in dev_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5216, 0.3098, 0.5961,  ..., 0.5294, 0.3098, 0.6157],\n",
       "          [0.5647, 0.3451, 0.6471,  ..., 0.6706, 0.4392, 0.6902],\n",
       "          [0.6510, 0.4157, 0.6706,  ..., 0.6510, 0.4078, 0.7137],\n",
       "          ...,\n",
       "          [0.6118, 0.3843, 0.7255,  ..., 0.5765, 0.3490, 0.6667],\n",
       "          [0.5412, 0.3098, 0.6353,  ..., 0.6118, 0.3882, 0.6588],\n",
       "          [0.6863, 0.4588, 0.7176,  ..., 0.8706, 0.6078, 0.8353]],\n",
       "\n",
       "         [[0.5843, 0.3608, 0.7059,  ..., 0.7451, 0.5098, 0.8275],\n",
       "          [0.6235, 0.3961, 0.7098,  ..., 0.6745, 0.4588, 0.7216],\n",
       "          [0.6431, 0.4235, 0.6941,  ..., 0.8118, 0.5490, 0.7804],\n",
       "          ...,\n",
       "          [0.8314, 0.6392, 0.8510,  ..., 0.8196, 0.5804, 0.8353],\n",
       "          [0.7961, 0.5608, 0.8157,  ..., 0.8196, 0.5725, 0.8196],\n",
       "          [0.7843, 0.5412, 0.7882,  ..., 0.7490, 0.5098, 0.7373]],\n",
       "\n",
       "         [[0.5804, 0.3765, 0.6392,  ..., 0.8000, 0.5569, 0.8196],\n",
       "          [0.7843, 0.5451, 0.8078,  ..., 0.9725, 0.7333, 0.9294],\n",
       "          [0.9490, 0.7059, 0.9059,  ..., 0.7804, 0.5529, 0.7843],\n",
       "          ...,\n",
       "          [0.7725, 0.5725, 0.8275,  ..., 0.7765, 0.5373, 0.8118],\n",
       "          [0.7529, 0.5098, 0.7765,  ..., 0.6784, 0.4431, 0.7490],\n",
       "          [0.6353, 0.4000, 0.7020,  ..., 0.7373, 0.4980, 0.7490]]],\n",
       "\n",
       "\n",
       "        [[[0.9843, 0.7804, 0.9294,  ..., 0.6745, 0.4353, 0.6745],\n",
       "          [0.7569, 0.5137, 0.7373,  ..., 0.5608, 0.3176, 0.6275],\n",
       "          [0.5490, 0.3176, 0.6118,  ..., 0.4588, 0.2392, 0.5647],\n",
       "          ...,\n",
       "          [0.6902, 0.4431, 0.7608,  ..., 0.5765, 0.3176, 0.6157],\n",
       "          [0.6196, 0.3647, 0.6667,  ..., 0.4706, 0.2706, 0.5725],\n",
       "          [0.5333, 0.3294, 0.6314,  ..., 0.6275, 0.4118, 0.7176]],\n",
       "\n",
       "         [[0.6784, 0.4275, 0.7608,  ..., 0.6235, 0.3725, 0.6667],\n",
       "          [0.6471, 0.3961, 0.6863,  ..., 0.4745, 0.2667, 0.5765],\n",
       "          [0.5373, 0.3255, 0.6392,  ..., 0.6078, 0.3922, 0.6824],\n",
       "          ...,\n",
       "          [0.5137, 0.3059, 0.6000,  ..., 0.5137, 0.2941, 0.6039],\n",
       "          [0.4863, 0.2706, 0.5882,  ..., 0.6549, 0.4471, 0.7176],\n",
       "          [0.6980, 0.4941, 0.7686,  ..., 0.7765, 0.5176, 0.7765]],\n",
       "\n",
       "         [[0.5216, 0.3098, 0.6196,  ..., 0.5412, 0.3216, 0.6353],\n",
       "          [0.4980, 0.2863, 0.6118,  ..., 0.7608, 0.5490, 0.8314],\n",
       "          [0.7765, 0.5725, 0.8431,  ..., 0.7922, 0.5333, 0.7922],\n",
       "          ...,\n",
       "          [0.7294, 0.5608, 0.7176,  ..., 0.7373, 0.5373, 0.7961],\n",
       "          [0.7294, 0.5294, 0.7882,  ..., 0.6314, 0.4000, 0.6902],\n",
       "          [0.6118, 0.3922, 0.6627,  ..., 0.6314, 0.3961, 0.6392]]],\n",
       "\n",
       "\n",
       "        [[[0.8314, 0.6000, 0.8039,  ..., 0.6000, 0.3608, 0.6510],\n",
       "          [0.6471, 0.4157, 0.6784,  ..., 0.7647, 0.5294, 0.8078],\n",
       "          [0.8157, 0.5843, 0.8706,  ..., 0.6118, 0.3843, 0.7255],\n",
       "          ...,\n",
       "          [0.8902, 0.6353, 0.8471,  ..., 0.5216, 0.3137, 0.6118],\n",
       "          [0.5098, 0.2980, 0.5882,  ..., 0.4824, 0.2706, 0.5725],\n",
       "          [0.4392, 0.2314, 0.5176,  ..., 0.8392, 0.6431, 0.8784]],\n",
       "\n",
       "         [[0.8902, 0.6353, 0.8510,  ..., 0.5569, 0.3333, 0.6353],\n",
       "          [0.4706, 0.2471, 0.5451,  ..., 0.4902, 0.2745, 0.6078],\n",
       "          [0.4706, 0.2549, 0.5882,  ..., 0.8314, 0.6392, 0.8510],\n",
       "          ...,\n",
       "          [0.8745, 0.6275, 0.8784,  ..., 0.8314, 0.6471, 0.8118],\n",
       "          [0.9686, 0.8157, 0.8941,  ..., 0.5725, 0.3725, 0.6627],\n",
       "          [0.5176, 0.3255, 0.5961,  ..., 0.6471, 0.4431, 0.7020]],\n",
       "\n",
       "         [[0.9059, 0.6667, 0.9059,  ..., 0.6431, 0.4314, 0.6627],\n",
       "          [0.6196, 0.4275, 0.6078,  ..., 0.4549, 0.2627, 0.5647],\n",
       "          [0.4706, 0.2784, 0.5765,  ..., 0.7725, 0.5725, 0.8275],\n",
       "          ...,\n",
       "          [0.6902, 0.4627, 0.7176,  ..., 0.5333, 0.3176, 0.6392],\n",
       "          [0.4941, 0.2784, 0.5961,  ..., 0.7020, 0.4784, 0.7255],\n",
       "          [0.6314, 0.4118, 0.6431,  ..., 0.8941, 0.7608, 0.9098]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.8627, 0.6235, 0.8314,  ..., 0.7255, 0.4431, 0.6902],\n",
       "          [0.9059, 0.6353, 0.8510,  ..., 0.7098, 0.4863, 0.7216],\n",
       "          [0.7294, 0.5059, 0.7490,  ..., 0.7529, 0.5647, 0.8118],\n",
       "          ...,\n",
       "          [0.8745, 0.6667, 0.8275,  ..., 0.9882, 0.7529, 0.9490],\n",
       "          [0.9373, 0.7059, 0.8784,  ..., 0.7529, 0.5725, 0.7412],\n",
       "          [0.8431, 0.6784, 0.8431,  ..., 0.8510, 0.5451, 0.7569]],\n",
       "\n",
       "         [[0.9490, 0.7569, 0.9059,  ..., 0.9922, 0.7725, 0.9333],\n",
       "          [0.9216, 0.6941, 0.8627,  ..., 0.9020, 0.7294, 0.9294],\n",
       "          [0.9961, 0.9098, 1.0000,  ..., 0.8627, 0.5569, 0.7647],\n",
       "          ...,\n",
       "          [1.0000, 0.8902, 0.9882,  ..., 0.9882, 0.7373, 0.9255],\n",
       "          [0.9765, 0.7529, 0.9373,  ..., 1.0000, 0.8745, 1.0000],\n",
       "          [0.9294, 0.7765, 0.9137,  ..., 0.6157, 0.3725, 0.6039]],\n",
       "\n",
       "         [[1.0000, 0.8431, 0.9569,  ..., 0.9843, 0.7451, 0.9373],\n",
       "          [0.8863, 0.6902, 0.8784,  ..., 0.9569, 0.8157, 0.9608],\n",
       "          [0.8000, 0.6392, 0.8118,  ..., 0.6784, 0.4392, 0.6745],\n",
       "          ...,\n",
       "          [0.8980, 0.6824, 0.8471,  ..., 0.4000, 0.2314, 0.5647],\n",
       "          [0.4824, 0.2941, 0.6078,  ..., 0.8353, 0.6353, 0.8431],\n",
       "          [0.9216, 0.7020, 0.9137,  ..., 0.9765, 0.7961, 0.9961]]],\n",
       "\n",
       "\n",
       "        [[[0.8980, 0.7020, 0.8706,  ..., 0.8157, 0.6039, 0.7686],\n",
       "          [0.9373, 0.7216, 0.8745,  ..., 0.5804, 0.3647, 0.6510],\n",
       "          [0.5020, 0.3059, 0.5608,  ..., 0.8588, 0.5608, 0.7804],\n",
       "          ...,\n",
       "          [1.0000, 0.9255, 1.0000,  ..., 0.9059, 0.6706, 0.8431],\n",
       "          [0.8863, 0.6235, 0.8078,  ..., 0.9333, 0.7176, 0.9137],\n",
       "          [1.0000, 0.8157, 0.9725,  ..., 0.6902, 0.4824, 0.7451]],\n",
       "\n",
       "         [[0.9922, 0.9020, 0.9843,  ..., 0.9020, 0.6549, 0.8235],\n",
       "          [0.9333, 0.6667, 0.8510,  ..., 1.0000, 0.7882, 0.9765],\n",
       "          [0.9608, 0.7765, 0.9098,  ..., 0.6863, 0.4824, 0.7686],\n",
       "          ...,\n",
       "          [0.6980, 0.4784, 0.6941,  ..., 0.9098, 0.6549, 0.8157],\n",
       "          [0.8471, 0.6431, 0.8902,  ..., 0.9176, 0.7294, 0.9059],\n",
       "          [0.9451, 0.7569, 0.9255,  ..., 0.5294, 0.3176, 0.6353]],\n",
       "\n",
       "         [[0.8000, 0.5608, 0.7647,  ..., 0.9294, 0.6745, 0.8471],\n",
       "          [0.9020, 0.6863, 0.9373,  ..., 0.7882, 0.6000, 0.7882],\n",
       "          [0.9137, 0.7176, 0.9020,  ..., 0.4863, 0.2784, 0.6000],\n",
       "          ...,\n",
       "          [0.9020, 0.6667, 0.8353,  ..., 0.7882, 0.5686, 0.7216],\n",
       "          [0.9255, 0.7569, 0.8784,  ..., 0.7098, 0.5020, 0.7137],\n",
       "          [0.8627, 0.6588, 0.8471,  ..., 0.7882, 0.6118, 0.8588]]],\n",
       "\n",
       "\n",
       "        [[[0.7098, 0.4745, 0.7765,  ..., 0.7569, 0.5059, 0.7490],\n",
       "          [0.7255, 0.4784, 0.7020,  ..., 0.7373, 0.4941, 0.7294],\n",
       "          [0.7412, 0.4863, 0.7333,  ..., 0.9137, 0.7020, 0.8902],\n",
       "          ...,\n",
       "          [0.8588, 0.6157, 0.8235,  ..., 0.6392, 0.4078, 0.6902],\n",
       "          [0.4196, 0.2196, 0.5490,  ..., 1.0000, 0.8588, 0.9686],\n",
       "          [1.0000, 0.8588, 1.0000,  ..., 0.9216, 0.7412, 0.9020]],\n",
       "\n",
       "         [[0.8039, 0.5529, 0.7647,  ..., 0.5569, 0.3451, 0.6157],\n",
       "          [0.4235, 0.2392, 0.5333,  ..., 0.9843, 0.8000, 0.9059],\n",
       "          [0.9529, 0.7647, 0.9098,  ..., 0.9765, 0.8118, 0.9569],\n",
       "          ...,\n",
       "          [0.6235, 0.3882, 0.6627,  ..., 0.7765, 0.5255, 0.8157],\n",
       "          [0.6275, 0.4039, 0.7529,  ..., 0.7882, 0.5529, 0.7922],\n",
       "          [0.7804, 0.5294, 0.8000,  ..., 0.8510, 0.6039, 0.8157]],\n",
       "\n",
       "         [[0.6235, 0.3843, 0.6902,  ..., 0.7961, 0.5529, 0.8118],\n",
       "          [0.7490, 0.5176, 0.8235,  ..., 0.8157, 0.5647, 0.8235],\n",
       "          [0.7020, 0.4549, 0.7608,  ..., 0.8118, 0.5765, 0.7961],\n",
       "          ...,\n",
       "          [0.7647, 0.5373, 0.7725,  ..., 0.8118, 0.5765, 0.7961],\n",
       "          [0.8275, 0.5922, 0.8039,  ..., 0.9961, 0.9059, 0.9882],\n",
       "          [0.8667, 0.6863, 0.8314,  ..., 0.7529, 0.5020, 0.7804]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide.squeeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 27, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 36, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(36, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(1200, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (classification_layer): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_conv_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = 512\n",
    "n_rnn_layers = 2\n",
    "dropout=0.5\n",
    "gen = Generator(n_conv_layers, kernel_size, n_conv_filters, hidden_size, n_rnn_layers, dropout=dropout)\n",
    "gen.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 2])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide = slide.cuda()\n",
    "preds = gen(slide)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 27, 27])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = torch.argmax(preds, dim=2)\n",
    "selector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale = slide[selector.squeeze_(0)==1,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (n): Dropout(p=0.5)\n",
       "  (relu): ReLU()\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 36, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(36, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1200, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5)\n",
       "  )\n",
       "  (classification_layer): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_conv_layers = 2\n",
    "n_fc_layers = 2\n",
    "kernel_size = [4,3]\n",
    "n_conv_filters = [36,48]\n",
    "hidden_size = [512,512]\n",
    "dropout=0.5\n",
    "enc = models.ConvNet(n_conv_layers, n_fc_layers, kernel_size, n_conv_filters, hidden_size, dropout=dropout)\n",
    "enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = enc(rationale)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pool_fn(x):\n",
    "    #v,a = torch.max(x,0)\n",
    "    v = torch.mean(x,0)\n",
    "    return v\n",
    "\n",
    "pool = pool_fn(output).unsqueeze(0)\n",
    "output = enc.classification_layer(pool)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fn\n",
    "lamb1 = 0.01\n",
    "lamb2 = 0.01\n",
    "\n",
    "xent = nn.CrossEntropyLoss()\n",
    "znorm = torch.norm(selector.float(), p=1)\n",
    "zdist = torch.sum(torch.abs(selector[:-1] - selector[1:]))\n",
    "omega = (lamb1 * znorm) + (lamb2 * zdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1499, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = xent(output, label.cuda()) + omega\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsm = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage\n",
    "zis = []\n",
    "grads = []\n",
    "all_grads = []\n",
    "for p in gen.parameters():\n",
    "    grads.append(torch.zeros(p.shape, device='cuda'))\n",
    "    start = [batch_size]\n",
    "    start.extend(list(p.shape))\n",
    "    all_grads.append(torch.zeros(start, device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass, sample, backward pass\n",
    "for sample in range(batch_size):\n",
    "    preds = gen(slide)\n",
    "    logits = lsm(preds).squeeze(0)\n",
    "    b = torch.distributions.bernoulli.Bernoulli(logits=logits[:,1])\n",
    "    zi = b.sample() #zis = b.sample(torch.Size([batch_size]))\n",
    "    zis.append(zi)\n",
    "\n",
    "    logprobs = b.log_prob(zi).sum()\n",
    "    logprobs.backward()\n",
    "\n",
    "    idx = 0\n",
    "    for p in gen.parameters():\n",
    "        all_grads[idx][sample] = p.grad\n",
    "        idx += 1\n",
    "\n",
    "    gen.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zis = torch.stack(zis)\n",
    "zis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rationales\n",
    "rationales = [slide[zi==1,:,:,:] for zi in zis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len = zis.sum(dim=1).max().cpu().numpy().astype(int)\n",
    "#padded_rationales = torch.stack([F.pad(r,(0,0,0,0,0,0,0,max_len - r.shape[0])) for r in rationales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate rationales\n",
    "lens = zis.sum(dim=1)\n",
    "sampled_rationales = torch.cat(rationales,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sampled_rationales.shape[0] == lens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed rationales to encoder\n",
    "outputs = enc(sampled_rationales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwind rationales\n",
    "indexs = torch.cat([torch.zeros(1),torch.cumsum(lens,0).cpu()]).int()\n",
    "outputs = [outputs[indexs[n]:indexs[n+1]] for n,ix in enumerate(indexs[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool\n",
    "pool = torch.stack([pool_fn(o).unsqueeze(0) for o in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get preds\n",
    "y_hat = enc.classification_layer(pool.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc cost\n",
    "znorm = torch.norm(zis.float(), p=1, dim=1)\n",
    "zdist = torch.sum(torch.abs(zis[:,:-1] - zis[:,1:]), dim=1)\n",
    "omega = (lamb1 * znorm) + (lamb2 * zdist)\n",
    "cost = xent(y_hat, label.repeat(batch_size).cuda()) + omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(batch_size):\n",
    "    idx = 0\n",
    "    for p in gen.parameters():\n",
    "        grads[idx] += cost[sample] * all_grads[idx][sample]\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-1.4194e-01, -5.9658e-02,  5.4201e-02, -8.4589e-02],\n",
      "          [-6.6064e-02, -8.9865e-02,  9.8630e-02, -5.9396e-02],\n",
      "          [-6.5374e-02, -4.2674e-03,  8.2360e-02,  1.3028e-01],\n",
      "          [-6.7901e-02,  1.2268e-01, -8.8947e-02,  7.0122e-02]],\n",
      "\n",
      "         [[ 6.9880e-02, -1.3379e-01, -1.0102e-02,  9.1821e-02],\n",
      "          [ 6.5529e-02, -5.4471e-02,  6.7287e-02,  7.2666e-02],\n",
      "          [-5.4700e-02,  1.1422e-01,  7.5434e-02, -3.3597e-02],\n",
      "          [ 1.2686e-01, -5.9633e-02,  7.1653e-02,  3.5697e-02]],\n",
      "\n",
      "         [[ 5.5489e-02, -2.7086e-02,  1.2634e-01,  2.9274e-02],\n",
      "          [-9.8619e-02,  3.9167e-02,  8.0053e-02, -1.0697e-02],\n",
      "          [-2.4472e-03, -7.5699e-02,  1.7157e-02,  1.5214e-05],\n",
      "          [-1.4202e-01, -1.7843e-02, -1.1125e-01, -2.7665e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.4006e-02,  1.1912e-01, -4.5461e-02, -5.5560e-02],\n",
      "          [ 8.0556e-02, -5.6804e-02,  3.4504e-02,  8.6827e-02],\n",
      "          [-1.0442e-02, -9.6858e-03, -6.3741e-02, -9.8955e-02],\n",
      "          [-3.0317e-02, -1.3779e-01,  9.0331e-02, -2.5783e-02]],\n",
      "\n",
      "         [[ 2.0498e-02,  8.0781e-02, -2.6667e-02,  1.2073e-01],\n",
      "          [-4.2474e-02,  6.8856e-02,  3.7479e-02,  1.2896e-01],\n",
      "          [-5.2344e-02, -5.4538e-02, -4.5137e-02,  6.2873e-02],\n",
      "          [-1.2400e-01,  1.0127e-01,  1.4070e-01,  1.1152e-01]],\n",
      "\n",
      "         [[-1.4399e-01,  8.4566e-02,  7.0349e-02,  9.7478e-02],\n",
      "          [-7.4455e-02,  1.2174e-01, -1.0107e-01,  1.1810e-01],\n",
      "          [-7.8116e-02,  1.2545e-01,  1.4406e-01,  1.4334e-01],\n",
      "          [-5.4359e-02, -4.5326e-02,  2.5960e-02,  1.1847e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.4941e-03,  4.2201e-02, -9.4182e-02,  1.7769e-02],\n",
      "          [ 1.0327e-01,  4.8642e-02,  2.0486e-02,  1.2321e-02],\n",
      "          [ 9.6315e-04,  2.5307e-02,  1.2120e-02, -7.6708e-03],\n",
      "          [ 1.0060e-01,  1.3060e-01,  2.8037e-02, -5.8965e-02]],\n",
      "\n",
      "         [[-1.2128e-01,  5.3373e-02, -7.3301e-02,  8.7904e-02],\n",
      "          [ 2.7103e-02,  1.2185e-01, -8.1094e-02, -2.0397e-02],\n",
      "          [-3.5837e-02,  2.0097e-02,  1.3838e-01, -1.4073e-01],\n",
      "          [ 4.4682e-02,  1.0758e-02, -3.5350e-02,  5.4021e-02]],\n",
      "\n",
      "         [[-2.4837e-02,  9.8930e-02, -1.1974e-01,  1.1025e-01],\n",
      "          [ 4.0326e-02,  1.1405e-01, -8.1512e-02,  5.2052e-02],\n",
      "          [ 2.6997e-02, -1.0495e-01, -5.2942e-02,  7.4802e-02],\n",
      "          [ 7.7192e-02, -5.4510e-02, -1.1123e-01, -8.4851e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.3892e-02, -8.5709e-02, -5.8364e-02,  9.4538e-02],\n",
      "          [-1.1189e-01,  6.8786e-02,  1.1701e-01, -9.6636e-02],\n",
      "          [-1.1577e-01, -9.0977e-02,  8.2740e-02, -6.9141e-02],\n",
      "          [ 2.2490e-03, -2.5403e-03,  9.8131e-02,  8.7615e-02]],\n",
      "\n",
      "         [[ 4.3823e-02,  1.3782e-01, -8.0859e-02, -7.0798e-02],\n",
      "          [-5.5767e-02, -2.7559e-02, -5.8812e-02, -3.7335e-02],\n",
      "          [ 8.2889e-02, -7.7761e-02,  3.5694e-02, -7.7317e-02],\n",
      "          [ 1.3488e-01, -1.0100e-01, -5.4284e-02,  2.5478e-03]],\n",
      "\n",
      "         [[-1.3387e-01,  5.4335e-03, -7.2896e-02, -1.3108e-01],\n",
      "          [-4.8748e-02,  1.2103e-01,  7.9866e-02, -7.2166e-04],\n",
      "          [-3.6284e-02,  1.2622e-01, -4.8398e-02,  1.6402e-02],\n",
      "          [-3.1004e-02, -1.9558e-02,  6.8150e-02,  4.2311e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.4939e-02, -1.4038e-01, -4.5353e-02, -7.0841e-02],\n",
      "          [ 1.1298e-01, -3.8256e-02, -1.8676e-02,  9.6148e-02],\n",
      "          [ 4.0669e-02,  1.1527e-02,  1.3550e-01,  4.1039e-02],\n",
      "          [-2.8056e-02,  1.3205e-01,  8.7406e-02, -1.1212e-01]],\n",
      "\n",
      "         [[ 7.4907e-02, -9.0033e-02, -6.2284e-02, -7.4798e-02],\n",
      "          [ 8.7421e-02,  4.5741e-02, -2.7822e-02, -1.9329e-03],\n",
      "          [ 7.0426e-02,  1.1819e-01,  5.8570e-02, -5.6641e-02],\n",
      "          [ 9.3946e-02, -6.0318e-02, -1.1806e-02,  1.3231e-01]],\n",
      "\n",
      "         [[-2.6599e-02,  1.1931e-01,  1.0447e-01, -5.4102e-02],\n",
      "          [ 3.9200e-02, -5.9557e-02,  4.3733e-03,  1.2102e-01],\n",
      "          [-1.4735e-02,  6.5172e-02,  1.1276e-01, -1.0765e-01],\n",
      "          [ 9.4720e-03, -7.9977e-02,  9.2126e-02, -9.3376e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.5830e-02,  1.2805e-01, -6.0794e-02,  1.1447e-01],\n",
      "          [ 1.8800e-02, -1.3324e-01, -1.2947e-01, -3.7051e-02],\n",
      "          [ 6.7296e-02, -1.1578e-01, -6.7639e-03, -1.2559e-01],\n",
      "          [-5.9835e-02,  8.4296e-03, -9.3267e-02, -1.1591e-01]],\n",
      "\n",
      "         [[ 1.0224e-02,  1.1017e-01, -1.1647e-01, -8.8799e-02],\n",
      "          [-8.2115e-02,  3.2766e-02, -1.1508e-01, -3.6549e-02],\n",
      "          [-1.2359e-01,  6.2788e-02, -1.3418e-01, -5.4384e-02],\n",
      "          [ 9.2442e-02,  1.0512e-01,  1.2387e-01, -1.4407e-01]],\n",
      "\n",
      "         [[ 1.2639e-01, -5.7304e-02, -1.4366e-01,  1.2777e-01],\n",
      "          [-9.6869e-02,  7.2121e-02, -8.8595e-02, -7.3436e-02],\n",
      "          [-9.1572e-04, -9.9471e-02, -1.2196e-01,  1.4238e-01],\n",
      "          [-3.5521e-02,  4.9884e-02, -1.1396e-01,  1.1592e-01]]]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0195, -0.0508,  0.0954,  0.0014, -0.0173,  0.0730, -0.1064,  0.0855,\n",
      "        -0.0512,  0.0884,  0.0039,  0.0864,  0.1348,  0.0999,  0.0620, -0.1304,\n",
      "        -0.0096,  0.0925,  0.0321,  0.0397, -0.0569, -0.0319, -0.0584,  0.1335,\n",
      "        -0.0212, -0.0608,  0.0729,  0.1287, -0.0714,  0.0941, -0.0379,  0.1145,\n",
      "         0.0628,  0.0640, -0.1214, -0.1203], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-3.7996e-02, -4.7209e-02,  5.2747e-02],\n",
      "          [-2.0788e-02,  4.3140e-02,  3.5502e-02],\n",
      "          [-2.3488e-02, -2.3979e-02, -1.6800e-02]],\n",
      "\n",
      "         [[-2.6981e-02, -5.7776e-03, -3.2644e-02],\n",
      "          [-1.3324e-02,  1.4746e-02, -2.5728e-02],\n",
      "          [ 2.1962e-02,  4.4079e-02,  2.4547e-02]],\n",
      "\n",
      "         [[ 8.4393e-03, -3.5784e-02,  5.3867e-02],\n",
      "          [ 3.4937e-02, -2.7996e-02, -1.6079e-02],\n",
      "          [-4.7654e-02, -2.4958e-02, -5.0921e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4614e-02,  7.1277e-03, -3.9918e-02],\n",
      "          [ 1.0743e-02, -1.7166e-02,  3.7292e-02],\n",
      "          [-1.0393e-02, -4.5811e-02, -5.1099e-02]],\n",
      "\n",
      "         [[-2.9775e-02,  5.1265e-02, -2.8078e-02],\n",
      "          [-1.6197e-02, -3.6654e-03, -1.3016e-02],\n",
      "          [-5.0415e-02,  3.2516e-02, -3.1818e-02]],\n",
      "\n",
      "         [[-5.5252e-03,  4.1469e-02, -1.5862e-02],\n",
      "          [-5.3921e-02,  4.1734e-02, -2.7420e-02],\n",
      "          [ 1.3003e-02, -1.2829e-02, -2.0279e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.5189e-02, -4.4485e-02, -6.7143e-03],\n",
      "          [ 3.7028e-03, -8.5123e-05,  1.9873e-03],\n",
      "          [-5.0826e-02,  3.2404e-02, -3.7007e-02]],\n",
      "\n",
      "         [[-4.9989e-03, -2.9321e-02, -5.0632e-02],\n",
      "          [ 4.8636e-02, -3.3431e-02, -1.7086e-02],\n",
      "          [ 1.4469e-02, -5.4696e-02,  2.5651e-02]],\n",
      "\n",
      "         [[-3.4520e-02,  5.7905e-03, -9.1665e-03],\n",
      "          [ 3.6636e-02,  1.4624e-02,  5.4317e-02],\n",
      "          [-6.5469e-03,  8.9197e-03,  3.9192e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2853e-02,  3.0293e-02,  3.8974e-02],\n",
      "          [-1.7112e-02,  4.1259e-03,  4.4190e-02],\n",
      "          [-4.5828e-02,  3.0586e-02, -2.6620e-02]],\n",
      "\n",
      "         [[-3.2147e-02,  3.5844e-03,  4.3430e-02],\n",
      "          [ 3.8262e-02, -4.1411e-02, -1.2660e-02],\n",
      "          [-2.5186e-03, -3.2424e-02, -3.9928e-02]],\n",
      "\n",
      "         [[ 7.0696e-03, -2.5841e-03,  5.1655e-02],\n",
      "          [-4.1724e-02,  3.1434e-02,  5.7274e-03],\n",
      "          [ 5.1678e-02, -3.9539e-02, -3.0804e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.6221e-02,  2.8086e-02,  3.3795e-02],\n",
      "          [-1.5369e-02, -3.1009e-02, -2.5381e-02],\n",
      "          [-3.7596e-02,  1.9240e-02,  2.7289e-02]],\n",
      "\n",
      "         [[-4.2261e-02, -2.7096e-02,  5.3600e-02],\n",
      "          [-2.1119e-02, -2.5687e-03,  3.1490e-02],\n",
      "          [ 2.7444e-02,  2.3615e-04,  2.4762e-02]],\n",
      "\n",
      "         [[ 8.4374e-03,  3.0716e-02,  2.6337e-03],\n",
      "          [-1.6547e-02,  3.9033e-03, -4.4398e-02],\n",
      "          [-1.2283e-02, -2.2112e-03,  1.0591e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1434e-03,  5.4800e-02,  4.3570e-02],\n",
      "          [ 4.4456e-02,  1.3059e-03,  4.9894e-02],\n",
      "          [-1.4576e-03, -4.3037e-02,  4.9312e-02]],\n",
      "\n",
      "         [[ 8.3519e-03, -2.1162e-02,  4.8078e-02],\n",
      "          [-9.9472e-03,  1.9304e-02, -8.6346e-03],\n",
      "          [ 1.5797e-02,  3.2146e-02,  2.6620e-02]],\n",
      "\n",
      "         [[ 4.6878e-02, -3.8604e-02, -1.6269e-02],\n",
      "          [-4.1617e-02, -5.3145e-02,  4.8534e-02],\n",
      "          [-2.6322e-02, -8.1674e-03,  5.4270e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.7456e-02, -1.5225e-02, -2.2990e-02],\n",
      "          [-4.1145e-02,  2.7237e-02,  4.5295e-02],\n",
      "          [-1.4334e-02,  2.0586e-02, -9.5493e-03]],\n",
      "\n",
      "         [[ 1.3903e-02,  5.0693e-02,  2.1087e-02],\n",
      "          [-1.9734e-02,  3.5195e-02, -2.4422e-02],\n",
      "          [ 9.6339e-03,  5.0086e-02, -4.0081e-02]],\n",
      "\n",
      "         [[-1.8412e-02, -5.1395e-02, -2.5758e-02],\n",
      "          [ 3.7292e-02,  4.1048e-02, -1.9981e-02],\n",
      "          [ 1.3286e-02, -1.2035e-02, -2.5337e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.7906e-02,  2.5828e-02,  1.4421e-02],\n",
      "          [-1.8102e-02,  3.1317e-02,  4.6645e-02],\n",
      "          [ 8.5749e-03,  2.2501e-02, -5.4481e-02]],\n",
      "\n",
      "         [[ 5.4556e-02, -4.8749e-02, -5.0211e-02],\n",
      "          [-1.7613e-02,  5.2517e-02, -3.1759e-02],\n",
      "          [-1.8459e-02, -1.7646e-02,  1.8129e-02]],\n",
      "\n",
      "         [[ 4.1151e-02,  4.6307e-02,  2.0965e-02],\n",
      "          [ 3.4855e-02, -3.9158e-02,  4.4170e-02],\n",
      "          [-3.0781e-02, -1.0182e-02, -3.6222e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.3210e-02,  6.8241e-03, -1.4002e-02],\n",
      "          [ 3.0237e-02, -4.3372e-02, -4.0291e-02],\n",
      "          [-1.2509e-02, -4.0389e-02, -5.4900e-02]],\n",
      "\n",
      "         [[-5.1477e-02, -5.3938e-02, -1.1222e-02],\n",
      "          [-5.9550e-03, -6.5714e-03,  8.9024e-03],\n",
      "          [ 1.8079e-02,  4.4424e-02, -4.0137e-02]],\n",
      "\n",
      "         [[-4.7367e-02,  1.0339e-02, -3.9158e-02],\n",
      "          [ 1.6470e-02,  2.4894e-02, -1.2527e-02],\n",
      "          [-8.3386e-03,  4.5086e-02,  3.3996e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5793e-02,  1.5904e-02,  4.0751e-02],\n",
      "          [ 4.0226e-02, -3.2079e-03, -4.3044e-02],\n",
      "          [-5.3014e-03, -2.4004e-02, -1.4384e-02]],\n",
      "\n",
      "         [[-6.2000e-03, -5.0306e-02, -1.2234e-02],\n",
      "          [-3.0568e-02,  2.5908e-02, -4.2852e-02],\n",
      "          [-1.9618e-02, -3.5123e-02, -1.7018e-02]],\n",
      "\n",
      "         [[ 2.6284e-02, -2.5645e-02,  4.1717e-02],\n",
      "          [-4.4076e-02,  5.4679e-02,  1.9780e-02],\n",
      "          [ 4.8661e-02,  6.9374e-03,  4.3545e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3421e-02, -5.4369e-03,  3.3067e-02],\n",
      "          [ 2.1795e-02,  2.8732e-02, -2.0211e-02],\n",
      "          [ 4.2020e-02, -4.4340e-02, -1.4741e-02]],\n",
      "\n",
      "         [[-3.0505e-03, -1.5344e-02, -5.0979e-03],\n",
      "          [-2.5562e-02,  3.5858e-02, -1.0833e-02],\n",
      "          [-4.5411e-02,  8.9226e-03, -2.5755e-02]],\n",
      "\n",
      "         [[-1.6362e-03, -4.8547e-02,  1.6301e-02],\n",
      "          [-5.4334e-02,  3.9726e-03,  5.3186e-02],\n",
      "          [-1.9642e-02,  2.2328e-02,  5.4810e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5928e-02,  9.2982e-03, -3.4164e-02],\n",
      "          [-1.0793e-02, -3.9097e-02,  7.1010e-03],\n",
      "          [-2.6257e-03, -3.2184e-02, -2.2803e-02]],\n",
      "\n",
      "         [[ 3.3350e-02, -2.6358e-02,  2.4312e-02],\n",
      "          [-3.9332e-02,  4.3552e-02,  3.2874e-02],\n",
      "          [ 1.6594e-02, -2.1446e-02, -4.8491e-02]],\n",
      "\n",
      "         [[ 3.3519e-02,  2.6008e-02, -1.2985e-02],\n",
      "          [ 6.6130e-03,  4.6048e-02, -1.0652e-02],\n",
      "          [ 2.1667e-02, -7.3958e-04,  3.5114e-02]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0347, -0.0152,  0.0498,  0.0057, -0.0497, -0.0015, -0.0241,  0.0390,\n",
      "         0.0400, -0.0249, -0.0367, -0.0244, -0.0013,  0.0536, -0.0388, -0.0270,\n",
      "        -0.0540, -0.0546,  0.0313, -0.0400,  0.0436, -0.0278, -0.0477, -0.0196,\n",
      "         0.0214,  0.0040,  0.0463,  0.0553,  0.0017, -0.0116, -0.0249,  0.0228,\n",
      "         0.0035, -0.0479,  0.0455, -0.0346, -0.0029,  0.0488, -0.0364, -0.0460,\n",
      "         0.0011,  0.0346,  0.0294, -0.0330, -0.0500, -0.0183, -0.0042,  0.0033],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0319, -0.0059,  0.0298,  ..., -0.0257, -0.0025,  0.0377],\n",
      "        [ 0.0103,  0.0055, -0.0188,  ..., -0.0305,  0.0056, -0.0117],\n",
      "        [-0.0187, -0.0032, -0.0011,  ..., -0.0119, -0.0291, -0.0402],\n",
      "        ...,\n",
      "        [-0.0167, -0.0341,  0.0267,  ...,  0.0006,  0.0184,  0.0414],\n",
      "        [-0.0128,  0.0085,  0.0099,  ..., -0.0367, -0.0122,  0.0174],\n",
      "        [ 0.0069,  0.0279, -0.0206,  ...,  0.0063, -0.0141,  0.0023]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0146, -0.0218,  0.0126,  ..., -0.0400,  0.0268,  0.0395],\n",
      "        [-0.0214,  0.0245, -0.0414,  ..., -0.0100, -0.0399,  0.0171],\n",
      "        [ 0.0286,  0.0243, -0.0191,  ...,  0.0369, -0.0303,  0.0297],\n",
      "        ...,\n",
      "        [ 0.0187,  0.0398, -0.0331,  ..., -0.0087,  0.0154, -0.0115],\n",
      "        [ 0.0336, -0.0290,  0.0350,  ...,  0.0235, -0.0331, -0.0173],\n",
      "        [ 0.0180, -0.0106,  0.0258,  ...,  0.0196,  0.0247,  0.0048]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 3.6548e-02, -1.4078e-02,  5.0077e-03,  ...,  2.6494e-02,\n",
      "        -1.0483e-05,  8.0230e-03], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0231,  0.0097, -0.0264,  ..., -0.0113,  0.0128, -0.0339],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3388e-02, -8.8794e-03,  2.2858e-02,  ...,  4.5490e-03,\n",
      "         -2.5759e-02, -1.7617e-03],\n",
      "        [-1.4574e-02,  3.8452e-02, -5.5484e-05,  ...,  1.0884e-02,\n",
      "         -5.5033e-03, -6.2015e-03],\n",
      "        [ 4.1382e-02,  9.0814e-03,  9.6311e-03,  ..., -3.4916e-02,\n",
      "         -3.0494e-02, -3.9127e-02],\n",
      "        ...,\n",
      "        [ 1.4653e-02,  3.7297e-03, -2.4023e-02,  ..., -3.8004e-02,\n",
      "         -1.3485e-02, -6.3941e-03],\n",
      "        [-2.5104e-02, -2.0358e-02, -2.0515e-02,  ...,  1.9151e-02,\n",
      "         -3.9441e-03, -8.5475e-03],\n",
      "        [-3.8517e-02,  7.6923e-03, -1.2951e-02,  ...,  2.7026e-02,\n",
      "         -2.0808e-02,  3.5847e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0283, -0.0158,  0.0266,  ..., -0.0397,  0.0376, -0.0261],\n",
      "        [ 0.0242, -0.0076, -0.0383,  ..., -0.0082,  0.0392,  0.0371],\n",
      "        [-0.0142,  0.0412,  0.0193,  ..., -0.0262, -0.0117, -0.0356],\n",
      "        ...,\n",
      "        [-0.0104,  0.0278, -0.0205,  ...,  0.0292,  0.0130,  0.0301],\n",
      "        [ 0.0417,  0.0058,  0.0018,  ...,  0.0274, -0.0416, -0.0163],\n",
      "        [-0.0094,  0.0413,  0.0026,  ...,  0.0307, -0.0134,  0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0094,  0.0155, -0.0395,  ...,  0.0270, -0.0249, -0.0313],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0091,  0.0396, -0.0003,  ..., -0.0220,  0.0142,  0.0067],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0128, -0.0279, -0.0338,  ...,  0.0318, -0.0394,  0.0333],\n",
      "        [ 0.0268, -0.0270,  0.0271,  ..., -0.0043, -0.0002, -0.0131],\n",
      "        [-0.0107, -0.0215, -0.0124,  ...,  0.0174, -0.0290, -0.0360],\n",
      "        ...,\n",
      "        [-0.0109, -0.0090, -0.0367,  ...,  0.0267,  0.0187, -0.0354],\n",
      "        [-0.0168,  0.0309, -0.0251,  ..., -0.0214, -0.0382,  0.0197],\n",
      "        [-0.0145, -0.0062,  0.0323,  ..., -0.0280, -0.0076, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0152, -0.0034,  0.0335,  ...,  0.0372,  0.0397, -0.0038],\n",
      "        [-0.0268, -0.0241,  0.0109,  ..., -0.0277, -0.0223,  0.0173],\n",
      "        [ 0.0089, -0.0121, -0.0095,  ...,  0.0178,  0.0278, -0.0391],\n",
      "        ...,\n",
      "        [ 0.0397, -0.0249,  0.0302,  ...,  0.0057,  0.0103,  0.0147],\n",
      "        [ 0.0186, -0.0053, -0.0351,  ..., -0.0420,  0.0027, -0.0267],\n",
      "        [ 0.0081,  0.0321,  0.0213,  ...,  0.0026, -0.0180,  0.0069]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0169, -0.0080,  0.0276,  ...,  0.0089, -0.0031,  0.0247],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0286, -0.0208, -0.0042,  ..., -0.0108,  0.0134, -0.0235],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0384, -0.0069,  0.0419,  ...,  0.0347, -0.0027,  0.0163],\n",
      "        [ 0.0175,  0.0421,  0.0375,  ...,  0.0264,  0.0360, -0.0018],\n",
      "        [ 0.0109,  0.0375, -0.0048,  ...,  0.0343, -0.0189,  0.0359],\n",
      "        ...,\n",
      "        [ 0.0384,  0.0064,  0.0325,  ...,  0.0399, -0.0080, -0.0078],\n",
      "        [-0.0380, -0.0183, -0.0207,  ...,  0.0212, -0.0179, -0.0287],\n",
      "        [ 0.0050,  0.0033,  0.0278,  ...,  0.0148, -0.0295, -0.0216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0171,  0.0266, -0.0018,  ..., -0.0364,  0.0309,  0.0100],\n",
      "        [ 0.0176,  0.0102, -0.0394,  ..., -0.0307,  0.0015, -0.0140],\n",
      "        [ 0.0379,  0.0352,  0.0254,  ..., -0.0207,  0.0384, -0.0045],\n",
      "        ...,\n",
      "        [ 0.0164, -0.0071, -0.0351,  ...,  0.0145, -0.0096, -0.0036],\n",
      "        [-0.0062,  0.0352, -0.0389,  ...,  0.0291,  0.0275, -0.0338],\n",
      "        [ 0.0302, -0.0281,  0.0260,  ..., -0.0030, -0.0431,  0.0362]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0350, -0.0081,  0.0044,  ...,  0.0402,  0.0113, -0.0331],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0164, -0.0353, -0.0198,  ...,  0.0356,  0.0391,  0.0134],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.6986e-03,  2.6851e-02,  2.1777e-02,  ..., -1.5518e-02,\n",
      "         -1.7173e-02,  1.5399e-03],\n",
      "        [-2.1117e-02,  1.1924e-02,  3.1001e-02,  ..., -6.4715e-03,\n",
      "          1.3970e-02, -1.6006e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0086, -0.0131], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in gen.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "idx = 0\n",
    "for p in gen.parameters():\n",
    "    p.data = p.data - learning_rate * grads[idx]\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.1390, -0.0565,  0.0568, -0.0819],\n",
      "          [-0.0637, -0.0872,  0.1006, -0.0571],\n",
      "          [-0.0638, -0.0020,  0.0839,  0.1321],\n",
      "          [-0.0653,  0.1253, -0.0868,  0.0723]],\n",
      "\n",
      "         [[ 0.0715, -0.1326, -0.0098,  0.0935],\n",
      "          [ 0.0680, -0.0518,  0.0690,  0.0746],\n",
      "          [-0.0507,  0.1184,  0.0782, -0.0297],\n",
      "          [ 0.1292, -0.0578,  0.0723,  0.0378]],\n",
      "\n",
      "         [[ 0.0592, -0.0238,  0.1287,  0.0327],\n",
      "          [-0.0977,  0.0402,  0.0812, -0.0083],\n",
      "          [ 0.0002, -0.0726,  0.0189,  0.0023],\n",
      "          [-0.1379, -0.0140, -0.1093, -0.0241]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0924,  0.1287, -0.0379, -0.0464],\n",
      "          [ 0.0885, -0.0483,  0.0414,  0.0947],\n",
      "          [-0.0019, -0.0006, -0.0574, -0.0910],\n",
      "          [-0.0224, -0.1283,  0.0978, -0.0164]],\n",
      "\n",
      "         [[ 0.0303,  0.0913, -0.0181,  0.1313],\n",
      "          [-0.0338,  0.0789,  0.0453,  0.1369],\n",
      "          [-0.0435, -0.0459, -0.0392,  0.0706],\n",
      "          [-0.1145,  0.1112,  0.1486,  0.1212]],\n",
      "\n",
      "         [[-0.1368,  0.0930,  0.0766,  0.1055],\n",
      "          [-0.0659,  0.1317, -0.0929,  0.1273],\n",
      "          [-0.0696,  0.1342,  0.1500,  0.1515],\n",
      "          [-0.0470, -0.0369,  0.0321,  0.1264]]],\n",
      "\n",
      "\n",
      "        [[[-0.0004,  0.0448, -0.0915,  0.0212],\n",
      "          [ 0.1056,  0.0523,  0.0234,  0.0158],\n",
      "          [ 0.0049,  0.0290,  0.0158, -0.0040],\n",
      "          [ 0.1042,  0.1336,  0.0313, -0.0550]],\n",
      "\n",
      "         [[-0.1179,  0.0549, -0.0724,  0.0897],\n",
      "          [ 0.0302,  0.1251, -0.0787, -0.0174],\n",
      "          [-0.0326,  0.0234,  0.1406, -0.1381],\n",
      "          [ 0.0472,  0.0116, -0.0350,  0.0551]],\n",
      "\n",
      "         [[-0.0215,  0.1034, -0.1154,  0.1139],\n",
      "          [ 0.0444,  0.1190, -0.0766,  0.0572],\n",
      "          [ 0.0288, -0.1030, -0.0516,  0.0768],\n",
      "          [ 0.0803, -0.0502, -0.1069, -0.0810]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0631, -0.0869, -0.0593,  0.0940],\n",
      "          [-0.1120,  0.0677,  0.1169, -0.0964],\n",
      "          [-0.1155, -0.0916,  0.0821, -0.0694],\n",
      "          [ 0.0016, -0.0036,  0.0977,  0.0875]],\n",
      "\n",
      "         [[ 0.0450,  0.1377, -0.0802, -0.0698],\n",
      "          [-0.0557, -0.0284, -0.0595, -0.0378],\n",
      "          [ 0.0828, -0.0787,  0.0360, -0.0769],\n",
      "          [ 0.1359, -0.1012, -0.0540,  0.0032]],\n",
      "\n",
      "         [[-0.1346,  0.0040, -0.0737, -0.1317],\n",
      "          [-0.0493,  0.1197,  0.0794, -0.0010],\n",
      "          [-0.0367,  0.1250, -0.0488,  0.0160],\n",
      "          [-0.0316, -0.0209,  0.0677,  0.0419]]],\n",
      "\n",
      "\n",
      "        [[[-0.0515, -0.1346, -0.0402, -0.0671],\n",
      "          [ 0.1162, -0.0340, -0.0139,  0.1000],\n",
      "          [ 0.0457,  0.0179,  0.1417,  0.0454],\n",
      "          [-0.0244,  0.1378,  0.0923, -0.1088]],\n",
      "\n",
      "         [[ 0.0792, -0.0845, -0.0570, -0.0711],\n",
      "          [ 0.0912,  0.0501, -0.0232,  0.0021],\n",
      "          [ 0.0744,  0.1234,  0.0642, -0.0532],\n",
      "          [ 0.0983, -0.0549, -0.0068,  0.1362]],\n",
      "\n",
      "         [[-0.0238,  0.1231,  0.1096, -0.0509],\n",
      "          [ 0.0443, -0.0534,  0.0102,  0.1258],\n",
      "          [-0.0125,  0.0686,  0.1164, -0.1057],\n",
      "          [ 0.0128, -0.0756,  0.0980, -0.0899]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0358,  0.1281, -0.0608,  0.1145],\n",
      "          [ 0.0188, -0.1332, -0.1295, -0.0371],\n",
      "          [ 0.0673, -0.1158, -0.0068, -0.1256],\n",
      "          [-0.0598,  0.0084, -0.0933, -0.1159]],\n",
      "\n",
      "         [[ 0.0102,  0.1102, -0.1165, -0.0888],\n",
      "          [-0.0821,  0.0328, -0.1151, -0.0365],\n",
      "          [-0.1236,  0.0628, -0.1342, -0.0544],\n",
      "          [ 0.0924,  0.1051,  0.1239, -0.1441]],\n",
      "\n",
      "         [[ 0.1264, -0.0573, -0.1437,  0.1278],\n",
      "          [-0.0969,  0.0721, -0.0886, -0.0734],\n",
      "          [-0.0009, -0.0995, -0.1220,  0.1424],\n",
      "          [-0.0355,  0.0499, -0.1140,  0.1159]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0203, -0.0389,  0.0991, -0.0105, -0.0172,  0.0790, -0.1064,  0.0861,\n",
      "        -0.0474,  0.0847, -0.0004,  0.0825,  0.1364,  0.0861,  0.0616, -0.1304,\n",
      "        -0.0194,  0.0925,  0.0321,  0.0489, -0.0575, -0.0281, -0.0591,  0.1333,\n",
      "        -0.0208, -0.0604,  0.0729,  0.1294, -0.0641,  0.0941, -0.0355,  0.1151,\n",
      "         0.0628,  0.0623, -0.1154, -0.1203], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0380, -0.0472,  0.0527],\n",
      "          [-0.0208,  0.0431,  0.0355],\n",
      "          [-0.0235, -0.0240, -0.0168]],\n",
      "\n",
      "         [[-0.0270, -0.0058, -0.0326],\n",
      "          [-0.0133,  0.0147, -0.0257],\n",
      "          [ 0.0220,  0.0441,  0.0245]],\n",
      "\n",
      "         [[ 0.0084, -0.0358,  0.0539],\n",
      "          [ 0.0349, -0.0280, -0.0161],\n",
      "          [-0.0477, -0.0250, -0.0509]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0246,  0.0071, -0.0399],\n",
      "          [ 0.0107, -0.0172,  0.0373],\n",
      "          [-0.0104, -0.0458, -0.0511]],\n",
      "\n",
      "         [[-0.0298,  0.0513, -0.0281],\n",
      "          [-0.0162, -0.0037, -0.0130],\n",
      "          [-0.0504,  0.0325, -0.0318]],\n",
      "\n",
      "         [[-0.0055,  0.0415, -0.0159],\n",
      "          [-0.0539,  0.0417, -0.0274],\n",
      "          [ 0.0130, -0.0128, -0.0203]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0560, -0.0422, -0.0055],\n",
      "          [ 0.0049,  0.0023,  0.0032],\n",
      "          [-0.0510,  0.0332, -0.0366]],\n",
      "\n",
      "         [[ 0.0021, -0.0231, -0.0443],\n",
      "          [ 0.0540, -0.0287, -0.0124],\n",
      "          [ 0.0205, -0.0492,  0.0317]],\n",
      "\n",
      "         [[-0.0320,  0.0070, -0.0048],\n",
      "          [ 0.0397,  0.0162,  0.0593],\n",
      "          [-0.0035,  0.0107,  0.0442]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0135,  0.0303,  0.0386],\n",
      "          [-0.0165,  0.0055,  0.0447],\n",
      "          [-0.0453,  0.0311, -0.0266]],\n",
      "\n",
      "         [[-0.0294,  0.0076,  0.0468],\n",
      "          [ 0.0418, -0.0369, -0.0089],\n",
      "          [-0.0010, -0.0297, -0.0378]],\n",
      "\n",
      "         [[ 0.0071, -0.0026,  0.0517],\n",
      "          [-0.0417,  0.0314,  0.0057],\n",
      "          [ 0.0517, -0.0395, -0.0308]]],\n",
      "\n",
      "\n",
      "        [[[-0.0328,  0.0332,  0.0380],\n",
      "          [-0.0114, -0.0253, -0.0202],\n",
      "          [-0.0346,  0.0238,  0.0315]],\n",
      "\n",
      "         [[-0.0191, -0.0032,  0.0777],\n",
      "          [ 0.0024,  0.0221,  0.0569],\n",
      "          [ 0.0510,  0.0246,  0.0504]],\n",
      "\n",
      "         [[ 0.0231,  0.0477,  0.0200],\n",
      "          [-0.0017,  0.0207, -0.0273],\n",
      "          [ 0.0025,  0.0142,  0.0273]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0026,  0.0550,  0.0431],\n",
      "          [ 0.0446,  0.0016,  0.0505],\n",
      "          [-0.0012, -0.0430,  0.0495]],\n",
      "\n",
      "         [[ 0.0211, -0.0066,  0.0614],\n",
      "          [ 0.0037,  0.0337,  0.0039],\n",
      "          [ 0.0290,  0.0465,  0.0395]],\n",
      "\n",
      "         [[ 0.0469, -0.0386, -0.0163],\n",
      "          [-0.0416, -0.0531,  0.0485],\n",
      "          [-0.0263, -0.0082,  0.0543]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0463, -0.0144, -0.0220],\n",
      "          [-0.0413,  0.0281,  0.0450],\n",
      "          [-0.0130,  0.0221, -0.0090]],\n",
      "\n",
      "         [[ 0.0199,  0.0572,  0.0279],\n",
      "          [-0.0134,  0.0414, -0.0189],\n",
      "          [ 0.0164,  0.0569, -0.0330]],\n",
      "\n",
      "         [[-0.0143, -0.0468, -0.0215],\n",
      "          [ 0.0414,  0.0456, -0.0163],\n",
      "          [ 0.0176, -0.0074, -0.0204]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0485,  0.0262,  0.0151],\n",
      "          [-0.0181,  0.0314,  0.0463],\n",
      "          [ 0.0088,  0.0226, -0.0540]],\n",
      "\n",
      "         [[ 0.0584, -0.0446, -0.0462],\n",
      "          [-0.0149,  0.0558, -0.0289],\n",
      "          [-0.0148, -0.0147,  0.0201]],\n",
      "\n",
      "         [[ 0.0412,  0.0463,  0.0210],\n",
      "          [ 0.0349, -0.0392,  0.0442],\n",
      "          [-0.0308, -0.0102, -0.0362]]],\n",
      "\n",
      "\n",
      "        [[[-0.0132,  0.0068, -0.0140],\n",
      "          [ 0.0302, -0.0434, -0.0403],\n",
      "          [-0.0125, -0.0404, -0.0549]],\n",
      "\n",
      "         [[-0.0515, -0.0539, -0.0112],\n",
      "          [-0.0060, -0.0066,  0.0089],\n",
      "          [ 0.0181,  0.0444, -0.0401]],\n",
      "\n",
      "         [[-0.0474,  0.0103, -0.0392],\n",
      "          [ 0.0165,  0.0249, -0.0125],\n",
      "          [-0.0083,  0.0451,  0.0340]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0158,  0.0159,  0.0408],\n",
      "          [ 0.0402, -0.0032, -0.0430],\n",
      "          [-0.0053, -0.0240, -0.0144]],\n",
      "\n",
      "         [[-0.0062, -0.0503, -0.0122],\n",
      "          [-0.0306,  0.0259, -0.0429],\n",
      "          [-0.0196, -0.0351, -0.0170]],\n",
      "\n",
      "         [[ 0.0263, -0.0256,  0.0417],\n",
      "          [-0.0441,  0.0547,  0.0198],\n",
      "          [ 0.0487,  0.0069,  0.0435]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0228, -0.0071,  0.0312],\n",
      "          [ 0.0211,  0.0280, -0.0214],\n",
      "          [ 0.0418, -0.0457, -0.0165]],\n",
      "\n",
      "         [[-0.0071, -0.0191, -0.0082],\n",
      "          [-0.0287,  0.0319, -0.0140],\n",
      "          [-0.0485,  0.0050, -0.0287]],\n",
      "\n",
      "         [[-0.0038, -0.0492,  0.0156],\n",
      "          [-0.0568,  0.0042,  0.0531],\n",
      "          [-0.0221,  0.0220,  0.0543]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0261,  0.0095, -0.0338],\n",
      "          [-0.0112, -0.0391,  0.0066],\n",
      "          [-0.0025, -0.0320, -0.0222]],\n",
      "\n",
      "         [[ 0.0314, -0.0280,  0.0214],\n",
      "          [-0.0399,  0.0414,  0.0307],\n",
      "          [ 0.0158, -0.0234, -0.0518]],\n",
      "\n",
      "         [[ 0.0335,  0.0260, -0.0130],\n",
      "          [ 0.0066,  0.0460, -0.0107],\n",
      "          [ 0.0217, -0.0007,  0.0351]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0347, -0.0065,  0.0840,  0.0029, -0.0323, -0.0024, -0.0241,  0.0488,\n",
      "         0.0400, -0.0249, -0.0367, -0.0244, -0.0025,  0.0386, -0.0363, -0.0270,\n",
      "        -0.0344, -0.0546,  0.0313, -0.0630,  0.0439, -0.0099, -0.0477, -0.0322,\n",
      "         0.0278,  0.0049,  0.0643,  0.0592,  0.0071, -0.0291, -0.0471,  0.0207,\n",
      "         0.0227, -0.0460,  0.0415, -0.0346, -0.0112,  0.0314, -0.0364, -0.0460,\n",
      "         0.0011,  0.0323,  0.0165, -0.0418, -0.0500, -0.0088, -0.0042,  0.0017],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0319, -0.0059,  0.0298,  ..., -0.0257, -0.0025,  0.0377],\n",
      "        [ 0.0103,  0.0055, -0.0188,  ..., -0.0305,  0.0056, -0.0117],\n",
      "        [-0.0187, -0.0032, -0.0011,  ..., -0.0119, -0.0291, -0.0402],\n",
      "        ...,\n",
      "        [-0.0167, -0.0341,  0.0267,  ...,  0.0006,  0.0185,  0.0414],\n",
      "        [-0.0128,  0.0085,  0.0099,  ..., -0.0367, -0.0122,  0.0174],\n",
      "        [ 0.0069,  0.0279, -0.0206,  ...,  0.0063, -0.0140,  0.0023]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0146, -0.0218,  0.0126,  ..., -0.0400,  0.0268,  0.0395],\n",
      "        [-0.0214,  0.0245, -0.0414,  ..., -0.0100, -0.0399,  0.0171],\n",
      "        [ 0.0286,  0.0243, -0.0191,  ...,  0.0369, -0.0303,  0.0297],\n",
      "        ...,\n",
      "        [ 0.0187,  0.0398, -0.0331,  ..., -0.0087,  0.0154, -0.0115],\n",
      "        [ 0.0336, -0.0290,  0.0350,  ...,  0.0235, -0.0331, -0.0173],\n",
      "        [ 0.0180, -0.0106,  0.0258,  ...,  0.0196,  0.0247,  0.0048]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 3.6438e-02, -1.4229e-02,  4.9560e-03,  ...,  2.7226e-02,\n",
      "         7.4299e-05,  8.2495e-03], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0230,  0.0096, -0.0265,  ..., -0.0106,  0.0129, -0.0337],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3388e-02, -8.8794e-03,  2.2858e-02,  ...,  4.5510e-03,\n",
      "         -2.5765e-02, -1.7760e-03],\n",
      "        [-1.4574e-02,  3.8452e-02, -5.5484e-05,  ...,  1.0884e-02,\n",
      "         -5.5055e-03, -6.2036e-03],\n",
      "        [ 4.1382e-02,  9.0814e-03,  9.6311e-03,  ..., -3.4914e-02,\n",
      "         -3.0492e-02, -3.9128e-02],\n",
      "        ...,\n",
      "        [ 1.4653e-02,  3.7297e-03, -2.4023e-02,  ..., -3.8006e-02,\n",
      "         -1.3479e-02, -6.3908e-03],\n",
      "        [-2.5104e-02, -2.0358e-02, -2.0515e-02,  ...,  1.9155e-02,\n",
      "         -3.9383e-03, -8.5521e-03],\n",
      "        [-3.8517e-02,  7.6923e-03, -1.2951e-02,  ...,  2.7026e-02,\n",
      "         -2.0808e-02,  3.5848e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0283, -0.0158,  0.0266,  ..., -0.0397,  0.0375, -0.0261],\n",
      "        [ 0.0242, -0.0076, -0.0383,  ..., -0.0082,  0.0392,  0.0371],\n",
      "        [-0.0142,  0.0412,  0.0193,  ..., -0.0261, -0.0117, -0.0356],\n",
      "        ...,\n",
      "        [-0.0104,  0.0278, -0.0205,  ...,  0.0292,  0.0130,  0.0301],\n",
      "        [ 0.0418,  0.0058,  0.0017,  ...,  0.0274, -0.0415, -0.0163],\n",
      "        [-0.0094,  0.0413,  0.0026,  ...,  0.0307, -0.0134,  0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0090,  0.0155, -0.0392,  ...,  0.0268, -0.0245, -0.0313],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 8.6889e-03,  3.9523e-02, -1.8152e-05,  ..., -2.2181e-02,\n",
      "         1.4647e-02,  6.6531e-03], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0129, -0.0279, -0.0338,  ...,  0.0318, -0.0394,  0.0333],\n",
      "        [ 0.0268, -0.0270,  0.0271,  ..., -0.0044, -0.0002, -0.0131],\n",
      "        [-0.0107, -0.0215, -0.0124,  ...,  0.0174, -0.0290, -0.0360],\n",
      "        ...,\n",
      "        [-0.0109, -0.0090, -0.0367,  ...,  0.0267,  0.0187, -0.0354],\n",
      "        [-0.0168,  0.0309, -0.0251,  ..., -0.0213, -0.0383,  0.0197],\n",
      "        [-0.0145, -0.0062,  0.0323,  ..., -0.0280, -0.0076, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0152, -0.0034,  0.0335,  ...,  0.0372,  0.0397, -0.0038],\n",
      "        [-0.0268, -0.0241,  0.0109,  ..., -0.0277, -0.0223,  0.0173],\n",
      "        [ 0.0089, -0.0121, -0.0095,  ...,  0.0178,  0.0279, -0.0391],\n",
      "        ...,\n",
      "        [ 0.0397, -0.0249,  0.0302,  ...,  0.0057,  0.0103,  0.0147],\n",
      "        [ 0.0186, -0.0053, -0.0351,  ..., -0.0420,  0.0027, -0.0267],\n",
      "        [ 0.0081,  0.0321,  0.0213,  ...,  0.0026, -0.0180,  0.0069]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0167, -0.0079,  0.0278,  ...,  0.0090, -0.0031,  0.0247],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0287, -0.0208, -0.0040,  ..., -0.0107,  0.0134, -0.0235],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0384, -0.0069,  0.0419,  ...,  0.0347, -0.0027,  0.0163],\n",
      "        [ 0.0175,  0.0421,  0.0375,  ...,  0.0264,  0.0360, -0.0018],\n",
      "        [ 0.0109,  0.0375, -0.0048,  ...,  0.0343, -0.0189,  0.0359],\n",
      "        ...,\n",
      "        [ 0.0384,  0.0064,  0.0325,  ...,  0.0399, -0.0080, -0.0078],\n",
      "        [-0.0380, -0.0183, -0.0207,  ...,  0.0213, -0.0179, -0.0287],\n",
      "        [ 0.0050,  0.0033,  0.0278,  ...,  0.0148, -0.0294, -0.0216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0171,  0.0266, -0.0018,  ..., -0.0364,  0.0309,  0.0100],\n",
      "        [ 0.0176,  0.0102, -0.0394,  ..., -0.0307,  0.0015, -0.0140],\n",
      "        [ 0.0380,  0.0352,  0.0254,  ..., -0.0207,  0.0384, -0.0045],\n",
      "        ...,\n",
      "        [ 0.0164, -0.0071, -0.0351,  ...,  0.0145, -0.0096, -0.0036],\n",
      "        [-0.0062,  0.0352, -0.0389,  ...,  0.0290,  0.0275, -0.0338],\n",
      "        [ 0.0302, -0.0281,  0.0260,  ..., -0.0030, -0.0431,  0.0362]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0350, -0.0081,  0.0044,  ...,  0.0402,  0.0115, -0.0331],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0163, -0.0353, -0.0198,  ...,  0.0357,  0.0392,  0.0134],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0725e-02,  2.4192e-02, -8.1794e-03,  ..., -2.6156e-02,\n",
      "         -2.5318e-02,  2.0979e-03],\n",
      "        [-6.6933e-03,  1.4583e-02,  6.0958e-02,  ...,  4.1668e-03,\n",
      "          2.2115e-02, -1.6564e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1734, -0.1951], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in gen.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(enc.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cost.sum()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(slide, gen, num_samples):\n",
    "    zis = []\n",
    "    grads = []\n",
    "    all_grads = []\n",
    "    for p in gen.parameters():\n",
    "        start = [num_samples]\n",
    "        start.extend(list(p.shape))\n",
    "        all_grads.append(torch.zeros(start, device='cuda'))\n",
    "        grads.append(torch.zeros(p.shape, device='cuda'))\n",
    "        \n",
    "    for sample in range(num_samples):\n",
    "        preds = gen(slide)\n",
    "        logits = lsm(preds).squeeze(0)\n",
    "        b = torch.distributions.bernoulli.Bernoulli(logits=logits[:,1])\n",
    "        zi = b.sample() #zis = b.sample(torch.Size([batch_size]))\n",
    "        zis.append(zi)\n",
    "\n",
    "        logprobs = b.log_prob(zi).sum()\n",
    "        logprobs.backward()\n",
    "\n",
    "        for idx,p in enumerate(gen.parameters()):\n",
    "            all_grads[idx][sample] = p.grad\n",
    "\n",
    "        gen.zero_grad()\n",
    "        \n",
    "    return zis, grads, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationales_training(e, train_loader, gen, enc, num_samples, learning_rate, optimizer):\n",
    "    gen.train()\n",
    "    enc.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for slide,label in train_loader:\n",
    "        slide,label = slide.squeeze(0).cuda(),label.cuda()\n",
    "        zis, grads, all_grads = sampler(slide, gen, num_samples)\n",
    "        zis = torch.stack(zis)\n",
    "        \n",
    "        rationales = [slide[zi==1,:,:,:] for zi in zis]\n",
    "        sampled_rationales = torch.cat(rationales,dim=0)\n",
    "        outputs = enc(sampled_rationales)\n",
    "        \n",
    "        lens = zis.sum(dim=1)\n",
    "        indexs = torch.cat([torch.zeros(1),torch.cumsum(lens,0).cpu()]).int()\n",
    "        outputs = [outputs[indexs[n]:indexs[n+1]] for n,ix in enumerate(indexs[:-1])]\n",
    "        \n",
    "        pool = torch.stack([pool_fn(o).unsqueeze(0) for o in outputs])\n",
    "        y_hat = enc.classification_layer(pool.squeeze(1))\n",
    "        \n",
    "        znorm = torch.norm(zis.float(), p=1, dim=1)\n",
    "        zdist = torch.sum(torch.abs(zis[:,:-1] - zis[:,1:]), dim=1)\n",
    "        omega = (lamb1 * znorm) + (lamb2 * zdist)\n",
    "        cost = xent(y_hat, label.repeat(batch_size).cuda()) + omega\n",
    "        \n",
    "        for sample in range(num_samples):\n",
    "            for idx,p in enumerate(gen.parameters()):\n",
    "                grads[idx] += cost[sample] * all_grads[idx][sample] \n",
    "        \n",
    "        for idx,p in enumerate(gen.parameters()):\n",
    "            p.data = p.data - learning_rate * (grads[idx] / float(num_samples))\n",
    "            \n",
    "        loss = cost.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "    print('Epoch: {0}, Train Loss: {1:0.4f}'.format(e, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:179: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_BAD_PARAM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-375-096deeef5f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrationales_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-374-d792e52ebd8e>\u001b[0m in \u001b[0;36mrationales_training\u001b[0;34m(e, train_loader, gen, enc, num_samples, learning_rate, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrationales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mzi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mzi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msampled_rationales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrationales\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_rationales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MSI_prediction/labeled_nuclei_project/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 313\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_BAD_PARAM"
     ]
    }
   ],
   "source": [
    "e = 0\n",
    "rationales_training(e, dev_loader, gen, enc, batch_size, learning_rate, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
